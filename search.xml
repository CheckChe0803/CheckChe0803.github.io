<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>使用Docker部署Flink大数据项目</title>
      <link href="/2019/11/19/%E4%BD%BF%E7%94%A8Docker%E9%83%A8%E7%BD%B2Flink%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%A1%B9%E7%9B%AE/"/>
      <url>/2019/11/19/%E4%BD%BF%E7%94%A8Docker%E9%83%A8%E7%BD%B2Flink%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%A1%B9%E7%9B%AE/</url>
      
        <content type="html"><![CDATA[<p>本文是为<a href="https://github.com/CheckChe0803/flink-recommandSystem-demo" target="_blank" rel="noopener">基于Flink的商品推荐系统</a>所搭建的Docker环境，目的为了方便体验项目，一键部署项目需要的所有环境，并预填充所需要的数据。完整的环境包括Zookeeper 、Kafka 、 Hbase 、 Mysql 、 Redis 、 Flink 。</p><h2 id="环境介绍："><a href="#环境介绍：" class="headerlink" title="环境介绍："></a>环境介绍：</h2><ul><li>zookeeper 3.4.5</li><li>kafka 2.12-2.2.1</li><li>hbase 1.5.0</li><li>mysql 8.0.18</li><li>redis</li><li>flink 1.9.0</li></ul><p><img src="/2019/11/19/使用Docker部署Flink大数据项目/%E5%AE%B9%E5%99%A8%E7%8E%AF%E5%A2%83.png" alt></p><p>整个项目的部署和工作环境如上图所示。由于 kafka 和 hbase 均需要 zookeeper 的支持，所以没有使用集成式的 hbase docker镜像，所以笔者自己基于 hbase 1.5.0 制作了一版镜像，由于 hbase 的 standlone模式端口会随机生成，故也搭建成了伪分布式，HMaster 和 HRegionserver均在同一个 Docker Container 中。 在Kafka容器中，配置了对内对外两个地址，这是为了方便远程的队列消费，以及在容器内自动生产消息。</p><h2 id="搭建步骤："><a href="#搭建步骤：" class="headerlink" title="搭建步骤："></a>搭建步骤：</h2><h3 id="1-拉取镜像并启动"><a href="#1-拉取镜像并启动" class="headerlink" title="1. 拉取镜像并启动"></a>1. 拉取镜像并启动</h3><p>首先请下载 <a href>目录</a> 内的所有文件，因为mysql和hbase需要插入数据，kafka需要启动一个shell脚本自动的模拟用户点击行为。对应的请修改docker-compose.yml文件中对应的文件地址为自己所保存的地址。</p><p><img src="/2019/11/19/使用Docker部署Flink大数据项目/%E4%BF%AE%E6%94%B9volumes.png" alt></p><p>首先请保证已经成功安装了docker和docker-compose，其次请将docker-compose.yml文件中的 hbase 的 <code>hostname</code> 修改为自己的主机名。</p><p><strong>最后通过docker-compose启动所有容器</strong>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-linux recommend]# docker-compose up -d</span><br></pre></td></tr></table></figure><h3 id="2-给-Mysql-Redis-填充数据，给-Hbase-创建表结构"><a href="#2-给-Mysql-Redis-填充数据，给-Hbase-创建表结构" class="headerlink" title="2. 给 Mysql / Redis 填充数据，给 Hbase 创建表结构"></a>2. 给 Mysql / Redis 填充数据，给 Hbase 创建表结构</h3><p>进入mysql container， 连接mysql</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-linux recommend]# docker exec -ti mysql bash</span><br><span class="line">root@6fdc02332cd0:/# mysql -u root -p</span><br></pre></td></tr></table></figure><p>通过提前制作好的sql文件填充mysql</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span> source /opt/contact.sql</span><br></pre></td></tr></table></figure><p>之后是构建 hbase 表结构</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-linux recommend]# docker exec -ti hbase bash</span><br><span class="line">root@docker-linux:/opt/hbase# hbase shell /opt/hbase_ini.sql</span><br></pre></td></tr></table></figure><p>通过 list 命令可以看到我们创建的表都已成功</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):001:0&gt; list</span><br><span class="line">TABLE                                                             ··· </span><br><span class="line">8 row(s) in 0.1390 seconds</span><br><span class="line">=&gt; ["con", "p_history", "prod", "ps", "px", "u_history", "u_interest", "user"]</span><br></pre></td></tr></table></figure><p>最后进入redis，创建10个热度数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-linux recommend]# docker exec -ti redis bash</span><br><span class="line">root@7735aff1391f:/bin# redis-cli</span><br></pre></td></tr></table></figure><p>启动客户端后，之后按照如下格式创建 set 0  123之类的创建从0-9的10条数据即可，最后一位为商品id，在0-999内任意取值。</p><h3 id="3-启动-Kafka-消息生成器"><a href="#3-启动-Kafka-消息生成器" class="headerlink" title="3. 启动 Kafka 消息生成器"></a>3. 启动 Kafka 消息生成器</h3><p>进入kafka容器并启动shell脚本即可，脚本会按照每秒一次的频率发送message到log这个topic里。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-linux recommend]# docker exec -ti kafka bash</span><br><span class="line">bash-4.4# sh /opt/generator.sh -d</span><br></pre></td></tr></table></figure><p>验证成功发送消息的方法是重新开一个连接进入kafka容器并启动Consumer，注意kafka内部端口号配置的是9093</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash-4.4# $KAFKA_HOME/bin/kafka-console-consumer.sh --bootstrap-server kafka:9093 --topic log</span><br></pre></td></tr></table></figure><p>正确看到如下格式的消息即可</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">558,559,1574180802,1</span><br><span class="line">576,576,1574180819,3</span><br><span class="line">585,585,1574180828,3</span><br><span class="line">594,594,1574180837,3</span><br><span class="line">603,603,1574180846,3</span><br><span class="line">613,613,1574180856,1</span><br></pre></td></tr></table></figure><h2 id="执行任务："><a href="#执行任务：" class="headerlink" title="执行任务："></a>执行任务：</h2><p>我们这个项目一共有6个flink的任务，如果只是在本地测试的话，可以直接在IDEA中启动对应的任务即可，为了做实验，我们把项目打包并提交到 Docker 的 Flink 中，为了方便就不再把各种依赖单独放到集群中，而是直接打到任务的jar包中。正确的打包方法可以参考<a href="https://xinze.fun/2019/11/14/flink%E4%BD%BF%E7%94%A816-%E6%AD%A3%E7%A1%AE%E6%89%93%E5%8C%85Flink%E7%A8%8B%E5%BA%8F%E5%B9%B6%E4%BD%BF%E7%94%A8Cli%E6%8F%90%E4%BA%A4%E4%BB%BB%E5%8A%A1/">这篇博客</a></p><p>我们直接把打包好的jar包放到集群中，并拷贝到flink的docker中(jobmanager)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-linux pkg]# docker cp ./logtask.jar flink_jobmanager:/opt/</span><br></pre></td></tr></table></figure><p>然后进入flink jobmanager的容器中，提交jar包到集群</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-linux pkg]# docker exec -ti flink_jobmanager bash</span><br><span class="line">root@0f5edbeeecb9:/opt/flink# flink run /opt/logtask.jar</span><br></pre></td></tr></table></figure><p>提交成功后可以在Flink WebUI观察我们提交的任务</p><p><img src="/2019/11/19/使用Docker部署Flink大数据项目/flink%E7%95%8C%E9%9D%A2.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
            <tag> flink-recommend-demo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink使用16-正确打包Flink程序并使用Cli提交任务</title>
      <link href="/2019/11/14/flink%E4%BD%BF%E7%94%A816-%E6%AD%A3%E7%A1%AE%E6%89%93%E5%8C%85Flink%E7%A8%8B%E5%BA%8F%E5%B9%B6%E4%BD%BF%E7%94%A8Cli%E6%8F%90%E4%BA%A4%E4%BB%BB%E5%8A%A1/"/>
      <url>/2019/11/14/flink%E4%BD%BF%E7%94%A816-%E6%AD%A3%E7%A1%AE%E6%89%93%E5%8C%85Flink%E7%A8%8B%E5%BA%8F%E5%B9%B6%E4%BD%BF%E7%94%A8Cli%E6%8F%90%E4%BA%A4%E4%BB%BB%E5%8A%A1/</url>
      
        <content type="html"><![CDATA[<p>本文的计划是使用正确的maven插件打包当前教程代码库batch模块下的WordCount代码，并通过命令行的方式提交到Flink来启动任务。WordCount类即为Flink主方法类，该部分代码是Flink官方example的简单修改，只是对map方法填加了一点sleep来方便观察运行情况。</p><p><img src="/2019/11/14/flink使用16-正确打包Flink程序并使用Cli提交任务/package.png" alt></p><p>项目的运行环境使用Docker来部署Flink， Flink镜像可以从Docker hub上拉去，其Docker-Compose文件如下：</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">"2.1"</span></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line"><span class="attr">  jobmanager:</span></span><br><span class="line"><span class="attr">    image:</span> <span class="string">$&#123;FLINK_DOCKER_IMAGE_NAME:-flink&#125;</span></span><br><span class="line"><span class="attr">    expose:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"6123"</span></span><br><span class="line"><span class="attr">    ports:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"8081:8081"</span></span><br><span class="line"><span class="attr">    command:</span> <span class="string">jobmanager</span></span><br><span class="line"><span class="attr">    environment:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">JOB_MANAGER_RPC_ADDRESS=jobmanager</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  taskmanager:</span></span><br><span class="line"><span class="attr">    image:</span> <span class="string">$&#123;FLINK_DOCKER_IMAGE_NAME:-flink&#125;</span></span><br><span class="line"><span class="attr">    expose:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"6121"</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"6122"</span></span><br><span class="line"><span class="attr">    depends_on:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">jobmanager</span></span><br><span class="line"><span class="attr">    command:</span> <span class="string">taskmanager</span></span><br><span class="line"><span class="attr">    links:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"jobmanager:jobmanager"</span></span><br><span class="line"><span class="attr">    environment:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">JOB_MANAGER_RPC_ADDRESS=jobmanager</span></span><br></pre></td></tr></table></figure><p>正确启动Flink之后，就可以在WebUI上看到我们的环境了。</p><p><img src="/2019/11/14/flink使用16-正确打包Flink程序并使用Cli提交任务/webUI.png" alt></p><p>下面就开始打包我们的应用程序了。</p><p>官方推荐我们使用<code>maven-shade-plugin</code>插件，复制一下代码到POM中指定我们的主方法类即可。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-shade-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>shade<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">artifactSet</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">excludes</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>com.google.code.findbugs:jsr305<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>org.slf4j:*<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>log4j:*<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">excludes</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">artifactSet</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">filters</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">filter</span>&gt;</span></span><br><span class="line">                                <span class="comment">&lt;!-- Do not copy the signatures in the META-INF folder.</span></span><br><span class="line"><span class="comment">                                Otherwise, this might cause SecurityExceptions when using the JAR. --&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">artifact</span>&gt;</span>*:*<span class="tag">&lt;/<span class="name">artifact</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">excludes</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.SF<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.DSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.RSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">excludes</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">filters</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">transformers</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">transformer</span> <span class="attr">implementation</span>=<span class="string">"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">mainClass</span>&gt;</span>my.programs<span class="tag">&lt;/<span class="name">mainClass</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">transformer</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">transformers</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure><p>需要注意的是一般来说我们是不会将flink的一些相关的包直接打到项目里，通常有两种方案：</p><ul><li><p>将相关的jar包统一都放到<strong>flink/lib</strong>目录下</p></li><li><p>构建一个单独的common模块，所以使用到的包都放在这个模块中打包并上传到集群，之后其他模块只需要引用该common模块即可</p><p>具体的操作可以见<a href="https://mp.weixin.qq.com/s?__biz=MzUzMDYwOTAzOA==&mid=2247483672&idx=1&sn=8721acbc6df78caa36d3c758b50d6115&chksm=fa4e67f9cd39eeef4d0b53f058caade35fb724e1f261b98c55c66c944c5d082c1bf11884e5d4&mpshare=1&scene=1&srcid=&sharer_sharetime=1573729663613&sharer_shareid=f92bcee4884a1d62558833c4d5a5d308&key=b6271393c1b0a8ed7bff3bcfc8c3573298bc16bcdc7abea801f7bf8209bbdeb460398de24395a8aafd6e4fac0c082c837ccd4c8ad0debd61292defec7d9710ecb833b5e09af4a7ef11f64e4f9f4fc184&ascene=1&uin=MTgyMTgxNDcwMQ%3D%3D&devicetype=Windows+10&version=62070152&lang=zh_CN&pass_ticket=XKnNFqmGAAZ3n6MGPrD0XK7e1jY0p43XBBMsX0bVXKGEz2t9Z%2FLTlxxc%2FhsS%2FejF" target="_blank" rel="noopener">这篇文章</a></p></li></ul><p>打包好后就可以直接是用 FLink Cli 提交到集群来开始job了 。</p><p>Flink Cli 一般来讲主要作用有：提交并执行任务、取消任务、获取任务状态信息、列出正在运行和等待的任务、触发savepoint等。</p><p>我们将已经打包好的jar包放到docker中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker cp /opt/flink/wordcount.jar flink_jobmanager_1:/opt/</span><br></pre></td></tr></table></figure><p>然后就可以通过命令行启动任务了，启动完成后我们可以在webUI上看到任务的执行情况。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker exec -ti flink_jobmanager_1 bash -c 'flink run /opt/wordcount.jar'</span><br></pre></td></tr></table></figure><p><img src="/2019/11/14/flink使用16-正确打包Flink程序并使用Cli提交任务/runningJob.png" alt></p><p>Flink Cli 的命令有很多，具体的内容可以参考官网示例：</p><p><a href="https://ci.apache.org/projects/flink/flink-docs-stable/ops/cli.html" target="_blank" rel="noopener">Flink Cli Examples</a></p>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
            <tag> package </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用Docker部署Kafka时的网络应该如何配置</title>
      <link href="/2019/11/11/%E4%BD%BF%E7%94%A8Docker%E9%83%A8%E7%BD%B2Kafka%E6%97%B6%E7%9A%84%E7%BD%91%E7%BB%9C%E5%BA%94%E8%AF%A5%E5%A6%82%E4%BD%95%E9%85%8D%E7%BD%AE/"/>
      <url>/2019/11/11/%E4%BD%BF%E7%94%A8Docker%E9%83%A8%E7%BD%B2Kafka%E6%97%B6%E7%9A%84%E7%BD%91%E7%BB%9C%E5%BA%94%E8%AF%A5%E5%A6%82%E4%BD%95%E9%85%8D%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<p>本文是我在使用Docker部署kafka遇到一些问题之后，在网上看到的一篇比较优秀的英文资料<a href="https://www.kaaproject.org/kafka-docker" target="_blank" rel="noopener">Link</a>。借此翻译一下这篇文章，也帮助自己搞懂在使用Docker时遇到的一些网络问题，尤其是Host怎样配置。</p><p>作者的Kafka使用环境是Kafka Producer 和 Broker 均在 Docker 网络中， Kafka Consumer 在宿主机环境中。结构如下图这样子：</p><p><img src="/2019/11/11/使用Docker部署Kafka时的网络应该如何配置/use-case.png" alt></p><p>首先，我从Docker hub 中找到了一个Kafka Docker image。 我使用的是Wurstmeister <a href="https://hub.docker.com/r/wurstmeister/kafka/" target="_blank" rel="noopener">Kafka</a> and <a href="https://hub.docker.com/r/wurstmeister/zookeeper/" target="_blank" rel="noopener">ZooKeeper</a> images， 然后Docker-compose的文件是按照下面的格式定义的：</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">'2'</span></span><br><span class="line"></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  zookeeper:</span></span><br><span class="line"><span class="attr">    image:</span> <span class="string">wurstmeister/zookeeper:3.4.6</span></span><br><span class="line"><span class="attr">    expose:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">"2181"</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  kafka:</span></span><br><span class="line"><span class="attr">    image:</span> <span class="string">wurstmeister/kafka:2.11-2.0.0</span></span><br><span class="line"><span class="attr">    depends_on:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">zookeeper</span></span><br><span class="line"><span class="attr">    ports:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">"9092:9092"</span></span><br><span class="line"><span class="attr">    environment:</span></span><br><span class="line"><span class="attr">      KAFKA_ADVERTISED_LISTENERS:</span> <span class="attr">PLAINTEXT://localhost:9092</span></span><br><span class="line"><span class="attr">      KAFKA_LISTENERS:</span> <span class="attr">PLAINTEXT://0.0.0.0:9092</span></span><br><span class="line"><span class="attr">      KAFKA_ZOOKEEPER_CONNECT:</span> <span class="attr">zookeeper:2181</span></span><br></pre></td></tr></table></figure><p>启动Docker，然后按照<a href="https://kafka.apache.org/quickstart" target="_blank" rel="noopener">kafka QuickStart</a> 的步骤来使用<code>kafka-console-producer.sh</code> 和 <code>kafka-console-consumer.sh</code></p><p>在宿主机中运行 Producer 的结果：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">andrew@host$</span> bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test</span><br><span class="line"><span class="meta">&gt;</span>Hi there!</span><br><span class="line"><span class="meta">&gt;</span>It is a test message.</span><br></pre></td></tr></table></figure><p>在宿主机中运行 Consumer的结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">andrew@host$ bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning</span><br><span class="line">Hi there!</span><br><span class="line">It&apos;s a test message.</span><br></pre></td></tr></table></figure><p>可以看到，不管是producer还是consumer都可以在宿主机网络中正常工作。需要注意的是<code>KAFKA_ADVERTISED_LISTENERS</code>这个环境变量在Compose文件中的设置.Kafka会在client第一次连接的时候把这个变量值发送给client。在接收到这个变量值之后，client就可以使用它来从kafka 的 broker 中消费或者生产数据了。</p><p>由于我们定义的变量值为 <code>PLAINTEXT://localhost:9092</code>， producer和consumer在初始化连接时都会使用它并且之后所有的通信都会通过 9092 这个端口。</p><p><img src="/2019/11/11/使用Docker部署Kafka时的网络应该如何配置/client-on-host-kafka-in-docker-wrong.png" alt></p><p>这里的关键要点是客户端使用指定的Kafka地址（<code>--bootstrap-server</code> and <code>--broker-list</code>的值）。Kafka之后冲顶下他们的值为<code>KAFKA_ADVERTISED_LISTENERS</code></p><p>下面让我们在运行Kafka容器的同一Docker网络内的任意Docker容器内运行Producer：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">oot@869f83f2f265:/kafka# bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test</span><br><span class="line"><span class="meta">&gt;</span>Hi there!</span><br><span class="line">[2018-10-10 14:37:40,397] WARN [Producer clientId=console-producer] Connection to node -1 could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)</span><br></pre></td></tr></table></figure><p>这时候就会报错了。这时候发生的事情是client收到了<code>KAFKA_ADVERTISED_LISTENERS</code>这个环境变量的值（<code>PLAINTEXT://localhost:9092</code>）,之后尝试去连接它然后发现失败了，因为在自己docker网络中并没有这个地址。显然，从client的角度来说可以通过<code>kafka:9092</code>这个地址来连接kafka。因此，为了使 client 能够和 broker 通信，<code>KAFKA_ADVERTISED_LISTENERS</code>这个变量值就必须设置为<code>PLAINTEXT://kafka:9092</code>,那么，下面就来重新构建一下我们的Compose文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">kafka:</span><br><span class="line">    image: wurstmeister/kafka:2.11-2.0.0</span><br><span class="line">    depends_on:</span><br><span class="line">    - zookeeper</span><br><span class="line">    ports:</span><br><span class="line">    - "9092:9092"</span><br><span class="line">    environment:</span><br><span class="line">      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092</span><br><span class="line">      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092</span><br><span class="line">      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181</span><br></pre></td></tr></table></figure><p>然后测试producer：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">oot@7dfb9eaa81dc:/kafka# bin/kafka-console-producer.sh --broker-list kafka:9092 --topic test</span><br><span class="line"><span class="meta">&gt;</span>Hi there!</span><br></pre></td></tr></table></figure><p>成功！现在我们可以在Docker容器内使用Producer发送消息了。</p><p>下面我们来尝试通过宿主机上的Consumer消费Kafka的数据：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">andrew@host$</span> bin/kafka-console-consumer.sh --bootstrap-server kafka:9092 --topic test --from-beginning</span><br><span class="line">[2018-10-10 23:57:06,827] WARN Removing server kafka:9092 from bootstrap.servers as DNS resolution failed for kafka (org.apache.kafka.clients.ClientUtils)</span><br></pre></td></tr></table></figure><p>正如预料的，此时consumer并不能连接到broker因为宿主机并不能识别<code>kafka:9092</code>这个地址。我们需要再重新设置一下上面的Compose文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">kafka:</span><br><span class="line">  image: wurstmeister/kafka:2.11-2.0.0</span><br><span class="line">  depends_on:</span><br><span class="line">  - zookeeper</span><br><span class="line">  ports:</span><br><span class="line">  - "9092:9092"</span><br><span class="line">  expose:</span><br><span class="line">  - "9093"</span><br><span class="line">  environment:</span><br><span class="line">    KAFKA_ADVERTISED_LISTENERS: INSIDE://kafka:9093,OUTSIDE://localhost:9092</span><br><span class="line">    KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT</span><br><span class="line">    KAFKA_LISTENERS: INSIDE://0.0.0.0:9093,OUTSIDE://0.0.0.0:9092</span><br><span class="line">    KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181</span><br><span class="line">    KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE</span><br></pre></td></tr></table></figure><p>下面让我们来解释一下上面的几个环境变量：</p><ul><li><p><code>KAFKA_ADVERTISED_LISTENERS</code></p><p>kafka的broker将监听地址表（<code>0.0.0.0:9093</code>, <code>0.0.0.0:9092</code>）和 listener （<code>INSIDE</code>, <code>OUTSIDE</code>）</p></li><li><p><code>KAFKA_ADVERTISED_LISTENERS</code></p><p>指向Broker的可用地址列表，kafka 将会在初始连接时将地址发送给client</p></li><li><p><code>KAFKA_LISTENER_SECURITY_PROTOCOL_MAP</code></p><p>将上面定义的 listener 名称（INSIDE，OUTSIDE）映射到PLAINTEXT Kafka协议。</p></li><li><p><code>KAFKA_INTER_BROKER_LISTENER_NAME</code></p><p>指向跨Broker间通信时的命名地址</p></li></ul><p>这里我们定义了两个listeners（<code>INSIDE://0.0.0.0:9093</code>, <code>OUTSIDE://0.0.0.0:9092</code>）来分别表示Docker网络内部的流量和Docker主机外部的流量。我们为跨Broker间的通信定义了 INSIDE listener。通过<code>KAFKA_ADVERTISED_LISTENERS</code> 和<code>KAFKA_LISTENER_SECURITY_PROTOCOL_MAP</code></p><p>我们将<code>PLAINTEXT://kafka:9093</code>发送给那些使用<code>kafka:9093</code>连接的客户端和</p><p><code>PLAINTEXT://localhost:9092</code>发送给那些使用<code>localhost:9092</code>连接的客户端。</p><p>总之，我们定义了两种类型的客户端-内部和外部-并且配置kafka返回不同的地址给对应的客户端。</p><p>现在我们再来尝试下在Docker网络中使用Producer：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@7dfb9eaa81dc:/kafka# bin/kafka-console-producer.sh --broker-list kafka:9093 --topic test</span><br><span class="line"><span class="meta">&gt;</span>Hi there!</span><br></pre></td></tr></table></figure><p>在宿主机中使用Consumer：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">andrew@host$</span> bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning</span><br><span class="line">Hi there!</span><br></pre></td></tr></table></figure><p>现在全部都是成功的了！</p><p>producer和kafka broker都是在docker网络的内部。</p><p><img src="/2019/11/11/使用Docker部署Kafka时的网络应该如何配置/client-and-kafka-in-docker.png" alt></p><p>Consumer在外部， borker在Docker网络内部</p><p><img src="/2019/11/11/使用Docker部署Kafka时的网络应该如何配置/client-on-host-kafka-in-docker.png" alt></p><p>现在，当服务和kafka 部署在不同的网络环境中的时候，我们也知道该如何去配置Docker了。</p>]]></content>
      
      
      <categories>
          
          <category> kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
            <tag> kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink使用15-在Flink SQL中连接kafka和mysql</title>
      <link href="/2019/11/10/flink%E4%BD%BF%E7%94%A815-%E5%9C%A8Flink-SQL%E4%B8%AD%E8%BF%9E%E6%8E%A5kafka%E5%92%8Cmysql/"/>
      <url>/2019/11/10/flink%E4%BD%BF%E7%94%A815-%E5%9C%A8Flink-SQL%E4%B8%AD%E8%BF%9E%E6%8E%A5kafka%E5%92%8Cmysql/</url>
      
        <content type="html"><![CDATA[<p>本文主要介绍如何使用 FLink SQL 自己的 DDL语言来构建基于 kafka 的表和 基于Mysql 的表，并直接把从 kafka 接过来的 Json 格式的数据转换为 表结构后直接写入到Mysql，有了这样的经验之后，大家可以自行修改 DML操作来实现不同的业务。文章内容参考了一些阿里云邪大佬的文章<a href="http://wuchong.me/blog/2019/09/02/flink-sql-1-9-read-from-kafka-write-into-mysql/" target="_blank" rel="noopener">Link</a>，写的很好。</p><p>环境配置如下：</p><ul><li>zookeeper : 3.4.6</li><li>kafka: 2.12-2.2.1</li><li>mysql: 8.0</li></ul><p>以上三个组件全部是通过Docker搭建，我的环境是使用VirtualBox搭建的Centos7虚拟机，在虚拟机上安装Docker， 之后在本地主机IDE内调试代码。其中遇到了不少坑，主要是FLink与Kafka的通信，可以参考我的Docker-Compose文件的配置，已经解决了网络问题。注意<code>KAFKA_ADVERTISED_LISTENERS</code>的地址修改成自己的虚拟机IP地址。</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">'2.1'</span></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line"><span class="attr">  zookeeper:</span></span><br><span class="line"><span class="attr">    image:</span> <span class="string">wurstmeister/zookeeper:3.4.6</span></span><br><span class="line"><span class="attr">    ports:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"2181:2181"</span></span><br><span class="line"><span class="attr">  kafka:</span></span><br><span class="line"><span class="attr">    image:</span> <span class="string">wurstmeister/kafka:2.12-2.2.1</span></span><br><span class="line"><span class="attr">    ports:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"9092:9092"</span></span><br><span class="line"><span class="attr">    depends_on:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">zookeeper</span></span><br><span class="line"><span class="attr">    expose:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">"9093"</span></span><br><span class="line"><span class="attr">    environment:</span></span><br><span class="line"><span class="attr">      KAFKA_ADVERTISED_LISTENERS:</span> <span class="attr">INSIDE://kafka:9093,OUTSIDE://192.168.56.103:9092</span></span><br><span class="line"><span class="attr">      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP:</span> <span class="attr">INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT</span></span><br><span class="line"><span class="attr">      KAFKA_LISTENERS:</span> <span class="attr">INSIDE://0.0.0.0:9093,OUTSIDE://0.0.0.0:9092</span></span><br><span class="line"><span class="attr">      KAFKA_ZOOKEEPER_CONNECT:</span> <span class="attr">zookeeper:2181</span></span><br><span class="line"><span class="attr">      KAFKA_INTER_BROKER_LISTENER_NAME:</span> <span class="string">INSIDE</span></span><br><span class="line"><span class="attr">      KAFKA_CREATE_TOPICS:</span> <span class="string">"flink:1:1"</span></span><br><span class="line"><span class="attr">    volumes:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">/var/run/docker.sock:/var/run/docker.sock</span></span><br><span class="line"><span class="attr">  mysql:</span></span><br><span class="line"><span class="attr">    image:</span> <span class="string">mysql</span></span><br><span class="line"><span class="attr">    command:</span> <span class="bullet">--default-authentication-plugin=mysql_native_password</span></span><br><span class="line"><span class="attr">    restart:</span> <span class="string">always</span></span><br><span class="line"><span class="attr">    ports:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"3306:3306"</span></span><br><span class="line"><span class="attr">    environment:</span></span><br><span class="line"><span class="attr">      MYSQL_ROOT_PASSWORD:</span> <span class="number">123456</span></span><br></pre></td></tr></table></figure><p>环境搭建好之后就是正式的代码部分了：</p><p>核心是两段SQL代码，分别是用来连接Kafka和MYSQL的。</p><p>其中kafka使用json格式来解析。</p><p>样例数据（{“t”:1570,”user_name”:”xiaoming”,”cnt”:100}）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- source</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> user_log (</span><br><span class="line">    user_id <span class="built_in">VARCHAR</span>,</span><br><span class="line">    item_id <span class="built_in">VARCHAR</span>,</span><br><span class="line">    category_id <span class="built_in">VARCHAR</span>,</span><br><span class="line">    behavior <span class="built_in">VARCHAR</span>,</span><br><span class="line">    ts <span class="built_in">TIMESTAMP</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">    <span class="string">'connector.type'</span> = <span class="string">'kafka'</span>,</span><br><span class="line">    <span class="string">'connector.version'</span> = <span class="string">'universal'</span>,</span><br><span class="line">    <span class="string">'connector.topic'</span> = <span class="string">'user_behavior'</span>,</span><br><span class="line">    <span class="string">'connector.startup-mode'</span> = <span class="string">'earliest-offset'</span>,</span><br><span class="line">    <span class="string">'connector.properties.0.key'</span> = <span class="string">'group.id'</span>,</span><br><span class="line">    <span class="string">'connector.properties.0.value'</span> = <span class="string">'test-group'</span>,</span><br><span class="line">    <span class="string">'connector.properties.1.key'</span> = <span class="string">'bootstrap.servers'</span>,</span><br><span class="line">    <span class="string">'connector.properties.1.value'</span> = <span class="string">'localhost:9092'</span>,</span><br><span class="line">    <span class="string">'connector.specific-offsets.0.partition'</span> = <span class="string">'0'</span>,</span><br><span class="line">    <span class="string">'connector.specific-offsets.0.offset'</span> = <span class="string">'0'</span>,</span><br><span class="line">    <span class="string">'update-mode'</span> = <span class="string">'append'</span>,</span><br><span class="line">    <span class="string">'format.type'</span> = <span class="string">'json'</span>,</span><br><span class="line">    <span class="string">'format.derive-schema'</span> = <span class="string">'true'</span></span><br><span class="line">);</span><br><span class="line"><span class="comment">-- sink</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> pvuv_sink (</span><br><span class="line">    dt <span class="built_in">VARCHAR</span>,</span><br><span class="line">    pv <span class="built_in">BIGINT</span>,</span><br><span class="line">    uv <span class="built_in">BIGINT</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">    <span class="string">'connector.type'</span> = <span class="string">'jdbc'</span>,</span><br><span class="line">    <span class="string">'connector.url'</span> = <span class="string">'jdbc:mysql://localhost:3306/flink-test'</span>,</span><br><span class="line">    <span class="string">'connector.table'</span> = <span class="string">'pvuv_sink'</span>,</span><br><span class="line">    <span class="string">'connector.username'</span> = <span class="string">'root'</span>,</span><br><span class="line">    <span class="string">'connector.password'</span> = <span class="string">'123456'</span>,</span><br><span class="line">    <span class="string">'connector.write.flush.max-rows'</span> = <span class="string">'1'</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure><p>在 Java代码中，可以直接使用tEnv的sqlUpdate()方法来注册这两张表，之后就可以直接使用了。具体操作如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1. 连接kafka构建源表</span></span><br><span class="line">tEnv.sqlUpdate(kafkaSourceSql);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2. 定义要输出的表</span></span><br><span class="line">tEnv.sqlUpdate(mysqlSinkSql);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3. 自定义具体的 DML 操作,这里我直接将kafka写入到mysql</span></span><br><span class="line"><span class="comment">// 对于Insert Into 操作，同样还是要使用sqlUpdate（）方法</span></span><br><span class="line">tEnv.sqlUpdate(<span class="string">"INSERT INTO sink "</span> +</span><br><span class="line"><span class="string">"SELECT * from log where cnt=100"</span>);</span><br></pre></td></tr></table></figure><p>可以直接通过mysql的客户端看到我们的写入结果！</p><p><img src="/2019/11/10/flink使用15-在Flink-SQL中连接kafka和mysql/mysql.png" alt></p><p>以上就是使用 Flink SQL 的 DDL 语言通过不同的外部数据源建立表的过程。</p>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
            <tag> sql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink使用14-使用SQL操作几种window</title>
      <link href="/2019/11/01/flink%E4%BD%BF%E7%94%A814-%E4%BD%BF%E7%94%A8SQL%E6%93%8D%E4%BD%9C%E5%87%A0%E7%A7%8Dwindow/"/>
      <url>/2019/11/01/flink%E4%BD%BF%E7%94%A814-%E4%BD%BF%E7%94%A8SQL%E6%93%8D%E4%BD%9C%E5%87%A0%E7%A7%8Dwindow/</url>
      
        <content type="html"><![CDATA[<p>Flink SQL 支持三种窗口类型, 分别为 Tumble Windows / HOP Windows 和 Session Windows. 其中 HOP windows 对应 Table API 中的 Sliding Window, 同时每种窗口分别有相应的使用场景和方法.</p><table><thead><tr><th></th><th>Tumble Windows</th><th>HOP Window</th><th>Session Windows</th></tr></thead><tbody><tr><td></td><td>TUMBLE(time_attr, interval)</td><td>HOP(time_attr, interval1,interval2)</td><td>HOP(time_attr, interval)</td></tr></tbody></table><p>下面用几段代码演示如何使用上面 3组 API. 完整的代码见 <a href="https://github.com/CheckChe0803/flink-simple-tutorial/tree/master/table/src/main/java/sql/window" target="_blank" rel="noopener">Github</a></p><p>首先填充一点测试数据</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 初始数据  字段解释 -&gt; (timeStamp , name , value)</span></span><br><span class="line">DataStream&lt;Tuple3&lt;Long, String,Integer&gt;&gt; log = env.fromCollection(Arrays.asList(</span><br><span class="line">    <span class="comment">//时间 14:53:00</span></span><br><span class="line">    <span class="keyword">new</span> Tuple3&lt;&gt;(<span class="number">1572591180_000L</span>,<span class="string">"xiao_ming"</span>,<span class="number">300</span>),</span><br><span class="line">    <span class="comment">//时间 14:53:09</span></span><br><span class="line">    <span class="keyword">new</span> Tuple3&lt;&gt;(<span class="number">1572591189_000L</span>,<span class="string">"zhang_san"</span>,<span class="number">303</span>),</span><br><span class="line">    <span class="comment">//时间 14:53:12</span></span><br><span class="line">    <span class="keyword">new</span> Tuple3&lt;&gt;(<span class="number">1572591192_000L</span>, <span class="string">"xiao_li"</span>,<span class="number">204</span>),</span><br><span class="line">    <span class="comment">//时间 14:53:21</span></span><br><span class="line">    <span class="keyword">new</span> Tuple3&lt;&gt;(<span class="number">1572591201_000L</span>,<span class="string">"li_si"</span>, <span class="number">208</span>)));</span><br></pre></td></tr></table></figure><p>然后是转换为 Table</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//这里需要注意的是 如果采用了EventTime, 那么 对应字段后面加 .rowtime, 否则加 .proctime</span></span><br><span class="line">Table logT = tEnv.fromDataStream(logWithTime, <span class="string">"t.rowtime, name, v"</span>);</span><br></pre></td></tr></table></figure><h4 id="Tumble-Windows"><a href="#Tumble-Windows" class="headerlink" title="Tumble Windows"></a>Tumble Windows</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// GROUP BY TUMBLE(t, INTERVAL '10' SECOND) 相当于根据10s的时间来划分窗口</span></span><br><span class="line"><span class="comment">// TUMBLE_START(t, INTERVAL '10' SECOND) 获取窗口的开始时间</span></span><br><span class="line"><span class="comment">// TUMBLE_END(t, INTERVAL '10' SECOND) 获取窗口的结束时间</span></span><br><span class="line">tEnv.sqlQuery(<span class="string">"SELECT TUMBLE_START(t, INTERVAL '10' SECOND) AS window_start,"</span> +</span><br><span class="line">                <span class="string">"TUMBLE_END(t, INTERVAL '10' SECOND) AS window_end, SUM(v) FROM "</span></span><br><span class="line">                + logT + <span class="string">" GROUP BY TUMBLE(t, INTERVAL '10' SECOND)"</span>);</span><br></pre></td></tr></table></figure><h4 id="HOP-Windows"><a href="#HOP-Windows" class="headerlink" title="HOP Windows"></a>HOP Windows</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// HOP(time_attr, interval1, interval2)</span></span><br><span class="line"><span class="comment">// interval1 滑动长度</span></span><br><span class="line"><span class="comment">// interval2 窗口长度</span></span><br><span class="line"><span class="comment">// HOP_START(t, INTERVAL '5' SECOND, INTERVAL '10' SECOND) 表示窗口开始时间</span></span><br><span class="line"><span class="comment">// HOP_END(t, INTERVAL '5' SECOND, INTERVAL '10' SECOND) 表示窗口结束时间</span></span><br><span class="line">Table result = tEnv.sqlQuery(<span class="string">"SELECT HOP_START(t, INTERVAL '5' SECOND, INTERVAL '10' SECOND) AS window_start,"</span> </span><br><span class="line">                             + <span class="string">"HOP_END(t, INTERVAL '5' SECOND, INTERVAL '10' SECOND) AS window_end, SUM(v) FROM "</span></span><br><span class="line">                             + logT + <span class="string">" GROUP BY HOP(t, INTERVAL '5' SECOND, INTERVAL '10' SECOND)"</span>);</span><br></pre></td></tr></table></figure><h4 id="Session-Windows"><a href="#Session-Windows" class="headerlink" title="Session Windows"></a>Session Windows</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// SESSION(time_attr, interval)</span></span><br><span class="line"><span class="comment">// interval 表示两条数据触发session的最大间隔</span></span><br><span class="line">Table result = tEnv.sqlQuery(<span class="string">"SELECT SESSION_START(t, INTERVAL '5' SECOND) AS window_start,"</span> </span><br><span class="line">                             +<span class="string">"SESSION_END(t, INTERVAL '5' SECOND) AS window_end, SUM(v) FROM "</span></span><br><span class="line">                             + logT + <span class="string">" GROUP BY SESSION(t, INTERVAL '5' SECOND)"</span>);</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
            <tag> sql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink使用13-开始体验 Flink SQL</title>
      <link href="/2019/10/31/flink%E4%BD%BF%E7%94%A813-%E5%BC%80%E5%A7%8B%E4%BD%93%E9%AA%8C-Flink-SQL/"/>
      <url>/2019/10/31/flink%E4%BD%BF%E7%94%A813-%E5%BC%80%E5%A7%8B%E4%BD%93%E9%AA%8C-Flink-SQL/</url>
      
        <content type="html"><![CDATA[<p>SQL API 是 Flink 中最顶级的 API , 它构建了 Table API 之上, 也可以方便的和 Table 做转换, 构建 SQL 所使用的Environment 也是 Table Environment .  Flink SQL 底层使用 Apache Calcite 框架, 将标准的 Flink SQL 语句解析并转换成底层的算子处理逻辑. 下面就直接用 Flink 官方仓库中的 案例 <a href="https://github.com/CheckChe0803/flink-simple-tutorial/blob/master/table/src/main/java/sql/StreamSQLExample.java" target="_blank" rel="noopener">Code Link</a>来做一个演示.</p><ol><li>获取执行环境</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 首先同样有流处理和批处理的区别, </span></span><br><span class="line"><span class="comment">// 获取对应的environment之后直接转换为Table environment ,</span></span><br><span class="line"><span class="comment">// 就可以使用SQL API,</span></span><br><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);</span><br></pre></td></tr></table></figure><ol start="2"><li>拿到要操作的表</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 将 Stream 转换为 Table, 可以采用不同的办法</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 将 DataStream 转换为 Table</span></span><br><span class="line">Table tableA = tEnv.fromDataStream(orderA, <span class="string">"user, product, amount"</span>);</span><br><span class="line"><span class="comment">// 将 DataStream 注册成 Table</span></span><br><span class="line">tEnv.registerDataStream(<span class="string">"OrderB"</span>, orderB, <span class="string">"user, product, amount"</span>);</span><br></pre></td></tr></table></figure><ol start="3"><li>执行SQL语句</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// TableEnvironment 有 SqlQuery 和 SqlUpdate 两种操作符可以使用</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// union 两个 table</span></span><br><span class="line"> Table result = tEnv.sqlQuery(<span class="string">"SELECT * FROM "</span> + tableA + <span class="string">" WHERE amount &gt; 2 UNION ALL "</span> +<span class="string">"SELECT * FROM OrderB WHERE amount &gt; 2"</span>);</span><br></pre></td></tr></table></figure><p>SQL可以执行许多复杂的操作,本文先简单的了解下 SQL 的API</p>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
            <tag> sql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink使用12-用 table API 实现WordCount</title>
      <link href="/2019/10/08/flink%E4%BD%BF%E7%94%A812-%E7%94%A8-table-API-%E5%AE%9E%E7%8E%B0WordCount/"/>
      <url>/2019/10/08/flink%E4%BD%BF%E7%94%A812-%E7%94%A8-table-API-%E5%AE%9E%E7%8E%B0WordCount/</url>
      
        <content type="html"><![CDATA[<p>Table API 是 Flink 构建在 DataSet 和 DataStream API 之上的一套结构化编程接口. 本文希望通过一个简单的 wordCount 的例子首先来体验一下普通的 Flink Table 的代码是由哪些部分构成的.</p><ol><li><p><strong>获取 TableEnvironment</strong></p><p>ExecutionEnvironment 是必不可少的, 不管是Stream API 还是 batch API 都需要一个Environment来管理程序, TableEnvironment 也是在使用 Table API 时首先需要创建的, 它提供了注册内部表/ 执行 Flink SQL 语句/ 注册自定义函数等多种功能. 要获取 TableEnvironment, 首先需要根据情况先创建 DataSet 或者 DataStream 的 Environment, 之后再转换为 TableEnvironment.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 以获取 batch Table API 为例, stream类似</span></span><br><span class="line">ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">BatchTableEnvironment tEnv = BatchTableEnvironment.create(env);</span><br></pre></td></tr></table></figure></li><li><p><strong>拿到 Table</strong></p><p>在以 DataSet API 或者 DataStream API 操作时, 一般有通过集合 / 文件 / socket / 外部数据源 等多种方式来将数据输入. 在 Table API 中, 同样也可以以多种不同的形式去创建一个 Table, 只不过过程相对复杂一些. 拿到一张表的方式有多种, 有通过 Table Descriptor / 用户自定义 Table Source / 由DataStream或者DataSet转换的形式等来实现. 具体的实现操作以后的文章再来详细展示, 下面就以通过 DataSet转换的方式来简单完成.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// WC 为自定义的 POJO 来统计单词的信息. 首先构建DataSet</span></span><br><span class="line">DataSet&lt;WC&gt; input = env.fromElements(</span><br><span class="line">                <span class="keyword">new</span> WC(<span class="string">"Hello"</span>, <span class="number">1</span>),</span><br><span class="line">                <span class="keyword">new</span> WC(<span class="string">"flink"</span>, <span class="number">1</span>),</span><br><span class="line">                <span class="keyword">new</span> WC(<span class="string">"Hello"</span>, <span class="number">1</span>));</span><br><span class="line"><span class="comment">// 直接将 DataSet 转换为 Table</span></span><br><span class="line">Table table = tEnv.fromDataSet(input);</span><br></pre></td></tr></table></figure></li><li><p><strong>数据转换操作</strong></p><p>拿到了 Table 之后, 就可以正常的进行数据转换的操作了, 如底层的 API一样, 常见的操作 Table API 都已经实现好了, 包括 <strong>数据查询和过滤</strong> / <strong>窗口操作</strong> / <strong>聚合操作</strong> / <strong>多表关联</strong> / <strong>集合操作</strong> / <strong>排序操作</strong> / <strong>数据写入</strong>. 但是具体的 API语法还是与底层API有很大的不同的, 目的就是为了更方便更类似SQL的形式方便用户去写业务逻辑, 下面就展示一下如何实现 wordCount这个一个步骤.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Table filtered = table</span><br><span class="line">                .groupBy(<span class="string">"word"</span>)</span><br><span class="line">    <span class="comment">// 在 select 方法中可以方便的直接使用类sql语句进行操作</span></span><br><span class="line">                .select(<span class="string">"word, frequency.sum as frequency"</span>)</span><br><span class="line">                .filter(<span class="string">"frequency = 2"</span>);</span><br></pre></td></tr></table></figure></li><li><p><strong>数据输出</strong></p><p>数据处理完毕之后最后一步就是结果的输出, 这一步与底层的API也是大同小异的, 可以将结果直接insetInto()到其他在TableEnvironment注册的表中, 也可以将处理完成的结果转换为 DataSet 或者是 DataStream 亦或是通过自定义的 sink 输出. </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1. 直接输入到其他表中</span></span><br><span class="line">table.insertInto(<span class="string">"otherTbale"</span>);</span><br><span class="line"><span class="comment">// 2. 转换为 DataSet / DataStream</span></span><br><span class="line">DataSet&lt;WC&gt; result = tEnv.toDataSet(filtered, WC.class);</span><br><span class="line"><span class="comment">// 3. 自定义 sink</span></span><br><span class="line">CsvTableSink tableSink = <span class="keyword">new</span> CsvTableSink(path, <span class="string">","</span>);</span><br><span class="line">tEnv.registerTableSink(<span class="string">"csvSink"</span>, tableSink);</span><br></pre></td></tr></table></figure></li></ol><p>通过上面几步, 一个完整的 Flink Table API 的程序就构建完毕了, 有了前面学习 flink 的经验, 这部分其实是水到渠成的, 稍微了解下就可以上手了, 本文对应的 wordCount 案例在 <a href="https://github.com/CheckChe0803/flink-simple-tutorial/blob/master/table/src/main/java/wordCount/WordCountTable.java" target="_blank" rel="noopener">github</a> 中, 在后面的部分会继续讲解一下其他 Table API 的内容.</p>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
            <tag> table </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink使用11-了解broadcast的用法</title>
      <link href="/2019/10/02/flink%E4%BD%BF%E7%94%A811-%E4%BA%86%E8%A7%A3broadcast%E7%9A%84%E7%94%A8%E6%B3%95/"/>
      <url>/2019/10/02/flink%E4%BD%BF%E7%94%A811-%E4%BA%86%E8%A7%A3broadcast%E7%9A%84%E7%94%A8%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>在Flink中，同一个算子可能存在若干个不同的并行实例，计算过程可能不在同一个Slot中进行，不同算子之间更是如此，因此不同算子的计算数据之间不能像Java数组之间一样互相访问，而广播变量<code>Broadcast</code>便是解决这种情况的. 在 flink 中, 针对某一个算子需要使用公共变量的情况下, 就可以把对应的数据给广播出去, 这样在所有的节点中都可以使用了. 典型的代码结构如下所示:</p><p>在一个算子中使用广播变量主要有两个步骤:</p><ol><li><p><strong>广播变量</strong> (一般写在算子的后面即可) </p><p>使用 withBroadcastSet(data, “name”) 这个方法即可, name变量代表了获取该广播变量的名称</p></li><li><p><strong>使用广播变量</strong></p><p>使用方法主要是通过 RichFunction, 在 对应的 open( )方法中, 可以根据名称来获取对应的广播变量, 只需要一次获取, 就可以一直使用了, 具体方法如下:</p></li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">dataSet.map(<span class="keyword">new</span> RichMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">            List&lt;Integer&gt; bc;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">super</span>.open(parameters);</span><br><span class="line">                <span class="comment">// 2. 获取广播变量</span></span><br><span class="line">                <span class="keyword">this</span>.bc = getRuntimeContext().getBroadcastVariable(<span class="string">"broadcastData"</span>);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> s;</span><br><span class="line">            &#125;</span><br><span class="line">    <span class="comment">// 1. 将需要用的变量广播出去 (这一步可以写在后面)</span></span><br><span class="line">        &#125;).withBroadcastSet(broadcastData, <span class="string">"broadcastData"</span>).print();</span><br></pre></td></tr></table></figure><p>下面以一个获取用户年龄的例子来演示一个常见的使用案例:</p><p>broadcastData 是一个包含用户 (姓名, 年龄) 的数据表</p><p>需要在另外一个算子中通过姓名查找年龄, 那么就需要把上表广播</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BroadcastExample</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">4</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建需要广播的 数据集 (name, age)</span></span><br><span class="line">        Tuple2&lt;String, Integer&gt; john = <span class="keyword">new</span> Tuple2&lt;&gt;(<span class="string">"john"</span>, <span class="number">23</span>);</span><br><span class="line">        Tuple2&lt;String, Integer&gt; tom = <span class="keyword">new</span> Tuple2&lt;&gt;(<span class="string">"tom"</span>, <span class="number">24</span>);</span><br><span class="line">        Tuple2&lt;String, Integer&gt; shiny = <span class="keyword">new</span> Tuple2&lt;&gt;(<span class="string">"shiny"</span>, <span class="number">22</span>);</span><br><span class="line">        DataSource&lt;Tuple2&lt;String, Integer&gt;&gt; broadcastData = env.fromElements(john, tom, shiny);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 新建一个dataset -&gt; d1, 设置并行度为4</span></span><br><span class="line">        <span class="comment">// 此时 d1 是无法访问 broadcastData 的数据的, 因为两个dataset可能不在一个节点或者slot中, 所以 flink 是不允许去访问的</span></span><br><span class="line">        DataSet&lt;String&gt; d1 = env.fromElements(<span class="string">"john"</span>, <span class="string">"tom"</span>, <span class="string">"shiny"</span>).setParallelism(<span class="number">4</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 使用 RichMapFunction, 在open() 方法中拿到广播变量</span></span><br><span class="line">        d1.map(<span class="keyword">new</span> RichMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">            List&lt;Tuple2&lt;String, Integer&gt;&gt; bc;</span><br><span class="line">            HashMap&lt;String, Integer&gt; map = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">super</span>.open(parameters);</span><br><span class="line">                <span class="keyword">this</span>.bc = getRuntimeContext().getBroadcastVariable(<span class="string">"broadcastData"</span>);</span><br><span class="line">                <span class="keyword">for</span> (Tuple2&lt;String, Integer&gt; tp : bc) &#123;</span><br><span class="line">                    <span class="keyword">this</span>.map.put(tp.f0, tp.f1);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                Integer age = <span class="keyword">this</span>.map.get(s);</span><br><span class="line">                <span class="keyword">return</span> s + <span class="string">"-&gt;"</span> + age;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).withBroadcastSet(broadcastData, <span class="string">"broadcastData"</span>).print();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
            <tag> broadcast </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink使用10-通过Bulk iterator计算圆周率</title>
      <link href="/2019/10/02/flink%E4%BD%BF%E7%94%A810-%E9%80%9A%E8%BF%87Bulk-iterator%E8%AE%A1%E7%AE%97%E5%9C%86%E5%91%A8%E7%8E%87/"/>
      <url>/2019/10/02/flink%E4%BD%BF%E7%94%A810-%E9%80%9A%E8%BF%87Bulk-iterator%E8%AE%A1%E7%AE%97%E5%9C%86%E5%91%A8%E7%8E%87/</url>
      
        <content type="html"><![CDATA[<p> 迭代处理是批量处理处理中的常见操作, Flink 的 迭代计算支持两种模式, 分别是 Bulk Iteration (全量迭代计算) 和 Delt Iteration (增量迭代计算). 下面就一个计算圆周率的例子 来说一下使用 Bulk Iteration 都有哪几个步骤.</p><p>在 Bulk Iteration 中, 主要的步骤其实是分为3步, 第一步是指定最大循环次数, 第二步是指定在循环时的一个计算处理的过程, 最后一步就是调用计算过程, 指定结束条件. 具体代码如下所示</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BulkIteration</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">// 获取执行环境</span></span><br><span class="line">        ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"><span class="comment">// 构建输出数据</span></span><br><span class="line">        DataSource&lt;Integer&gt; data = env.fromElements(<span class="number">0</span>);</span><br><span class="line">        <span class="comment">// 1. 指定循环次数</span></span><br><span class="line">        IterativeDataSet&lt;Integer&gt; loop = data.iterate(<span class="number">1000</span>);</span><br><span class="line">        <span class="comment">// 2. 指循环计算过程</span></span><br><span class="line">        MapOperator&lt;Integer, Integer&gt; process = loop.map(<span class="keyword">new</span> MapFunction&lt;Integer, Integer&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Integer <span class="title">map</span><span class="params">(Integer i)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">double</span> x = Math.random();</span><br><span class="line">                <span class="keyword">double</span> y = Math.random();</span><br><span class="line">                <span class="keyword">int</span> result = (x * x + y * y) &lt; <span class="number">1</span> ? <span class="number">1</span> : <span class="number">0</span>;</span><br><span class="line">                <span class="keyword">return</span> i + result;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="comment">// 3. 使用 closeWith 调用计算过程</span></span><br><span class="line">        List&lt;Integer&gt; collect = loop.closeWith(process).collect();</span><br><span class="line">        <span class="comment">// 输出最终结果</span></span><br><span class="line">        <span class="keyword">for</span> (Integer i : collect) &#123;</span><br><span class="line">            System.out.println( i / <span class="number">1000.0</span> * <span class="number">4</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
            <tag> iteration </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink使用09-DataSet初体验之通过Inputformat构建dataSet</title>
      <link href="/2019/09/29/flink%E4%BD%BF%E7%94%A809-DataSet%E5%88%9D%E4%BD%93%E9%AA%8C%E4%B9%8B%E9%80%9A%E8%BF%87Inputformat%E6%9E%84%E5%BB%BAdataSet/"/>
      <url>/2019/09/29/flink%E4%BD%BF%E7%94%A809-DataSet%E5%88%9D%E4%BD%93%E9%AA%8C%E4%B9%8B%E9%80%9A%E8%BF%87Inputformat%E6%9E%84%E5%BB%BAdataSet/</url>
      
        <content type="html"><![CDATA[<p>Flink 提供了一套 DataSet 的 API 来做批处理. 其实 DataSet 的使用方法还是和 DataStream 很相似的, 本章主要是先简单的说一下 DataSet 的基本使用. </p><p>DataSet API 其实和 DataStream ApI 相似, 都是需要创建 ExecutionEnvironment 环境, 然后通过 ExecutionEnvironment 环境提供的方法读取外部数据, 将外部数据转换为 DataSet 数据集, 之后利用 DataSet 提供的 API 进行转换操作, 并处理成最后的结果, 并对结果进行输出.</p><h3 id="DataSources-数据输入"><a href="#DataSources-数据输入" class="headerlink" title="DataSources 数据输入"></a>DataSources 数据输入</h3><p>数据输入共有3种类型的接口, 分别是文件系统类型 / Java Collection 类型 / 以及通用数据类型. 其中前两种其实与 DataStream类型, 在前面的系列文章中已经说过了, 这边主要再说一下 通用数据类型接口怎样使用.</p><p>DataSet ApI 提供了 Inputformat 通用的数据接口, 已接入不同的数据源和格式类型的数据. Inputformat 接口主要分为两种类型: 一种是基于文件类型, 在 DataSet API 对应的 readFile( ) 方法; 另外一种是基于通用数据类型的接口, 例如读取 RDBMS 或者 NoSQL 数据库等.</p><p>下面一个方法就以读取一个csv文件的方式举例, 其中首先定义好了每一行的转换类型, 之后将每一行数据输入都转换为对应的 pojo. 使用 env.createInput() 将 PojoCsvInputFormat 转换为 dataSet.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> &lt;T&gt; <span class="function">DataSet&lt;T&gt; <span class="title">getSource</span><span class="params">(ExecutionEnvironment env, String path, String[] fieldOrder, Class&lt;T&gt; type)</span> <span class="keyword">throws</span> URISyntaxException </span>&#123;</span><br><span class="line">        <span class="comment">// 本地文件路径</span></span><br><span class="line">        URL fileUrl = InputFormatExample.class.getClassLoader().getResource(path);</span><br><span class="line">        Path filePath = Path.fromLocalFile(<span class="keyword">new</span> File(fileUrl.getPath()));</span><br><span class="line">        <span class="comment">// 抽取  TypeInformation，是一个 PojoTypeInfo</span></span><br><span class="line">        PojoTypeInfo&lt;T&gt; pojoType = (PojoTypeInfo&lt;T&gt;) TypeExtractor.createTypeInfo(type);</span><br><span class="line">        <span class="comment">// 由于 Java 反射抽取出的字段顺序是不确定的，需要显式指定下文件中字段的顺序</span></span><br><span class="line">        <span class="comment">// 创建 PojoCsvInputFormat</span></span><br><span class="line">        PojoCsvInputFormat&lt;T&gt; csvInput = <span class="keyword">new</span> PojoCsvInputFormat&lt;&gt;(filePath, pojoType, fieldOrder);</span><br><span class="line">        <span class="keyword">return</span> env.createInput(csvInput, pojoType);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h3 id="DataSet-转换操作"><a href="#DataSet-转换操作" class="headerlink" title="DataSet 转换操作"></a>DataSet 转换操作</h3><p>Flink 提供了丰富的 API 对 dataSet 做转换处理, 例如数据处理(Map / FlatMap / MapPartiton / Filter), 聚合操作(Reduce / ReduceGroup / Aggregate), 多表关联(Join / OuterJoin / Cogroup / Cross), 集合操作 (Union / Rebalance / Hash-Partition / Range-Partition / Sort Partition), 排序操作(first / minBy / maxBy). 具体的API操作太多, 本文就不一一赘述了,这里就将一些 join 方法的使用.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 与流处理的合并类型, 也是依靠 where().equalTO()来实现两个dataSet的判断</span></span><br><span class="line">dataSet1.join(dataSet2).where(<span class="string">"key"</span>).equalTo(<span class="string">"key"</span>).with(&lt;JoinFunction&gt;)</span><br></pre></td></tr></table></figure><p> 下面一个例子展示了如何去使用 join 方法关联两个 dataSet.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 首先使用上面讲到的方法读取csv文件数据,转为DataSet&lt;POJO&gt;</span></span><br><span class="line"><span class="comment">// item dataSet 格式为(id, price)</span></span><br><span class="line">        String itemPath = <span class="string">"item.csv"</span>;</span><br><span class="line">        String[] itemField = <span class="keyword">new</span> String[]&#123;<span class="string">"id"</span>, <span class="string">"price"</span>&#125;; <span class="comment">// java反射会导致乱序,手动指定字段序</span></span><br><span class="line">        DataSet&lt;Item&gt; items = getSource(env, itemPath, itemField, Item.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// info dataSet 格式为(id, color, country)</span></span><br><span class="line">        String infoPath = <span class="string">"info.csv"</span>;</span><br><span class="line">        String[] infoField = <span class="keyword">new</span> String[]&#123;<span class="string">"id"</span>, <span class="string">"color"</span>, <span class="string">"country"</span>&#125;;</span><br><span class="line">        DataSet&lt;Info&gt; infos = getSource(env, infoPath, infoField, Info.class);</span><br><span class="line">        <span class="comment">// 关联两个dataset</span></span><br><span class="line">        JoinOperator.DefaultJoin&lt;Item, Info&gt; dataSet = items.join(infos).where(<span class="string">"id"</span>).equalTo(<span class="string">"id"</span>);</span><br><span class="line">        <span class="comment">// 使用 joinFunction 处理合并后的两个dataSet</span></span><br><span class="line">        dataSet.with(<span class="keyword">new</span> JoinFunction&lt;Item, Info, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> String <span class="title">join</span><span class="params">(Item item, Info info)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">"商品ID:"</span> + item.getId() + <span class="string">" 价格:"</span>+item.getPrice() + <span class="string">" 颜色:"</span>+ info.getColor() + <span class="string">" 国家:"</span> + info.getCountry();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).print();</span><br><span class="line"><span class="comment">// 样例数据结果:↓↓↓↓↓</span></span><br><span class="line"><span class="comment">//商品ID:1 价格:50 颜色:red 国家:china</span></span><br><span class="line"><span class="comment">//商品ID:2 价格:120 颜色:black 国家:usa</span></span><br><span class="line"><span class="comment">//商品ID:3 价格:89 颜色:green 国家:korea</span></span><br></pre></td></tr></table></figure><h3 id="DataSinks-数据输出"><a href="#DataSinks-数据输出" class="headerlink" title="DataSinks 数据输出"></a>DataSinks 数据输出</h3><p>为了能够让用户更灵活的使用外部数据, Flink抽象出通用的 OutputFormat 接口, 批量数据输出全部实现于此接口.</p><p>Flink 内置了常用的数据存储介质对应的接口, 如 TextOutputFormat /CsvOutputFormat / HadoopOutputFormat / JDBCOutputFormat 等. </p><p>Flink 在 DataSet ApI 中的数据输出总共以下3类:</p><h4 id="1-基于文件输出接口"><a href="#1-基于文件输出接口" class="headerlink" title="1. 基于文件输出接口"></a>1. 基于文件输出接口</h4><p>WriteAsText / WriteAsCsv. 可以直接使用这两个方法输出到文件, 用户也可以指定写入文件的模式, 分为 OVERWRITE 模式(覆盖) 和 NOT_OVERWRITE 模式(不覆盖 ). 均可以写到hdfs或者本地</p><h4 id="2-通用数据接口"><a href="#2-通用数据接口" class="headerlink" title="2. 通用数据接口"></a>2. 通用数据接口</h4><p>用户可以自己自定义OutputFormat 方法来定义存储, 例如 HadoopOutputFormat.</p><h4 id="3-客户端输出"><a href="#3-客户端输出" class="headerlink" title="3. 客户端输出"></a>3. 客户端输出</h4><p>如果想在本地调试的话, 那么最简单的方式就是通过 print()的方法直接将flink的数据拉回到client, 然后输出.</p>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flnik </tag>
            
            <tag> dataset </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink使用08-在dataStream中使用AsyncFunction</title>
      <link href="/2019/09/27/flink%E4%BD%BF%E7%94%A808-%E5%9C%A8dataStream%E4%B8%AD%E4%BD%BF%E7%94%A8AsyncFunction/"/>
      <url>/2019/09/27/flink%E4%BD%BF%E7%94%A808-%E5%9C%A8dataStream%E4%B8%AD%E4%BD%BF%E7%94%A8AsyncFunction/</url>
      
        <content type="html"><![CDATA[<p>在流式处理的过程中, 在中间步骤的处理中, 如果涉及到一些费事的操作或者是外部系统的数据交互, 那么就会给整个流造成一定的延迟. 在 flink 的 1.2 版本中引入了 Asynchronous I/O, 能够支持异步的操作, 以提高 flink 系统与外部数据系统交互的性能及吞吐量.</p><p>在使用 Flink 的异步 IO 时, 主要有两个 API可以使用, 一个是AsyncDataStream.unorderedWait( ), 另一个AsyncDataStream.orderedWait( ).在异步处理过程中,原本数据的顺序可能会发生变化, 使用unorderWait的方法, 不会考虑顺序的问题, 一旦处理完成就会直接返回结果, 这种方法具有较低的延迟和负载. 那么orderWait的方法就是想对应的, 严格按照原本流中的数据顺序做返回, 会对系统造成一定的延迟. 实际中应该根据具体的业务情况做选择.unorderedWait或orderedWait有两个关于async operation的参数，一个是timeout参数用于设置async的超时时间，一个是capacity参数用于指定同一时刻最大允许多少个(<code>并发</code>)async request在执行；</p><p>在使用异步IO时,需要自己去继承AsyncFunction,AsyncFunction接口继承了Function，它定义了asyncInvoke方法以及一个default的timeout方法；asyncInvoke方法执行异步逻辑，然后通过ResultFuture.complete将结果或异常设置到ResultFuture，如果异常则通过ResultFuture.completeExceptionally(Throwable)来传递 ResultFuture；RichAsyncFunction继承了AbstractRichFunction，同时声明实现AsyncFunction接口，它不没有实现asyncInvoke，交由子类实现；它覆盖了setRuntimeContext方法，这里使用RichAsyncFunctionRuntimeContext或者RichAsyncFunctionIterationRuntimeContext进行包装.</p><p>下面是一个验证 Async I/O 的demo, 具体代码见仓库 -&gt; <a href="https://github.com/CheckChe0803/flink-simple-tutorial/tree/master/streaming/src/main/java/async" target="_blank" rel="noopener">code link</a></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AsyncIOExample</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        DataStream&lt;String&gt; inp = env.fromElements(AsyncIOData.WORDS);</span><br><span class="line"><span class="comment">// 接收数据</span></span><br><span class="line">        SingleOutputStreamOperator&lt;String&gt; out = inp.map(<span class="keyword">new</span> MapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                System.out.println(<span class="string">"读取数据:"</span> + s + <span class="string">"  当前时间:"</span> + System.currentTimeMillis());</span><br><span class="line">                <span class="keyword">return</span> s;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="comment">// 使用 AsyncFunction 对函数做一个简单的处理, 中间随机睡眠 1-10s</span></span><br><span class="line">        DataStream&lt;String&gt; asyncStream = AsyncDataStream.unorderedWait(out, <span class="keyword">new</span> SimpleAsyncFunction(), <span class="number">20_000L</span>, TimeUnit.MILLISECONDS);</span><br><span class="line"><span class="comment">// 对已经被 AsyncFunction 处理过的数据再输出一次</span></span><br><span class="line">        asyncStream.map(<span class="keyword">new</span> MapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                System.out.println(<span class="string">"数据处理完毕:"</span> + s + <span class="string">"  当前时间:"</span> + System.currentTimeMillis());</span><br><span class="line">                <span class="keyword">return</span> s;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"AsyncFunction Demo"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">SimpleAsyncFunction</span> <span class="keyword">extends</span> <span class="title">RichAsyncFunction</span>&lt;<span class="title">String</span>, <span class="title">String</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">long</span> waitTime;</span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">final</span> Random rnd = <span class="keyword">new</span> Random(hashCode());</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">asyncInvoke</span><span class="params">(String input, ResultFuture&lt;String&gt; resultFuture)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="comment">// 随机睡眠 1 - 10s</span></span><br><span class="line">            System.out.println(<span class="string">"开始 AsyncFunction  target -&gt; "</span> + input);</span><br><span class="line">            waitTime = rnd.nextInt(<span class="number">10</span>);</span><br><span class="line">            Thread.sleep(waitTime * <span class="number">1000</span>);</span><br><span class="line">            String out = input + input;</span><br><span class="line">            resultFuture.complete(Collections.singletonList(out));</span><br><span class="line">            System.out.println(<span class="string">"结束 AsyncFunction  target -&gt; "</span> + input + <span class="string">"  Sleep time = "</span> + waitTime + <span class="string">"s"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>以上代码的输出结果为:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">读取数据:D  当前时间:<span class="number">1569574233046</span></span><br><span class="line">读取数据:C  当前时间:<span class="number">1569574233047</span></span><br><span class="line">读取数据:A  当前时间:<span class="number">1569574233048</span></span><br><span class="line">读取数据:B  当前时间:<span class="number">1569574233049</span></span><br><span class="line">开始 AsyncFunction  target -&gt; D</span><br><span class="line">开始 AsyncFunction  target -&gt; C</span><br><span class="line">开始 AsyncFunction  target -&gt; A</span><br><span class="line">开始 AsyncFunction  target -&gt; B</span><br><span class="line">结束 AsyncFunction  target -&gt; DSleep time = <span class="number">6</span>s</span><br><span class="line">数据处理完毕:DD  当前时间:<span class="number">1569574239065</span></span><br><span class="line">结束 AsyncFunction  target -&gt; CSleep time = <span class="number">6</span>s</span><br><span class="line">数据处理完毕:CC  当前时间:<span class="number">1569574239069</span></span><br><span class="line">结束 AsyncFunction  target -&gt; ASleep time = <span class="number">6</span>s</span><br><span class="line">数据处理完毕:AA  当前时间:<span class="number">1569574239072</span></span><br><span class="line">结束 AsyncFunction  target -&gt; BSleep time = <span class="number">6</span>s</span><br><span class="line">数据处理完毕:BB  当前时间:<span class="number">1569574239076</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
            <tag> async </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink使用07-通过join合并流的操作</title>
      <link href="/2019/09/27/flink%E4%BD%BF%E7%94%A807-%E9%80%9A%E8%BF%87join%E5%90%88%E5%B9%B6%E6%B5%81%E7%9A%84%E6%93%8D%E4%BD%9C/"/>
      <url>/2019/09/27/flink%E4%BD%BF%E7%94%A807-%E9%80%9A%E8%BF%87join%E5%90%88%E5%B9%B6%E6%B5%81%E7%9A%84%E6%93%8D%E4%BD%9C/</url>
      
        <content type="html"><![CDATA[<p>Flink 中支持窗口上的多流合并, 需要保证的是输入的 stream 要构建在相同的 Window 上, 并使用相同类型的 Key 作为关联条件.代码如下所示, 先通过 join 方法将 inputStream1 数据集和 inputStream2 关联, 调用 where( ) 方法指定  inputStream1 的 key, 调用 equalTo( ) 方法指定 inputStream2 对应关联的 key. 通过 window( ) 方法指定 window Assigner, 最后再通过 apply( ) 方法传入用户自定义的 JoinFunction 或者 FlatJoinFunction 对输入的数据元素做窗口计算.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">inputStream1.join(inputStream2)</span><br><span class="line"><span class="comment">// 指定inputStream1的关联key</span></span><br><span class="line">.where(<span class="number">0</span>)</span><br><span class="line"><span class="comment">// 指定inputStream2的关联key</span></span><br><span class="line">.equalTo(<span class="number">1</span>)</span><br><span class="line"><span class="comment">// 指定 window Assigner</span></span><br><span class="line">.window(TumblingEventTimeWindows.of(Time.seconds(<span class="number">10</span>)))</span><br><span class="line"><span class="comment">// 指定窗口计算函数</span></span><br><span class="line">.apply(&lt;JoinFunction&gt;)</span><br></pre></td></tr></table></figure><p>下面就用 flink 官方仓库中的join example来做演示, 完整代码见仓库 -&gt; <a href="https://github.com/CheckChe0803/flink-simple-tutorial/tree/master/streaming/src/main/java/join" target="_blank" rel="noopener">code link</a></p><p><strong>样例中有两个流, 分别记录的是员工的等级和员工的薪水, 流中数据的格式分别是 (name, grade) / (name, salary), 代码实现的功能是合并两个流, 转变为 (name, grade, salary) 格式的流.</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> <span class="keyword">long</span> windowSize = <span class="number">200L</span>;</span><br><span class="line"><span class="keyword">final</span> <span class="keyword">long</span> rate = <span class="number">3L</span>;</span><br><span class="line"></span><br><span class="line">System.out.println(<span class="string">"Using windowSize="</span> + windowSize + <span class="string">", data rate="</span> + rate);</span><br><span class="line">System.out.println(<span class="string">"To customize example, use: WindowJoin [--windowSize &lt;window-size-in-millis&gt;] [--rate &lt;elements-per-second&gt;]"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取env, 配置为"ingestion time"</span></span><br><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 生成 grade 和 salary 两个流 分别是 (name, grade) / (name, salary)</span></span><br><span class="line">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; grades = WindowJoinSampleData.GradeSource.getSource(env, rate);</span><br><span class="line">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; salaries = WindowJoinSampleData.SalarySource.getSource(env, rate);</span><br><span class="line"></span><br><span class="line">DataStream&lt;Tuple3&lt;String, Integer, Integer&gt;&gt; joinedStream = runWindowJoin(grades, salaries, windowSize);</span><br><span class="line"></span><br><span class="line">joinedStream.print().setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">env.execute(<span class="string">"Windowed Join Example"</span>);</span><br></pre></td></tr></table></figure><p>其中, 数据流的添加是通过一个Iterator 不停的添加进去的, 具体的 join 逻辑通过 runWindowJoin( )方法, 以为为该方法的具体内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">public static DataStream&lt;Tuple3&lt;String, Integer, Integer&gt;&gt; runWindowJoin(</span><br><span class="line">            DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; grades,</span><br><span class="line">            DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; salaries,</span><br><span class="line">            long windowSize) &#123;</span><br><span class="line"></span><br><span class="line">        return grades.join(salaries)</span><br><span class="line">                .where(new NameKeySelector())</span><br><span class="line">                .equalTo(new NameKeySelector())</span><br><span class="line"></span><br><span class="line">                .window(TumblingEventTimeWindows.of(Time.milliseconds(windowSize)))</span><br><span class="line"></span><br><span class="line">                .apply(new JoinFunction&lt;Tuple2&lt;String, Integer&gt;, Tuple2&lt;String, Integer&gt;, Tuple3&lt;String, Integer, Integer&gt;&gt;() &#123;</span><br><span class="line"></span><br><span class="line">                    @Override</span><br><span class="line">                    public Tuple3&lt;String, Integer, Integer&gt; join(</span><br><span class="line">                            Tuple2&lt;String, Integer&gt; first,</span><br><span class="line">                            Tuple2&lt;String, Integer&gt; second) &#123;</span><br><span class="line">                        return new Tuple3&lt;String, Integer, Integer&gt;(first.f0, first.f1, second.f1);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
            <tag> join </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink使用06-如何处理窗口内的数据</title>
      <link href="/2019/09/26/flink%E4%BD%BF%E7%94%A806-%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E7%AA%97%E5%8F%A3%E5%86%85%E7%9A%84%E6%95%B0%E6%8D%AE/"/>
      <url>/2019/09/26/flink%E4%BD%BF%E7%94%A806-%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E7%AA%97%E5%8F%A3%E5%86%85%E7%9A%84%E6%95%B0%E6%8D%AE/</url>
      
        <content type="html"><![CDATA[<p>上一节主要是大致介绍了下 flink 的窗口组成, 以及如何去划分窗口的. 那么这一篇文章主要是对剩下的内容做一下总结, 说一下如何对窗口内的数据做处理.  </p><h4 id="Window-Function"><a href="#Window-Function" class="headerlink" title="Window Function"></a>Window Function</h4><p>Window Assigner 的作用是划分窗口的, 而 Window Function 就是对窗口内的数据做处理的一个过程. Flink 提供了 4 种类型的 Window Function, 分别是 ReduceFunction / AggregateFunction / FoldFunction / ProcessWindowFunction. 另外, 这四类还根据计算原理的不同分为增量聚合函数和全量窗口函数. 增量的计算性能比较高, 主要是基于中间状态的计算结果, 窗口中只维护中间结果的状态值.</p><h4 id="1-ReduceFunction-增量"><a href="#1-ReduceFunction-增量" class="headerlink" title="1. ReduceFunction (增量)"></a><strong>1. ReduceFunction</strong> (增量)</h4><p>对输入的两个相同类型的元素按照指定的计算方式进行聚合, 通过实现 ReduceFunction 接口就可以在reduce( ) 函数内部进行聚合操作了.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 将Tuple2 按照 f1 进行 keyBy, 之后将 f0字符合并起来</span></span><br><span class="line">input.keyBy(x -&gt; x.f1)</span><br><span class="line">.timeWindow(Time.seconds(<span class="number">10</span>), Time.seconds(<span class="number">1</span>))</span><br><span class="line">    .reduce(<span class="keyword">new</span> ReduceFunction&lt;Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Long&gt; <span class="title">reduce</span><span class="params">(Tuple2&lt;String, Long&gt; t1, Tuple2&lt;String, Long&gt; t2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(t1.f0 + t2.f0, t1.f1);</span><br><span class="line">        &#125;</span><br><span class="line">        &#125;);</span><br></pre></td></tr></table></figure><p>当然也可以使用匿名函数的方式,写起来会更加简洁.上述代码可以改为:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">input.keyBy(x -&gt; x.f1).timeWindow(Time.seconds(<span class="number">10</span>), Time.seconds(<span class="number">1</span>))</span><br><span class="line">.reduce((t1,t2) -&gt; <span class="keyword">new</span> Tuple2&lt;&gt;(t1.f0 + t2.f0, t1.f1));</span><br></pre></td></tr></table></figure><h4 id="2-AggregateFunction-增量"><a href="#2-AggregateFunction-增量" class="headerlink" title="2. AggregateFunction (增量)"></a><strong>2. AggregateFunction</strong> (增量)</h4><p>AggregateFunction 相对于ReduceFunction更加灵活,但是实现起来也更复杂, AggregateFunction有 4 个需要复写的方法, 其中createAccumulator( ) 定义累加器, add( ) 定义数据的添加逻辑, getResult( ) 定义了根据 accumulator 计算结果的逻辑, merge()方法定义合并 accumulator 的逻辑.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">input.keyBy(x -&gt; x.f1)</span><br><span class="line">    .timeWindow(Time.seconds(<span class="number">10</span>), Time.seconds(<span class="number">1</span>))</span><br><span class="line">    <span class="comment">// 自定义一个AggregateFunciton, 将相同标号 f1 的数据的 f0字符串字段合并在一起</span></span><br><span class="line">    <span class="comment">// ("hello", 1L) + ("world", 1L) = ("hello world", 1L)</span></span><br><span class="line">    .aggregate(<span class="keyword">new</span> MyAggregateFunction());</span><br></pre></td></tr></table></figure><p>通过自定义的 MyAggregateFunction() 来实现 AggregateFunction 接口</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyAggregateFunction</span> <span class="keyword">implements</span> <span class="title">AggregateFunction</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Long</span>&gt;, <span class="title">String</span>, <span class="title">String</span>&gt;</span>&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">createAccumulator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 初始化累加器</span></span><br><span class="line">            <span class="keyword">return</span> <span class="string">""</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">add</span><span class="params">(Tuple2&lt;String, Long&gt; t, String s)</span> </span>&#123;</span><br><span class="line">            <span class="comment">// 输入数据与累加器的合并</span></span><br><span class="line">            <span class="keyword">return</span> s + <span class="string">" "</span> +t.f0;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">getResult</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">            <span class="comment">// 得到累加器的结果</span></span><br><span class="line">            <span class="keyword">return</span> s.trim();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">merge</span><span class="params">(String s, String acc1)</span> </span>&#123;</span><br><span class="line">            <span class="comment">// 合并累加器</span></span><br><span class="line">            <span class="keyword">return</span> s + <span class="string">" "</span> + acc1;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h4 id="3-FoldFunction-增量"><a href="#3-FoldFunction-增量" class="headerlink" title="3. FoldFunction (增量)"></a><strong>3. FoldFunction</strong> (增量)</h4><p>FoldFunction定义了如何将窗口中的输入元素与外部的元素合并的逻辑</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">input.keyBy(x -&gt; x.f1)</span><br><span class="line">.timeWindow(Time.seconds(<span class="number">10</span>), Time.seconds(<span class="number">1</span>)).fold(<span class="string">"flink"</span>, (acc, t) -&gt;t.f0 + acc);</span><br></pre></td></tr></table></figure><p>FoldFunction在新版本已经被标记@Deprecated了, 建议使用AggregateFunction代替</p><h4 id="4-ProcessWindowFunction-全量"><a href="#4-ProcessWindowFunction-全量" class="headerlink" title="4. ProcessWindowFunction (全量)"></a><strong>4. ProcessWindowFunction</strong> (全量)</h4><p>ProcessWindowFunction 相较于其他的 Window Function, 可以实现一些更复杂的计算, 比如基于整个窗口做某些指标计算 或者需要操作窗口中的状态数据和窗口元数据. Flink 提供了 ProcessWindowFunction 这个抽象类, 继承此类就可以实现ProcessWindowFunction, 其中, 必须要实现 process( ) 方法, 这是处理窗口数据的主要方法.还在一下跟窗口数据相关的方法可以有选择的实现.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyProcessWindowFunction</span> <span class="keyword">extends</span> <span class="title">ProcessWindowFunction</span>&lt;<span class="title">Tuple3</span>&lt;<span class="title">String</span>, <span class="title">Long</span>, <span class="title">Long</span>&gt;, <span class="title">String</span>, <span class="title">Long</span>, <span class="title">TimeWindow</span>&gt; </span>&#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(Long s, Context context, Iterable&lt;Tuple3&lt;String, Long, Long&gt;&gt; elements, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">// 统计每个窗口内的所有数据的 f0字段加起来共有多少个单词</span></span><br><span class="line">    <span class="comment">// 也就做单个窗口的 wordcount</span></span><br><span class="line">Long count = <span class="number">0L</span>;</span><br><span class="line"><span class="keyword">for</span> (Tuple3&lt;String, Long, Long&gt; element : elements) &#123;</span><br><span class="line">count += element.f0.split(<span class="string">" "</span>).length;</span><br><span class="line">&#125;</span><br><span class="line">out.collect(<span class="string">"window: "</span> + context.window() + <span class="string">" word count: "</span> + count);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="5-增量与全量共同使用"><a href="#5-增量与全量共同使用" class="headerlink" title="5. 增量与全量共同使用"></a>5. 增量与全量共同使用</h4><p>增量聚合函数虽然性能好, 但是灵活性不如全量函数, 例如对窗口状态数据的操作以及对窗口中的元数据信息的获取. 但是如果用 ProcessWindowFunction 去完成一些基础的增量计算相对比较浪费资源, 因此可以两者结合的方式来实现.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">input.keyBy(x -&gt; x.f1)</span><br><span class="line">.timeWindow(Time.seconds(<span class="number">10</span>), Time.seconds(<span class="number">1</span>))</span><br><span class="line"><span class="comment">// 第一个Function为 ReduceFunction, 取窗口的最小值</span></span><br><span class="line">.reduce((r1, r2) -&gt; &#123;</span><br><span class="line"><span class="keyword">return</span> r1.f0 &lt; r2.f0 ? r1 : r2;</span><br><span class="line"><span class="comment">// 第二个Function为 ProcessWindowFunction, 获取窗口的时间信息</span></span><br><span class="line">&#125;, <span class="keyword">new</span> ProcessWindowFunction&lt;Tuple2&lt;Long, Long&gt;, String, Long, TimeWindow&gt;() &#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(Long aLong, Context context, Iterable&lt;Tuple2&lt;Long, Long&gt;&gt; elements, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">out.collect(<span class="string">"window: "</span> + context.window()); </span><br><span class="line">&#125;</span><br><span class="line">&#125;).print();</span><br></pre></td></tr></table></figure><h3 id="Flink-窗口中的其他组件"><a href="#Flink-窗口中的其他组件" class="headerlink" title="Flink 窗口中的其他组件"></a>Flink 窗口中的其他组件</h3><p>除了 Window Assigner 和 Window Function外,Flink的窗口中还有 Triger窗口触发器, 其负责判断何时将窗口中的数据取出做计算, flink已经默认为各种类型的窗口实现了 triger. 用户也可以自己手动指定. Evictors 是数据剔除器, 目的是把窗口中的数据按照需求做一定的剔除. Flink也有 API 针对延迟数据做处理, 延迟的数据可以丢弃也可以通过sideOutputLateDate( ) 方法处理.</p><h4 id="1-Triger-窗口触发器"><a href="#1-Triger-窗口触发器" class="headerlink" title="1. Triger 窗口触发器"></a>1. Triger 窗口触发器</h4><p><strong>EventTimeTrigger</strong>: 通过对比 watermark 和窗口 EndTime 确定是否触发窗口</p><p><strong>ProcessTimeTrigger</strong>: 通过对比 ProcessTime 和窗口 EndTime 确定是否触发窗口</p><p><strong>ContinuousEventTimeTrigger</strong>: 根据间隔时间周期性触发窗口</p><p><strong>ContinuousEventTimeTrigger</strong>: 同上, 区别是使用ProcessTime</p><p><strong>CountTrigger</strong>: 根据接入数量是否超过阈值</p><p><strong>DeltaTrigger</strong>: 根据计算出来的 Delta 指标是否超过指定的 Threshold</p><p><strong>PurgingTrigger</strong>: 可以将任意触发器作为参数转换为Purge类型触发器</p><h4 id="2-Evictors触发器"><a href="#2-Evictors触发器" class="headerlink" title="2. Evictors触发器"></a>2. Evictors触发器</h4><p>CountEvictor: 保持固定数量的数据, 超过的剔除</p><p>DeltaEvictor: 通过定义 delta 和 threshold , 计算两个数据之间的 delta 值, 超过则剔除</p><p>TimeEvictor: 指定时间间隔, 将当前窗口中的最新元素的时间减去Interval, 然后将小于该结果的数据全部剔除</p>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
            <tag> window </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink使用05-窗口简介和简单的使用</title>
      <link href="/2019/09/25/flink%E4%BD%BF%E7%94%A805-%E7%AA%97%E5%8F%A3%E7%AE%80%E4%BB%8B%E5%92%8C%E7%AE%80%E5%8D%95%E7%9A%84%E4%BD%BF%E7%94%A8/"/>
      <url>/2019/09/25/flink%E4%BD%BF%E7%94%A805-%E7%AA%97%E5%8F%A3%E7%AE%80%E4%BB%8B%E5%92%8C%E7%AE%80%E5%8D%95%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<h3 id="1-窗口组成简介"><a href="#1-窗口组成简介" class="headerlink" title="1. 窗口组成简介"></a>1. 窗口组成简介</h3><p>窗口是流式计算中非常重要的一个概念, 很多常见的功能都是通过各种窗口实现的, 比如每5分钟统计一下刚去1小时的热度. Flink DataStream API 将窗口独立成 Operator. 每个窗口算子包含了以下几个部分:</p><h4 id="Windows-Assigner"><a href="#Windows-Assigner" class="headerlink" title="Windows Assigner"></a>Windows Assigner</h4><p>指定窗口的类型, 定义如何将数据流分配到一个或者多个窗口</p><h4 id="Windows-Trigger"><a href="#Windows-Trigger" class="headerlink" title="Windows Trigger"></a>Windows Trigger</h4><p>指定窗口触发的时机, 定义窗口满足什么样的条件触发计算</p><h4 id="Evictor"><a href="#Evictor" class="headerlink" title="Evictor"></a>Evictor</h4><p>用户数据剔除</p><h4 id="Lateness"><a href="#Lateness" class="headerlink" title="Lateness"></a>Lateness</h4><p>标记是否处理迟到的数据, 当迟到数据到达窗口中是否触发计算</p><h4 id="Output-Tag"><a href="#Output-Tag" class="headerlink" title="Output Tag"></a>Output Tag</h4><p>标记输出标签, 然后再通过 getSideOutput 将窗口中的数据根据标签输出</p><h4 id="Windows-Function"><a href="#Windows-Function" class="headerlink" title="Windows Function"></a>Windows Function</h4><p>定义窗口上的数据处理的逻辑, 例如对数据进行sum</p><hr><h3 id="2-Window-Assigner"><a href="#2-Window-Assigner" class="headerlink" title="2. Window Assigner"></a>2. Window Assigner</h3><p>首先最需要了解的就是 windows Assigner了, 我们想要一个什么样的窗口划分, 主要就是通过他来实现的. </p><p>根据 flink 上游的数据集是否为 KeyedStream 类型 来做分别的处理. 如果使用了keyBy( ) 则对应使用window( ) 来处理, 否则可以使用 windowAll( )来使用</p><p>Flink 可以支持两种类型的窗口, 分别是基于时间的窗口和基于数量的窗口.基于时间的意思就是按照时间去划分窗口,同理,基于数量的也是根据窗口中的数量来做切分的. 对应的分别就是 timeWindow() 和 countWindow() 来使用, 下面的示例主要使用 timeWindow() 来演示.</p><p>对于不同的 Window Assigner, 还可以把窗口划分为4大类, 分别是 滚动窗口(Tumbling Windows) / 滑动窗口(Sliding Window) / 会话窗口(Session Window) 和 全局窗口(Global Window).</p><h4 id="滚动窗口"><a href="#滚动窗口" class="headerlink" title="滚动窗口"></a>滚动窗口</h4><p>DataStream API 提供基于 EventTime 和 ProcessingTime 的两种类型的 Tumbling window.对应的 Assigner 分别是 TumblingEventTimeWindow 和 ProcessingEventTimeWindow . 举例如下,完整代码见Github.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 使用ProcessTime的滚动时间窗口, 长度为10s</span></span><br><span class="line">stream.keyBy(x -&gt; x.f1)</span><br><span class="line">    .window(TumblingProcessingTimeWindows.of(Time.seconds(<span class="number">10</span>))).process(...)</span><br><span class="line"><span class="comment">// 使用ProcessTime的滚动时间窗口, 长度为10s</span></span><br><span class="line">stream.keyBy(x -&gt;x.f1).window(TumblingEventTimeWindows.of(Time.seconds(<span class="number">10</span>))).process(...)</span><br></pre></td></tr></table></figure><p>使用 window(TumblingProcessingTimeWindows.of(Time.seconds(10))) 的方法有点啰嗦, Flink 还提供了timeWindow( ) 的 API 来简化这一行代码. </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 直接使用 timeWindow API 便可实现滚动窗口的操作, 参数依旧是窗口的长度</span></span><br><span class="line"><span class="comment">// 窗口类型的时间由 time characteristic 确定, 如果指定为 event time,那么窗口也会自动用这个时间</span></span><br><span class="line">input.keyBy(x -&gt; x.f1).timeWindow(Time.seconds(<span class="number">10</span>));</span><br></pre></td></tr></table></figure><h4 id="滑动窗口"><a href="#滑动窗口" class="headerlink" title="滑动窗口"></a>滑动窗口</h4><p>滑动窗口顾名思义就是一个在不断往后滑动的窗口, 比如说 每5分钟 统计一个 最近一小时的时间, 那么就需要用滑动窗口来做处理. 滑动窗口主要是依靠 window size 和 slide time 来确定. 与滚动窗口类似的, flink 也提供了对应不同时间的 Assigner API(SlidingEventTimeWindow / SlidingEventTimeWindow), 语法基本类似, 只是由原本的一个参数(窗口长度) 变为了两个参数(窗口长度和滑动时间), 同样的, 为了简化代码, 依然可以使用timeWindow() 来简化.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 两个参数分别是 窗口长度 和 滑动时间, 窗口时间类型依旧通过time characteristic 确定</span></span><br><span class="line">input.keyBy(x -&gt; x.f1).timeWindow(Time.seconds(<span class="number">10</span>), Time.seconds(<span class="number">1</span>))</span><br></pre></td></tr></table></figure><h4 id="会话窗口"><a href="#会话窗口" class="headerlink" title="会话窗口"></a>会话窗口</h4><p>会话窗口主要是将某段时间内活跃度较高的数据聚合成一个窗口计算. 触发条件是 Session Gap. 在规定的时间内没有数据接入则认为这个窗口结束,然后触发窗口计算. Session Gap 除了固定间隔的方式, 也可以动态抽取.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建 Session Window, 间隔为 3s</span></span><br><span class="line">        DataStream&lt;Tuple3&lt;String, Long, Integer&gt;&gt; aggregated = source</span><br><span class="line">                .keyBy(<span class="number">0</span>)</span><br><span class="line">                .window(EventTimeSessionWindows.withGap(Time.seconds(<span class="number">3L</span>)))</span><br><span class="line">                .sum(<span class="number">2</span>);</span><br></pre></td></tr></table></figure><h4 id="全局窗口"><a href="#全局窗口" class="headerlink" title="全局窗口"></a>全局窗口</h4><p>全局窗口将所有key的数据分配到单个窗口中计算结果.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建 GlobalWindow</span></span><br><span class="line">        input.keyBy(<span class="number">1</span>)</span><br><span class="line">                .window(GlobalWindows.create())</span><br><span class="line">                .sum(<span class="number">1</span>);</span><br></pre></td></tr></table></figure><p><strong>上面就是构建不同的窗口的方法了, 下文会介绍在有了窗口之后怎样对窗口中的数据做处理</strong></p>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
            <tag> window </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink使用04-几种时间概念和watermark</title>
      <link href="/2019/09/24/flink%E4%BD%BF%E7%94%A804-%E5%87%A0%E7%A7%8D%E6%97%B6%E9%97%B4%E6%A6%82%E5%BF%B5%E5%92%8Cwatermark/"/>
      <url>/2019/09/24/flink%E4%BD%BF%E7%94%A804-%E5%87%A0%E7%A7%8D%E6%97%B6%E9%97%B4%E6%A6%82%E5%BF%B5%E5%92%8Cwatermark/</url>
      
        <content type="html"><![CDATA[<h3 id="时间概念"><a href="#时间概念" class="headerlink" title="时间概念"></a>时间概念</h3><p>在做实时计算的时候, 首先就需要搞清楚一个问题, 这个实时到底是怎么样的一个时间概念. 在 Flink 中, 总共有3种时间概念, 分别是 <strong>事件时间</strong> ( Event time ) / <strong>处理时间</strong> ( Processing time ) / <strong>接入时间</strong> ( Ingestion time).</p><p><img src="/2019/09/24/flink使用04-几种时间概念和watermark/%E6%97%B6%E9%97%B4%E7%9A%84%E6%A6%82%E5%BF%B5.png" alt></p><p><strong>事件时间</strong> ( Event time )就是真实的用户发生操作的时候所产生的时间, 对应到 flink 中, 需要用户 <strong>显示</strong> 的告诉 flink 到底每个输入中的哪一个字段代表这个事件时间。</p><p><strong>接入时间</strong> ( Ingestion time) 和<strong>处理时间</strong> ( Processing time )是不需要用户去指定的, flink自己会去处理这个时间. 接入时间的代表的是一个事件通过 source Operator 的时间, 相比于 event time, ingestion time 不能处理乱序事件, 因此也就不用生成对应的watermark. 处理时间是指事件在操作算子计算过程中获取到的所在主机的时间. processing time 适合用于时间计算精度要求不是特别高的计算场景, 例如统计某些延时非常高的日志数据.</p><hr><h3 id="水位线机制-watermark"><a href="#水位线机制-watermark" class="headerlink" title="水位线机制 watermark"></a>水位线机制 watermark</h3><h4 id="1-解释-watermark"><a href="#1-解释-watermark" class="headerlink" title="1, 解释 watermark"></a>1, 解释 watermark</h4><p>watermark 这个概念在 flink 中是与 event time 这个时间概念相互依存的, 其目的是为了解决数据乱序到达和系统延迟的问题. flink会把读取进系统的最新事件时间减去固定的时间间隔作为 watermark. 还是用一张图来解释watermark 的作用.</p><p>当事件进入 flink 中的时候, 根据提取的 event time 产生 watermark 时间戳, 记为 X, 进入 flink 中的 event time 记为 Y. 当窗口的 end time &lt; X 的时候, 则触发窗口计算结果并输出. 只要 X &lt; end time, 那么 事件就可以 一直进入到当前窗口中, 这样的话即便发生乱序, 也可以在窗口中调整. 调整的方法就是按照 Y. </p><p><img src="/2019/09/24/flink使用04-几种时间概念和watermark/watermark.gif" alt></p><h4 id="2-使用-watermark"><a href="#2-使用-watermark" class="headerlink" title="2, 使用 watermark"></a>2, 使用 watermark</h4><p> a. 在 Source Function 中 直接指定 Timestamps 和 Watermark</p><p>​    用户需要复写 SourceFunction 接口中 run( ) 方法实现数据逻辑, 同时调用 SourceContext 的 collectWithTimestamp( ) 方法生成 event time 时间戳, 调用 emitWatermark( ) 方法生成 watermark.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;String&gt; text = env.addSource(<span class="keyword">new</span> SourceFunction&lt;String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;String&gt; ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">for</span> (String s : elementInput) &#123;</span><br><span class="line">                    <span class="comment">// 切割每一条数据</span></span><br><span class="line">                    String[] inp = s.split(<span class="string">","</span>);</span><br><span class="line">                    Long timestamp = <span class="keyword">new</span> Long(inp[<span class="number">1</span>]);</span><br><span class="line">                    <span class="comment">// 生成 event time 时间戳</span></span><br><span class="line">                    ctx.collectWithTimestamp(s, timestamp);</span><br><span class="line">                    <span class="comment">// 调用 emitWatermark() 方法生成 watermark, 最大延迟设定为 2</span></span><br><span class="line">                    ctx.emitWatermark(<span class="keyword">new</span> Watermark(timestamp - <span class="number">2</span>));</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// 设定默认 watermark</span></span><br><span class="line">                ctx.emitWatermark(<span class="keyword">new</span> Watermark(Long.MAX_VALUE));</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br></pre></td></tr></table></figure><p>b. 通过 Flink 自带的 Timestamp Assigner 指定 Timestamp 和 生成 watermark </p><p>在使用了 flink 定义的外部数据源( 如 kafka) 之后, 就不能通过自定义 sourcefunction 的方式来生成 watermark 和 event time 了, 这个时候可以使用 Timestamp Assigner,  其需要在第一个时间相关的 Operator前使用. Flink 有自己定义好的 Timestamp Assigner 可以直接使用 (包括直接指定的方式和固定时间延迟的方式 ).Flink 将 watermark 分为 Periodic Watermarks (根据设定的时间间隔周期性的生成) 和 Punctuated Watermarks (根据接入数量生成), 用户也可以继承对应的类实现这两种 watermark.</p><p>b.1 使用 Ascending Timestamp Assigner 指定 Timestamps 和 Watermark</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 首先需要指定系统时间概念为 event time</span></span><br><span class="line">env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span><br><span class="line"><span class="comment">// 使用 Ascending 分配 时间信息和 watermark</span></span><br><span class="line">   DataStream&lt;Tuple2&lt;String, Long&gt;&gt; text = env.fromCollection(collectionInput);</span><br><span class="line">   text.assignTimestampsAndWatermarks(<span class="keyword">new</span> AscendingTimestampExtractor&lt;Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">   <span class="meta">@Override</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractAscendingTimestamp</span><span class="params">(Tuple2&lt;String, Long&gt; element)</span> </span>&#123;</span><br><span class="line">   <span class="keyword">return</span> element.f1;</span><br><span class="line">   &#125;</span><br><span class="line">   &#125;);</span><br></pre></td></tr></table></figure><p>b.2 使用固定时延间隔的 Timestamp Assigner 指定</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 使用 Ascending 分配 时间信息和 watermark 设定10s 代表最长的时延</span></span><br><span class="line">    DataStream&lt;Tuple2&lt;String, Long&gt;&gt; text = env.fromCollection(collectionInput);</span><br><span class="line">    text.assignTimestampsAndWatermarks(<span class="keyword">new</span> BoundedOutOfOrdernessTimestampExtractor&lt;Tuple2&lt;String, Long&gt;&gt;(Time.seconds(<span class="number">10</span>)) &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Tuple2&lt;String, Long&gt; element)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> element.f1;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure><p>c. 自定义 Timestamp Assigner 和 Watermark Generator</p><p>用户可以自定义实现 AssignerWithPeriodicWatermarks 和 AssignerWithPunctuatedWatermarks 两个接口来分别生成对应的两种 watermark. 这一块用的比较少, 以后有机会再细写.</p>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
            <tag> time </tag>
            
            <tag> watermark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink使用03-数据输入的几种不同方法</title>
      <link href="/2019/09/04/flink%E4%BD%BF%E7%94%A803-%E6%95%B0%E6%8D%AE%E8%BE%93%E5%85%A5%E7%9A%84%E5%87%A0%E7%A7%8D%E4%B8%8D%E5%90%8C%E6%96%B9%E6%B3%95/"/>
      <url>/2019/09/04/flink%E4%BD%BF%E7%94%A803-%E6%95%B0%E6%8D%AE%E8%BE%93%E5%85%A5%E7%9A%84%E5%87%A0%E7%A7%8D%E4%B8%8D%E5%90%8C%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h3 id="flink的数据输入源主要分为两大类"><a href="#flink的数据输入源主要分为两大类" class="headerlink" title="flink的数据输入源主要分为两大类:"></a>flink的数据输入源主要分为两大类:</h3><h4 id="1-内置数据源"><a href="#1-内置数据源" class="headerlink" title="1. 内置数据源"></a>1. 内置数据源</h4><ul><li><p>集合数据源</p><p>可以将数组或者集合作为 flink 的数据源,分别有不同的方法可以使用, 这种方式比较适合本地调试使用</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// 添加数组作为数据输入源</span><br><span class="line">String[] elementInput = new String[]&#123;&quot;hello Flink&quot;, &quot;Second Line&quot;&#125;;</span><br><span class="line">DataStream&lt;String&gt; text = env.fromElements(elementInput);</span><br><span class="line"></span><br><span class="line">// 添加List集合作为数据输入源</span><br><span class="line">List&lt;String&gt; collectionInput = new ArrayList&lt;&gt;();</span><br><span class="line">collectionInput.add(&quot;hello Flink&quot;);</span><br><span class="line">DataStream&lt;String&gt; text2 = env.fromCollection(collectionInput);</span><br></pre></td></tr></table></figure></li><li><p>Socket数据源</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">// 添加Socket作为数据输入源</span><br><span class="line">// 4个参数 -&gt; (hostname:Ip地址, port:端口, delimiter:分隔符, maxRetry:最大重试次数)</span><br><span class="line">DataStream&lt;String&gt; text3 = env.socketTextStream(&quot;localhost&quot;, 9999, &quot;\n&quot;, 4);</span><br></pre></td></tr></table></figure></li><li><p>文件数据源</p><p>可以使用 readTextFile 方法直接读取文本文件, 这种方式可以用来监控一下 log 日志文件, 也可以使用 readFile 方法通过指定 InputFormat 来读取特定数据类型的文件, InputFormat可以是内置类,如 CsvInputFormat 或者用户自定义 InputFormat 接口类.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">// 添加文件源</span><br><span class="line">// 直接读取文本文件</span><br><span class="line">DataStream&lt;String&gt; text4 = env.readTextFile(&quot;/opt/history.log&quot;);</span><br><span class="line"></span><br><span class="line">// 指定 CsvInputFormat, 监控csv文件(两种模式), 时间间隔是10ms</span><br><span class="line">        DataStream&lt;String&gt; text5 = env.readFile(new CsvInputFormat&lt;String&gt;(new Path(&quot;/opt/history.csv&quot;)) &#123;</span><br><span class="line">            @Override</span><br><span class="line">            protected String fillRecord(String s, Object[] objects) &#123;</span><br><span class="line">                return null;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;,&quot;/opt/history.csv&quot;, FileProcessingMode.PROCESS_CONTINUOUSLY,10);</span><br></pre></td></tr></table></figure><p>在 readFile() 方法中有一项参数为 WatchType, 共有两种模式 (PROCESS_CONTINUOUSLY / PROCESS_ONCE). 在 PROCESS_CONTINUOUSLY  模式下, 检测到文件变动就会将文件全部内容加载在 flink, 在 PROCESS_ONCE 模式下, 只会将文件变动的那部分加载到 flink.  </p></li></ul><h4 id="2-外部数据源"><a href="#2-外部数据源" class="headerlink" title="2. 外部数据源"></a>2. 外部数据源</h4><p>外部数据源是重头戏, 一般来说项目中均是使用外部数据源作为数据的源头, flink 通过实现 SourceFunction 定义了非常丰富的第三方数据连接器</p><ul><li><p>数据源连接器</p><p>对于第三方数据源, flink的支持分为三种,有只读型(Twitter Streaming API / Netty ), 只写型( Cassandra / Elasticsearch / hadoop FileSystem), 支持读写(Kafka / Amazon Kinesis / RabbitMQ)</p><p>Apache Kafka (Source / Sink)</p><p>Apache Cassandra (Sink)</p><p>Amazon Kinesis Streams (Source / Sink)</p><p>Elasticsearch (Sink)</p><p>Hadoop FileSystem (Sink)</p><p>RabbitMQ (Source / Sink)</p><p>Apache NiFI (Source / Sink)</p><p>Twitter Streaming API (Source)</p><p><strong>Apache Bahir 中的连接器</strong>:</p><p>Apache ActiveMQ (Source / Sink)</p><p>Apache Flume (Sink)</p><p>Redis (Sink)</p><p>Akka (Sink)</p><p>Netty (Source)</p></li></ul><p><strong>以Kafka 为例 做演示</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 配置 kafka 连接参数</span></span><br><span class="line">String topic = <span class="string">"topic_name"</span>;</span><br><span class="line">String bootStrapServers = <span class="string">"localhost:9092"</span>;</span><br><span class="line">String zkConnect = <span class="string">"localhost:2181"</span>;</span><br><span class="line">String groupID = <span class="string">"group_A"</span>;</span><br><span class="line">Properties prop = <span class="keyword">new</span> Properties();</span><br><span class="line">prop.setProperty(<span class="string">"bootstrap.servers"</span>, bootStrapServers);</span><br><span class="line">prop.setProperty(<span class="string">"zookeeper.connect"</span>, zkConnect);</span><br><span class="line">prop.setProperty(<span class="string">"group.id"</span>, groupID);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建 kafka connector source</span></span><br><span class="line">FlinkKafkaConsumer010&lt;String&gt; consumer010 = <span class="keyword">new</span> FlinkKafkaConsumer010&lt;&gt;(topic, <span class="keyword">new</span> SimpleStringSchema(), prop);</span><br><span class="line"></span><br><span class="line"><span class="comment">// add source</span></span><br><span class="line">DataStreamSource&lt;String&gt; dataStream = env.addSource(consumer010);</span><br></pre></td></tr></table></figure><ul><li><p>自定义数据源连接器</p><p>用户也可以自己定义连接器, 通过实现 SourceFunction 定义单个线程的接入的数据连接器, 也可以通过实现ParallelSourceFunction 接口或者继承 RichParallelSourceFunction 类定义并发数据源接入器.</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
            <tag> dataSource </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink使用02-从WordCount开始</title>
      <link href="/2019/09/03/flink%E4%BD%BF%E7%94%A802-%E4%BB%8EWordCount%E5%BC%80%E5%A7%8B/"/>
      <url>/2019/09/03/flink%E4%BD%BF%E7%94%A802-%E4%BB%8EWordCount%E5%BC%80%E5%A7%8B/</url>
      
        <content type="html"><![CDATA[<p>相信大家在学习spark的时候接触的第一个案例肯定也是 wordCount, 本文也想通过这样一个简单的例子来讲一下一个简单的 flink 程序是什么样子的, 让大家对 flink 的代码有一个简单的了解.</p><p>一个 flink程序主要分为5个部分:</p><ul><li><strong>1. 获取执行 Environment</strong></li></ul><p>environment 提供控制 job 执行的方法(例如设置并行度/容错/checkpoint 参数) 并且与外部系统做交互. flink可以做流计算也可以做批计算, 对应的也就有不同的environment , 在使用时根据不同的使用场景选择即可.</p><ul><li><p><strong>2. 获取输入流 Source</strong></p><p>一个流式计算框架自然是少不了数据的输入, 在 streamExecutionEnvironment 的可以看到有很多种创建输入流的方式, 不过在项目中使用最多的还是使用 addSource()方法来添加不同的数据源接入</p><p><img src="/2019/09/03/flink使用02-从WordCount开始/%E6%95%B0%E6%8D%AE%E6%BA%90%E6%8E%A5%E5%85%A5%E6%96%B9%E6%B3%95.png" alt></p></li><li><p><strong>3. 执行计算 Operator</strong></p><p>在spark中,对数据的转换计算是通过 action 算子和 transformation 算子来对 RDD 中的数据来进行操作, 而在flink中, 主要是通过 Operator来对一个流做处理, 通过一个 Operator 可以将一个流转换为另外一个流, flink中内置了很多算子来实现Operator操作.</p></li><li><p><strong>4. 输入结果 Sink</strong></p><p>在完成数据计算之后,就需要有一个输出的地方, 通常来讲也是通过 addSink() 方法来添加不同的数据输出目标,也可以通过 print() 来直接查看输出或者写入到csv等外部文件.</p></li><li><p><strong>5. 启动 flink,提交 job</strong></p><p>一个 flink 代码的启动执行, 必须通过 env.executor() 方法.这行代码主要做了以下事情:</p><ol><li>生成StreamGraph </li><li>生成JobGraph. </li><li>生成一系列配置</li><li>将 JobGraph和配置交给 flink 集群去运行</li><li>以本地模式运行的话,可以看到启动过程,如启动能量度,web模块,jobManager,ResourceManager,taskManager等等</li><li>启动任务</li></ol></li></ul><h4 id="以下为简单的-WordCount-代码"><a href="#以下为简单的-WordCount-代码" class="headerlink" title="以下为简单的 WordCount 代码"></a>以下为简单的 WordCount 代码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取 StreamEnv</span></span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取 输入流</span></span><br><span class="line">        DataStream&lt;String&gt; text = env.fromElements(WordCountData.WORDS);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 执行计算Operator</span></span><br><span class="line">        DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; counts</span><br><span class="line">                = text.flatMap(<span class="keyword">new</span> SplitFunction())</span><br><span class="line">                .keyBy(<span class="number">0</span>).sum(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 输出结果</span></span><br><span class="line">        counts.print();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 启动flink程序</span></span><br><span class="line">        env.execute(<span class="string">"WordCount Demo"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// *************************************************************************</span></span><br><span class="line">    <span class="comment">// 自定义切割Function切分一行输入</span></span><br><span class="line">    <span class="comment">// *************************************************************************</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">SplitFunction</span> <span class="keyword">implements</span> <span class="title">FlatMapFunction</span>&lt;<span class="title">String</span>, <span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Integer</span>&gt;&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String s, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            String[] words = s.toLowerCase().split(<span class="string">" "</span>);</span><br><span class="line">            <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">                <span class="keyword">if</span> (word.length() &gt; <span class="number">0</span>)&#123;</span><br><span class="line">                    collector.collect(<span class="keyword">new</span> Tuple2&lt;&gt;(word, <span class="number">1</span>));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink使用01-本系列简介</title>
      <link href="/2019/09/03/flink%E4%BD%BF%E7%94%A801-%E6%9C%AC%E7%B3%BB%E5%88%97%E7%AE%80%E4%BB%8B/"/>
      <url>/2019/09/03/flink%E4%BD%BF%E7%94%A801-%E6%9C%AC%E7%B3%BB%E5%88%97%E7%AE%80%E4%BB%8B/</url>
      
        <content type="html"><![CDATA[<p>​    Flink 是一款能够同时支持高吞吐/低延迟/高性能的分布式处理框架.</p><p>​    本系列叫做 &lt;Flink简易使用教程&gt;,  目的是记录自己学习 flink 的过程,并且把使用flink的方方面面介绍给大家.尽量用简单的话把使用方法说清楚,在使用某个具体功能的时候能够快速的查找到该使用方法.</p><p>​    本系列的主要例子会从 flink 官方仓库的 example 出发, 通过这些代码来使用 flink 的一些基本操作.</p><p><img src="/2019/09/03/flink使用01-本系列简介/flinkExample.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
