<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>flink使用12-用 table API 实现WordCount</title>
      <link href="/2019/10/08/flink%E4%BD%BF%E7%94%A812-%E7%94%A8-table-API-%E5%AE%9E%E7%8E%B0WordCount/"/>
      <url>/2019/10/08/flink%E4%BD%BF%E7%94%A812-%E7%94%A8-table-API-%E5%AE%9E%E7%8E%B0WordCount/</url>
      
        <content type="html"><![CDATA[<p>Table API 是 Flink 构建在 DataSet 和 DataStream API 之上的一套结构化编程接口. 本文希望通过一个简单的 wordCount 的例子首先来体验一下普通的 Flink Table 的代码是由哪些部分构成的.</p><ol><li><p><strong>获取 TableEnvironment</strong></p><p>ExecutionEnvironment 是必不可少的, 不管是Stream API 还是 batch API 都需要一个Environment来管理程序, TableEnvironment 也是在使用 Table API 时首先需要创建的, 它提供了注册内部表/ 执行 Flink SQL 语句/ 注册自定义函数等多种功能. 要获取 TableEnvironment, 首先需要根据情况先创建 DataSet 或者 DataStream 的 Environment, 之后再转换为 TableEnvironment.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 以获取 batch Table API 为例, stream类似</span></span><br><span class="line">ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">BatchTableEnvironment tEnv = BatchTableEnvironment.create(env);</span><br></pre></td></tr></table></figure></li><li><p><strong>拿到 Table</strong></p><p>在以 DataSet API 或者 DataStream API 操作时, 一般有通过集合 / 文件 / socket / 外部数据源 等多种方式来将数据输入. 在 Table API 中, 同样也可以以多种不同的形式去创建一个 Table, 只不过过程相对复杂一些. 拿到一张表的方式有多种, 有通过 Table Descriptor / 用户自定义 Table Source / 由DataStream或者DataSet转换的形式等来实现. 具体的实现操作以后的文章再来详细展示, 下面就以通过 DataSet转换的方式来简单完成.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// WC 为自定义的 POJO 来统计单词的信息. 首先构建DataSet</span></span><br><span class="line">DataSet&lt;WC&gt; input = env.fromElements(</span><br><span class="line">                <span class="keyword">new</span> WC(<span class="string">"Hello"</span>, <span class="number">1</span>),</span><br><span class="line">                <span class="keyword">new</span> WC(<span class="string">"flink"</span>, <span class="number">1</span>),</span><br><span class="line">                <span class="keyword">new</span> WC(<span class="string">"Hello"</span>, <span class="number">1</span>));</span><br><span class="line"><span class="comment">// 直接将 DataSet 转换为 Table</span></span><br><span class="line">Table table = tEnv.fromDataSet(input);</span><br></pre></td></tr></table></figure></li><li><p><strong>数据转换操作</strong></p><p>拿到了 Table 之后, 就可以正常的进行数据转换的操作了, 如底层的 API一样, 常见的操作 Table API 都已经实现好了, 包括 <strong>数据查询和过滤</strong> / <strong>窗口操作</strong> / <strong>聚合操作</strong> / <strong>多表关联</strong> / <strong>集合操作</strong> / <strong>排序操作</strong> / <strong>数据写入</strong>. 但是具体的 API语法还是与底层API有很大的不同的, 目的就是为了更方便更类似SQL的形式方便用户去写业务逻辑, 下面就展示一下如何实现 wordCount这个一个步骤.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Table filtered = table</span><br><span class="line">                .groupBy(<span class="string">"word"</span>)</span><br><span class="line">    <span class="comment">// 在 select 方法中可以方便的直接使用类sql语句进行操作</span></span><br><span class="line">                .select(<span class="string">"word, frequency.sum as frequency"</span>)</span><br><span class="line">                .filter(<span class="string">"frequency = 2"</span>);</span><br></pre></td></tr></table></figure></li><li><p><strong>数据输出</strong></p><p>数据处理完毕之后最后一步就是结果的输出, 这一步与底层的API也是大同小异的, 可以将结果直接insetInto()到其他在TableEnvironment注册的表中, 也可以将处理完成的结果转换为 DataSet 或者是 DataStream 亦或是通过自定义的 sink 输出. </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1. 直接输入到其他表中</span></span><br><span class="line">table.insertInto(<span class="string">"otherTbale"</span>);</span><br><span class="line"><span class="comment">// 2. 转换为 DataSet / DataStream</span></span><br><span class="line">DataSet&lt;WC&gt; result = tEnv.toDataSet(filtered, WC.class);</span><br><span class="line"><span class="comment">// 3. 自定义 sink</span></span><br><span class="line">CsvTableSink tableSink = <span class="keyword">new</span> CsvTableSink(path, <span class="string">","</span>);</span><br><span class="line">tEnv.registerTableSink(<span class="string">"csvSink"</span>, tableSink);</span><br></pre></td></tr></table></figure></li></ol><p>通过上面几步, 一个完整的 Flink Table API 的程序就构建完毕了, 有了前面学习 flink 的经验, 这部分其实是水到渠成的, 稍微了解下就可以上手了, 本文对应的 wordCount 案例在 <a href="https://github.com/CheckChe0803/flink-simple-tutorial/blob/master/table/src/main/java/wordCount/WordCountTable.java" target="_blank" rel="noopener">github</a> 中, 在后面的部分会继续讲解一下其他 Table API 的内容.</p>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
            <tag> table </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink使用11-了解broadcast的用法</title>
      <link href="/2019/10/02/flink%E4%BD%BF%E7%94%A811-%E4%BA%86%E8%A7%A3broadcast%E7%9A%84%E7%94%A8%E6%B3%95/"/>
      <url>/2019/10/02/flink%E4%BD%BF%E7%94%A811-%E4%BA%86%E8%A7%A3broadcast%E7%9A%84%E7%94%A8%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>在Flink中，同一个算子可能存在若干个不同的并行实例，计算过程可能不在同一个Slot中进行，不同算子之间更是如此，因此不同算子的计算数据之间不能像Java数组之间一样互相访问，而广播变量<code>Broadcast</code>便是解决这种情况的. 在 flink 中, 针对某一个算子需要使用公共变量的情况下, 就可以把对应的数据给广播出去, 这样在所有的节点中都可以使用了. 典型的代码结构如下所示:</p><p>在一个算子中使用广播变量主要有两个步骤:</p><ol><li><p><strong>广播变量</strong> (一般写在算子的后面即可) </p><p>使用 withBroadcastSet(data, “name”) 这个方法即可, name变量代表了获取该广播变量的名称</p></li><li><p><strong>使用广播变量</strong></p><p>使用方法主要是通过 RichFunction, 在 对应的 open( )方法中, 可以根据名称来获取对应的广播变量, 只需要一次获取, 就可以一直使用了, 具体方法如下:</p></li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">dataSet.map(<span class="keyword">new</span> RichMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">            List&lt;Integer&gt; bc;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">super</span>.open(parameters);</span><br><span class="line">                <span class="comment">// 2. 获取广播变量</span></span><br><span class="line">                <span class="keyword">this</span>.bc = getRuntimeContext().getBroadcastVariable(<span class="string">"broadcastData"</span>);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> s;</span><br><span class="line">            &#125;</span><br><span class="line">    <span class="comment">// 1. 将需要用的变量广播出去 (这一步可以写在后面)</span></span><br><span class="line">        &#125;).withBroadcastSet(broadcastData, <span class="string">"broadcastData"</span>).print();</span><br></pre></td></tr></table></figure><p>下面以一个获取用户年龄的例子来演示一个常见的使用案例:</p><p>broadcastData 是一个包含用户 (姓名, 年龄) 的数据表</p><p>需要在另外一个算子中通过姓名查找年龄, 那么就需要把上表广播</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BroadcastExample</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">4</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建需要广播的 数据集 (name, age)</span></span><br><span class="line">        Tuple2&lt;String, Integer&gt; john = <span class="keyword">new</span> Tuple2&lt;&gt;(<span class="string">"john"</span>, <span class="number">23</span>);</span><br><span class="line">        Tuple2&lt;String, Integer&gt; tom = <span class="keyword">new</span> Tuple2&lt;&gt;(<span class="string">"tom"</span>, <span class="number">24</span>);</span><br><span class="line">        Tuple2&lt;String, Integer&gt; shiny = <span class="keyword">new</span> Tuple2&lt;&gt;(<span class="string">"shiny"</span>, <span class="number">22</span>);</span><br><span class="line">        DataSource&lt;Tuple2&lt;String, Integer&gt;&gt; broadcastData = env.fromElements(john, tom, shiny);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 新建一个dataset -&gt; d1, 设置并行度为4</span></span><br><span class="line">        <span class="comment">// 此时 d1 是无法访问 broadcastData 的数据的, 因为两个dataset可能不在一个节点或者slot中, 所以 flink 是不允许去访问的</span></span><br><span class="line">        DataSet&lt;String&gt; d1 = env.fromElements(<span class="string">"john"</span>, <span class="string">"tom"</span>, <span class="string">"shiny"</span>).setParallelism(<span class="number">4</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 使用 RichMapFunction, 在open() 方法中拿到广播变量</span></span><br><span class="line">        d1.map(<span class="keyword">new</span> RichMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">            List&lt;Tuple2&lt;String, Integer&gt;&gt; bc;</span><br><span class="line">            HashMap&lt;String, Integer&gt; map = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">super</span>.open(parameters);</span><br><span class="line">                <span class="keyword">this</span>.bc = getRuntimeContext().getBroadcastVariable(<span class="string">"broadcastData"</span>);</span><br><span class="line">                <span class="keyword">for</span> (Tuple2&lt;String, Integer&gt; tp : bc) &#123;</span><br><span class="line">                    <span class="keyword">this</span>.map.put(tp.f0, tp.f1);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                Integer age = <span class="keyword">this</span>.map.get(s);</span><br><span class="line">                <span class="keyword">return</span> s + <span class="string">"-&gt;"</span> + age;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).withBroadcastSet(broadcastData, <span class="string">"broadcastData"</span>).print();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
            <tag> broadcast </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink使用10-通过Bulk iterator计算圆周率</title>
      <link href="/2019/10/02/flink%E4%BD%BF%E7%94%A810-%E9%80%9A%E8%BF%87Bulk-iterator%E8%AE%A1%E7%AE%97%E5%9C%86%E5%91%A8%E7%8E%87/"/>
      <url>/2019/10/02/flink%E4%BD%BF%E7%94%A810-%E9%80%9A%E8%BF%87Bulk-iterator%E8%AE%A1%E7%AE%97%E5%9C%86%E5%91%A8%E7%8E%87/</url>
      
        <content type="html"><![CDATA[<p> 迭代处理是批量处理处理中的常见操作, Flink 的 迭代计算支持两种模式, 分别是 Bulk Iteration (全量迭代计算) 和 Delt Iteration (增量迭代计算). 下面就一个计算圆周率的例子 来说一下使用 Bulk Iteration 都有哪几个步骤.</p><p>在 Bulk Iteration 中, 主要的步骤其实是分为3步, 第一步是指定最大循环次数, 第二步是指定在循环时的一个计算处理的过程, 最后一步就是调用计算过程, 指定结束条件. 具体代码如下所示</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BulkIteration</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">// 获取执行环境</span></span><br><span class="line">        ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"><span class="comment">// 构建输出数据</span></span><br><span class="line">        DataSource&lt;Integer&gt; data = env.fromElements(<span class="number">0</span>);</span><br><span class="line">        <span class="comment">// 1. 指定循环次数</span></span><br><span class="line">        IterativeDataSet&lt;Integer&gt; loop = data.iterate(<span class="number">1000</span>);</span><br><span class="line">        <span class="comment">// 2. 指循环计算过程</span></span><br><span class="line">        MapOperator&lt;Integer, Integer&gt; process = loop.map(<span class="keyword">new</span> MapFunction&lt;Integer, Integer&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Integer <span class="title">map</span><span class="params">(Integer i)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">double</span> x = Math.random();</span><br><span class="line">                <span class="keyword">double</span> y = Math.random();</span><br><span class="line">                <span class="keyword">int</span> result = (x * x + y * y) &lt; <span class="number">1</span> ? <span class="number">1</span> : <span class="number">0</span>;</span><br><span class="line">                <span class="keyword">return</span> i + result;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="comment">// 3. 使用 closeWith 调用计算过程</span></span><br><span class="line">        List&lt;Integer&gt; collect = loop.closeWith(process).collect();</span><br><span class="line">        <span class="comment">// 输出最终结果</span></span><br><span class="line">        <span class="keyword">for</span> (Integer i : collect) &#123;</span><br><span class="line">            System.out.println( i / <span class="number">1000.0</span> * <span class="number">4</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
            <tag> iteration </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink使用09-DataSet初体验之通过Inputformat构建dataSet</title>
      <link href="/2019/09/29/flink%E4%BD%BF%E7%94%A809-DataSet%E5%88%9D%E4%BD%93%E9%AA%8C%E4%B9%8B%E9%80%9A%E8%BF%87Inputformat%E6%9E%84%E5%BB%BAdataSet/"/>
      <url>/2019/09/29/flink%E4%BD%BF%E7%94%A809-DataSet%E5%88%9D%E4%BD%93%E9%AA%8C%E4%B9%8B%E9%80%9A%E8%BF%87Inputformat%E6%9E%84%E5%BB%BAdataSet/</url>
      
        <content type="html"><![CDATA[<p>Flink 提供了一套 DataSet 的 API 来做批处理. 其实 DataSet 的使用方法还是和 DataStream 很相似的, 本章主要是先简单的说一下 DataSet 的基本使用. </p><p>DataSet API 其实和 DataStream ApI 相似, 都是需要创建 ExecutionEnvironment 环境, 然后通过 ExecutionEnvironment 环境提供的方法读取外部数据, 将外部数据转换为 DataSet 数据集, 之后利用 DataSet 提供的 API 进行转换操作, 并处理成最后的结果, 并对结果进行输出.</p><h3 id="DataSources-数据输入"><a href="#DataSources-数据输入" class="headerlink" title="DataSources 数据输入"></a>DataSources 数据输入</h3><p>数据输入共有3种类型的接口, 分别是文件系统类型 / Java Collection 类型 / 以及通用数据类型. 其中前两种其实与 DataStream类型, 在前面的系列文章中已经说过了, 这边主要再说一下 通用数据类型接口怎样使用.</p><p>DataSet ApI 提供了 Inputformat 通用的数据接口, 已接入不同的数据源和格式类型的数据. Inputformat 接口主要分为两种类型: 一种是基于文件类型, 在 DataSet API 对应的 readFile( ) 方法; 另外一种是基于通用数据类型的接口, 例如读取 RDBMS 或者 NoSQL 数据库等.</p><p>下面一个方法就以读取一个csv文件的方式举例, 其中首先定义好了每一行的转换类型, 之后将每一行数据输入都转换为对应的 pojo. 使用 env.createInput() 将 PojoCsvInputFormat 转换为 dataSet.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> &lt;T&gt; <span class="function">DataSet&lt;T&gt; <span class="title">getSource</span><span class="params">(ExecutionEnvironment env, String path, String[] fieldOrder, Class&lt;T&gt; type)</span> <span class="keyword">throws</span> URISyntaxException </span>&#123;</span><br><span class="line">        <span class="comment">// 本地文件路径</span></span><br><span class="line">        URL fileUrl = InputFormatExample.class.getClassLoader().getResource(path);</span><br><span class="line">        Path filePath = Path.fromLocalFile(<span class="keyword">new</span> File(fileUrl.getPath()));</span><br><span class="line">        <span class="comment">// 抽取  TypeInformation，是一个 PojoTypeInfo</span></span><br><span class="line">        PojoTypeInfo&lt;T&gt; pojoType = (PojoTypeInfo&lt;T&gt;) TypeExtractor.createTypeInfo(type);</span><br><span class="line">        <span class="comment">// 由于 Java 反射抽取出的字段顺序是不确定的，需要显式指定下文件中字段的顺序</span></span><br><span class="line">        <span class="comment">// 创建 PojoCsvInputFormat</span></span><br><span class="line">        PojoCsvInputFormat&lt;T&gt; csvInput = <span class="keyword">new</span> PojoCsvInputFormat&lt;&gt;(filePath, pojoType, fieldOrder);</span><br><span class="line">        <span class="keyword">return</span> env.createInput(csvInput, pojoType);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h3 id="DataSet-转换操作"><a href="#DataSet-转换操作" class="headerlink" title="DataSet 转换操作"></a>DataSet 转换操作</h3><p>Flink 提供了丰富的 API 对 dataSet 做转换处理, 例如数据处理(Map / FlatMap / MapPartiton / Filter), 聚合操作(Reduce / ReduceGroup / Aggregate), 多表关联(Join / OuterJoin / Cogroup / Cross), 集合操作 (Union / Rebalance / Hash-Partition / Range-Partition / Sort Partition), 排序操作(first / minBy / maxBy). 具体的API操作太多, 本文就不一一赘述了,这里就将一些 join 方法的使用.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 与流处理的合并类型, 也是依靠 where().equalTO()来实现两个dataSet的判断</span></span><br><span class="line">dataSet1.join(dataSet2).where(<span class="string">"key"</span>).equalTo(<span class="string">"key"</span>).with(&lt;JoinFunction&gt;)</span><br></pre></td></tr></table></figure><p> 下面一个例子展示了如何去使用 join 方法关联两个 dataSet.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 首先使用上面讲到的方法读取csv文件数据,转为DataSet&lt;POJO&gt;</span></span><br><span class="line"><span class="comment">// item dataSet 格式为(id, price)</span></span><br><span class="line">        String itemPath = <span class="string">"item.csv"</span>;</span><br><span class="line">        String[] itemField = <span class="keyword">new</span> String[]&#123;<span class="string">"id"</span>, <span class="string">"price"</span>&#125;; <span class="comment">// java反射会导致乱序,手动指定字段序</span></span><br><span class="line">        DataSet&lt;Item&gt; items = getSource(env, itemPath, itemField, Item.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// info dataSet 格式为(id, color, country)</span></span><br><span class="line">        String infoPath = <span class="string">"info.csv"</span>;</span><br><span class="line">        String[] infoField = <span class="keyword">new</span> String[]&#123;<span class="string">"id"</span>, <span class="string">"color"</span>, <span class="string">"country"</span>&#125;;</span><br><span class="line">        DataSet&lt;Info&gt; infos = getSource(env, infoPath, infoField, Info.class);</span><br><span class="line">        <span class="comment">// 关联两个dataset</span></span><br><span class="line">        JoinOperator.DefaultJoin&lt;Item, Info&gt; dataSet = items.join(infos).where(<span class="string">"id"</span>).equalTo(<span class="string">"id"</span>);</span><br><span class="line">        <span class="comment">// 使用 joinFunction 处理合并后的两个dataSet</span></span><br><span class="line">        dataSet.with(<span class="keyword">new</span> JoinFunction&lt;Item, Info, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> String <span class="title">join</span><span class="params">(Item item, Info info)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">"商品ID:"</span> + item.getId() + <span class="string">" 价格:"</span>+item.getPrice() + <span class="string">" 颜色:"</span>+ info.getColor() + <span class="string">" 国家:"</span> + info.getCountry();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).print();</span><br><span class="line"><span class="comment">// 样例数据结果:↓↓↓↓↓</span></span><br><span class="line"><span class="comment">//商品ID:1 价格:50 颜色:red 国家:china</span></span><br><span class="line"><span class="comment">//商品ID:2 价格:120 颜色:black 国家:usa</span></span><br><span class="line"><span class="comment">//商品ID:3 价格:89 颜色:green 国家:korea</span></span><br></pre></td></tr></table></figure><h3 id="DataSinks-数据输出"><a href="#DataSinks-数据输出" class="headerlink" title="DataSinks 数据输出"></a>DataSinks 数据输出</h3><p>为了能够让用户更灵活的使用外部数据, Flink抽象出通用的 OutputFormat 接口, 批量数据输出全部实现于此接口.</p><p>Flink 内置了常用的数据存储介质对应的接口, 如 TextOutputFormat /CsvOutputFormat / HadoopOutputFormat / JDBCOutputFormat 等. </p><p>Flink 在 DataSet ApI 中的数据输出总共以下3类:</p><h4 id="1-基于文件输出接口"><a href="#1-基于文件输出接口" class="headerlink" title="1. 基于文件输出接口"></a>1. 基于文件输出接口</h4><p>WriteAsText / WriteAsCsv. 可以直接使用这两个方法输出到文件, 用户也可以指定写入文件的模式, 分为 OVERWRITE 模式(覆盖) 和 NOT_OVERWRITE 模式(不覆盖 ). 均可以写到hdfs或者本地</p><h4 id="2-通用数据接口"><a href="#2-通用数据接口" class="headerlink" title="2. 通用数据接口"></a>2. 通用数据接口</h4><p>用户可以自己自定义OutputFormat 方法来定义存储, 例如 HadoopOutputFormat.</p><h4 id="3-客户端输出"><a href="#3-客户端输出" class="headerlink" title="3. 客户端输出"></a>3. 客户端输出</h4><p>如果想在本地调试的话, 那么最简单的方式就是通过 print()的方法直接将flink的数据拉回到client, 然后输出.</p>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flnik </tag>
            
            <tag> dataset </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink使用08-在dataStream中使用AsyncFunction</title>
      <link href="/2019/09/27/flink%E4%BD%BF%E7%94%A808-%E5%9C%A8dataStream%E4%B8%AD%E4%BD%BF%E7%94%A8AsyncFunction/"/>
      <url>/2019/09/27/flink%E4%BD%BF%E7%94%A808-%E5%9C%A8dataStream%E4%B8%AD%E4%BD%BF%E7%94%A8AsyncFunction/</url>
      
        <content type="html"><![CDATA[<p>在流式处理的过程中, 在中间步骤的处理中, 如果涉及到一些费事的操作或者是外部系统的数据交互, 那么就会给整个流造成一定的延迟. 在 flink 的 1.2 版本中引入了 Asynchronous I/O, 能够支持异步的操作, 以提高 flink 系统与外部数据系统交互的性能及吞吐量.</p><p>在使用 Flink 的异步 IO 时, 主要有两个 API可以使用, 一个是AsyncDataStream.unorderedWait( ), 另一个AsyncDataStream.orderedWait( ).在异步处理过程中,原本数据的顺序可能会发生变化, 使用unorderWait的方法, 不会考虑顺序的问题, 一旦处理完成就会直接返回结果, 这种方法具有较低的延迟和负载. 那么orderWait的方法就是想对应的, 严格按照原本流中的数据顺序做返回, 会对系统造成一定的延迟. 实际中应该根据具体的业务情况做选择.unorderedWait或orderedWait有两个关于async operation的参数，一个是timeout参数用于设置async的超时时间，一个是capacity参数用于指定同一时刻最大允许多少个(<code>并发</code>)async request在执行；</p><p>在使用异步IO时,需要自己去继承AsyncFunction,AsyncFunction接口继承了Function，它定义了asyncInvoke方法以及一个default的timeout方法；asyncInvoke方法执行异步逻辑，然后通过ResultFuture.complete将结果或异常设置到ResultFuture，如果异常则通过ResultFuture.completeExceptionally(Throwable)来传递 ResultFuture；RichAsyncFunction继承了AbstractRichFunction，同时声明实现AsyncFunction接口，它不没有实现asyncInvoke，交由子类实现；它覆盖了setRuntimeContext方法，这里使用RichAsyncFunctionRuntimeContext或者RichAsyncFunctionIterationRuntimeContext进行包装.</p><p>下面是一个验证 Async I/O 的demo, 具体代码见仓库 -&gt; <a href="https://github.com/CheckChe0803/flink-simple-tutorial/tree/master/streaming/src/main/java/async" target="_blank" rel="noopener">code link</a></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AsyncIOExample</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        DataStream&lt;String&gt; inp = env.fromElements(AsyncIOData.WORDS);</span><br><span class="line"><span class="comment">// 接收数据</span></span><br><span class="line">        SingleOutputStreamOperator&lt;String&gt; out = inp.map(<span class="keyword">new</span> MapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                System.out.println(<span class="string">"读取数据:"</span> + s + <span class="string">"  当前时间:"</span> + System.currentTimeMillis());</span><br><span class="line">                <span class="keyword">return</span> s;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="comment">// 使用 AsyncFunction 对函数做一个简单的处理, 中间随机睡眠 1-10s</span></span><br><span class="line">        DataStream&lt;String&gt; asyncStream = AsyncDataStream.unorderedWait(out, <span class="keyword">new</span> SimpleAsyncFunction(), <span class="number">20_000L</span>, TimeUnit.MILLISECONDS);</span><br><span class="line"><span class="comment">// 对已经被 AsyncFunction 处理过的数据再输出一次</span></span><br><span class="line">        asyncStream.map(<span class="keyword">new</span> MapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                System.out.println(<span class="string">"数据处理完毕:"</span> + s + <span class="string">"  当前时间:"</span> + System.currentTimeMillis());</span><br><span class="line">                <span class="keyword">return</span> s;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"AsyncFunction Demo"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">SimpleAsyncFunction</span> <span class="keyword">extends</span> <span class="title">RichAsyncFunction</span>&lt;<span class="title">String</span>, <span class="title">String</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">long</span> waitTime;</span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">final</span> Random rnd = <span class="keyword">new</span> Random(hashCode());</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">asyncInvoke</span><span class="params">(String input, ResultFuture&lt;String&gt; resultFuture)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="comment">// 随机睡眠 1 - 10s</span></span><br><span class="line">            System.out.println(<span class="string">"开始 AsyncFunction  target -&gt; "</span> + input);</span><br><span class="line">            waitTime = rnd.nextInt(<span class="number">10</span>);</span><br><span class="line">            Thread.sleep(waitTime * <span class="number">1000</span>);</span><br><span class="line">            String out = input + input;</span><br><span class="line">            resultFuture.complete(Collections.singletonList(out));</span><br><span class="line">            System.out.println(<span class="string">"结束 AsyncFunction  target -&gt; "</span> + input + <span class="string">"  Sleep time = "</span> + waitTime + <span class="string">"s"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>以上代码的输出结果为:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">读取数据:D  当前时间:<span class="number">1569574233046</span></span><br><span class="line">读取数据:C  当前时间:<span class="number">1569574233047</span></span><br><span class="line">读取数据:A  当前时间:<span class="number">1569574233048</span></span><br><span class="line">读取数据:B  当前时间:<span class="number">1569574233049</span></span><br><span class="line">开始 AsyncFunction  target -&gt; D</span><br><span class="line">开始 AsyncFunction  target -&gt; C</span><br><span class="line">开始 AsyncFunction  target -&gt; A</span><br><span class="line">开始 AsyncFunction  target -&gt; B</span><br><span class="line">结束 AsyncFunction  target -&gt; DSleep time = <span class="number">6</span>s</span><br><span class="line">数据处理完毕:DD  当前时间:<span class="number">1569574239065</span></span><br><span class="line">结束 AsyncFunction  target -&gt; CSleep time = <span class="number">6</span>s</span><br><span class="line">数据处理完毕:CC  当前时间:<span class="number">1569574239069</span></span><br><span class="line">结束 AsyncFunction  target -&gt; ASleep time = <span class="number">6</span>s</span><br><span class="line">数据处理完毕:AA  当前时间:<span class="number">1569574239072</span></span><br><span class="line">结束 AsyncFunction  target -&gt; BSleep time = <span class="number">6</span>s</span><br><span class="line">数据处理完毕:BB  当前时间:<span class="number">1569574239076</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
            <tag> async </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink使用07-通过join合并流的操作</title>
      <link href="/2019/09/27/flink%E4%BD%BF%E7%94%A807-%E9%80%9A%E8%BF%87join%E5%90%88%E5%B9%B6%E6%B5%81%E7%9A%84%E6%93%8D%E4%BD%9C/"/>
      <url>/2019/09/27/flink%E4%BD%BF%E7%94%A807-%E9%80%9A%E8%BF%87join%E5%90%88%E5%B9%B6%E6%B5%81%E7%9A%84%E6%93%8D%E4%BD%9C/</url>
      
        <content type="html"><![CDATA[<p>Flink 中支持窗口上的多流合并, 需要保证的是输入的 stream 要构建在相同的 Window 上, 并使用相同类型的 Key 作为关联条件.代码如下所示, 先通过 join 方法将 inputStream1 数据集和 inputStream2 关联, 调用 where( ) 方法指定  inputStream1 的 key, 调用 equalTo( ) 方法指定 inputStream2 对应关联的 key. 通过 window( ) 方法指定 window Assigner, 最后再通过 apply( ) 方法传入用户自定义的 JoinFunction 或者 FlatJoinFunction 对输入的数据元素做窗口计算.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">inputStream1.join(inputStream2)</span><br><span class="line"><span class="comment">// 指定inputStream1的关联key</span></span><br><span class="line">.where(<span class="number">0</span>)</span><br><span class="line"><span class="comment">// 指定inputStream2的关联key</span></span><br><span class="line">.equalTo(<span class="number">1</span>)</span><br><span class="line"><span class="comment">// 指定 window Assigner</span></span><br><span class="line">.window(TumblingEventTimeWindows.of(Time.seconds(<span class="number">10</span>)))</span><br><span class="line"><span class="comment">// 指定窗口计算函数</span></span><br><span class="line">.apply(&lt;JoinFunction&gt;)</span><br></pre></td></tr></table></figure><p>下面就用 flink 官方仓库中的join example来做演示, 完整代码见仓库 -&gt; <a href="https://github.com/CheckChe0803/flink-simple-tutorial/tree/master/streaming/src/main/java/join" target="_blank" rel="noopener">code link</a></p><p><strong>样例中有两个流, 分别记录的是员工的等级和员工的薪水, 流中数据的格式分别是 (name, grade) / (name, salary), 代码实现的功能是合并两个流, 转变为 (name, grade, salary) 格式的流.</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> <span class="keyword">long</span> windowSize = <span class="number">200L</span>;</span><br><span class="line"><span class="keyword">final</span> <span class="keyword">long</span> rate = <span class="number">3L</span>;</span><br><span class="line"></span><br><span class="line">System.out.println(<span class="string">"Using windowSize="</span> + windowSize + <span class="string">", data rate="</span> + rate);</span><br><span class="line">System.out.println(<span class="string">"To customize example, use: WindowJoin [--windowSize &lt;window-size-in-millis&gt;] [--rate &lt;elements-per-second&gt;]"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取env, 配置为"ingestion time"</span></span><br><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 生成 grade 和 salary 两个流 分别是 (name, grade) / (name, salary)</span></span><br><span class="line">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; grades = WindowJoinSampleData.GradeSource.getSource(env, rate);</span><br><span class="line">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; salaries = WindowJoinSampleData.SalarySource.getSource(env, rate);</span><br><span class="line"></span><br><span class="line">DataStream&lt;Tuple3&lt;String, Integer, Integer&gt;&gt; joinedStream = runWindowJoin(grades, salaries, windowSize);</span><br><span class="line"></span><br><span class="line">joinedStream.print().setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">env.execute(<span class="string">"Windowed Join Example"</span>);</span><br></pre></td></tr></table></figure><p>其中, 数据流的添加是通过一个Iterator 不停的添加进去的, 具体的 join 逻辑通过 runWindowJoin( )方法, 以为为该方法的具体内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">public static DataStream&lt;Tuple3&lt;String, Integer, Integer&gt;&gt; runWindowJoin(</span><br><span class="line">            DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; grades,</span><br><span class="line">            DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; salaries,</span><br><span class="line">            long windowSize) &#123;</span><br><span class="line"></span><br><span class="line">        return grades.join(salaries)</span><br><span class="line">                .where(new NameKeySelector())</span><br><span class="line">                .equalTo(new NameKeySelector())</span><br><span class="line"></span><br><span class="line">                .window(TumblingEventTimeWindows.of(Time.milliseconds(windowSize)))</span><br><span class="line"></span><br><span class="line">                .apply(new JoinFunction&lt;Tuple2&lt;String, Integer&gt;, Tuple2&lt;String, Integer&gt;, Tuple3&lt;String, Integer, Integer&gt;&gt;() &#123;</span><br><span class="line"></span><br><span class="line">                    @Override</span><br><span class="line">                    public Tuple3&lt;String, Integer, Integer&gt; join(</span><br><span class="line">                            Tuple2&lt;String, Integer&gt; first,</span><br><span class="line">                            Tuple2&lt;String, Integer&gt; second) &#123;</span><br><span class="line">                        return new Tuple3&lt;String, Integer, Integer&gt;(first.f0, first.f1, second.f1);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
            <tag> join </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink使用06-如何处理窗口内的数据</title>
      <link href="/2019/09/26/flink%E4%BD%BF%E7%94%A806-%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E7%AA%97%E5%8F%A3%E5%86%85%E7%9A%84%E6%95%B0%E6%8D%AE/"/>
      <url>/2019/09/26/flink%E4%BD%BF%E7%94%A806-%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E7%AA%97%E5%8F%A3%E5%86%85%E7%9A%84%E6%95%B0%E6%8D%AE/</url>
      
        <content type="html"><![CDATA[<p>上一节主要是大致介绍了下 flink 的窗口组成, 以及如何去划分窗口的. 那么这一篇文章主要是对剩下的内容做一下总结, 说一下如何对窗口内的数据做处理.  </p><h4 id="Window-Function"><a href="#Window-Function" class="headerlink" title="Window Function"></a>Window Function</h4><p>Window Assigner 的作用是划分窗口的, 而 Window Function 就是对窗口内的数据做处理的一个过程. Flink 提供了 4 种类型的 Window Function, 分别是 ReduceFunction / AggregateFunction / FoldFunction / ProcessWindowFunction. 另外, 这四类还根据计算原理的不同分为增量聚合函数和全量窗口函数. 增量的计算性能比较高, 主要是基于中间状态的计算结果, 窗口中只维护中间结果的状态值.</p><h4 id="1-ReduceFunction-增量"><a href="#1-ReduceFunction-增量" class="headerlink" title="1. ReduceFunction (增量)"></a><strong>1. ReduceFunction</strong> (增量)</h4><p>对输入的两个相同类型的元素按照指定的计算方式进行聚合, 通过实现 ReduceFunction 接口就可以在reduce( ) 函数内部进行聚合操作了.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 将Tuple2 按照 f1 进行 keyBy, 之后将 f0字符合并起来</span></span><br><span class="line">input.keyBy(x -&gt; x.f1)</span><br><span class="line">.timeWindow(Time.seconds(<span class="number">10</span>), Time.seconds(<span class="number">1</span>))</span><br><span class="line">    .reduce(<span class="keyword">new</span> ReduceFunction&lt;Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Long&gt; <span class="title">reduce</span><span class="params">(Tuple2&lt;String, Long&gt; t1, Tuple2&lt;String, Long&gt; t2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(t1.f0 + t2.f0, t1.f1);</span><br><span class="line">        &#125;</span><br><span class="line">        &#125;);</span><br></pre></td></tr></table></figure><p>当然也可以使用匿名函数的方式,写起来会更加简洁.上述代码可以改为:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">input.keyBy(x -&gt; x.f1).timeWindow(Time.seconds(<span class="number">10</span>), Time.seconds(<span class="number">1</span>))</span><br><span class="line">.reduce((t1,t2) -&gt; <span class="keyword">new</span> Tuple2&lt;&gt;(t1.f0 + t2.f0, t1.f1));</span><br></pre></td></tr></table></figure><h4 id="2-AggregateFunction-增量"><a href="#2-AggregateFunction-增量" class="headerlink" title="2. AggregateFunction (增量)"></a><strong>2. AggregateFunction</strong> (增量)</h4><p>AggregateFunction 相对于ReduceFunction更加灵活,但是实现起来也更复杂, AggregateFunction有 4 个需要复写的方法, 其中createAccumulator( ) 定义累加器, add( ) 定义数据的添加逻辑, getResult( ) 定义了根据 accumulator 计算结果的逻辑, merge()方法定义合并 accumulator 的逻辑.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">input.keyBy(x -&gt; x.f1)</span><br><span class="line">    .timeWindow(Time.seconds(<span class="number">10</span>), Time.seconds(<span class="number">1</span>))</span><br><span class="line">    <span class="comment">// 自定义一个AggregateFunciton, 将相同标号 f1 的数据的 f0字符串字段合并在一起</span></span><br><span class="line">    <span class="comment">// ("hello", 1L) + ("world", 1L) = ("hello world", 1L)</span></span><br><span class="line">    .aggregate(<span class="keyword">new</span> MyAggregateFunction());</span><br></pre></td></tr></table></figure><p>通过自定义的 MyAggregateFunction() 来实现 AggregateFunction 接口</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyAggregateFunction</span> <span class="keyword">implements</span> <span class="title">AggregateFunction</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Long</span>&gt;, <span class="title">String</span>, <span class="title">String</span>&gt;</span>&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">createAccumulator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 初始化累加器</span></span><br><span class="line">            <span class="keyword">return</span> <span class="string">""</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">add</span><span class="params">(Tuple2&lt;String, Long&gt; t, String s)</span> </span>&#123;</span><br><span class="line">            <span class="comment">// 输入数据与累加器的合并</span></span><br><span class="line">            <span class="keyword">return</span> s + <span class="string">" "</span> +t.f0;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">getResult</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">            <span class="comment">// 得到累加器的结果</span></span><br><span class="line">            <span class="keyword">return</span> s.trim();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">merge</span><span class="params">(String s, String acc1)</span> </span>&#123;</span><br><span class="line">            <span class="comment">// 合并累加器</span></span><br><span class="line">            <span class="keyword">return</span> s + <span class="string">" "</span> + acc1;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h4 id="3-FoldFunction-增量"><a href="#3-FoldFunction-增量" class="headerlink" title="3. FoldFunction (增量)"></a><strong>3. FoldFunction</strong> (增量)</h4><p>FoldFunction定义了如何将窗口中的输入元素与外部的元素合并的逻辑</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">input.keyBy(x -&gt; x.f1)</span><br><span class="line">.timeWindow(Time.seconds(<span class="number">10</span>), Time.seconds(<span class="number">1</span>)).fold(<span class="string">"flink"</span>, (acc, t) -&gt;t.f0 + acc);</span><br></pre></td></tr></table></figure><p>FoldFunction在新版本已经被标记@Deprecated了, 建议使用AggregateFunction代替</p><h4 id="4-ProcessWindowFunction-全量"><a href="#4-ProcessWindowFunction-全量" class="headerlink" title="4. ProcessWindowFunction (全量)"></a><strong>4. ProcessWindowFunction</strong> (全量)</h4><p>ProcessWindowFunction 相较于其他的 Window Function, 可以实现一些更复杂的计算, 比如基于整个窗口做某些指标计算 或者需要操作窗口中的状态数据和窗口元数据. Flink 提供了 ProcessWindowFunction 这个抽象类, 继承此类就可以实现ProcessWindowFunction, 其中, 必须要实现 process( ) 方法, 这是处理窗口数据的主要方法.还在一下跟窗口数据相关的方法可以有选择的实现.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyProcessWindowFunction</span> <span class="keyword">extends</span> <span class="title">ProcessWindowFunction</span>&lt;<span class="title">Tuple3</span>&lt;<span class="title">String</span>, <span class="title">Long</span>, <span class="title">Long</span>&gt;, <span class="title">String</span>, <span class="title">Long</span>, <span class="title">TimeWindow</span>&gt; </span>&#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(Long s, Context context, Iterable&lt;Tuple3&lt;String, Long, Long&gt;&gt; elements, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">// 统计每个窗口内的所有数据的 f0字段加起来共有多少个单词</span></span><br><span class="line">    <span class="comment">// 也就做单个窗口的 wordcount</span></span><br><span class="line">Long count = <span class="number">0L</span>;</span><br><span class="line"><span class="keyword">for</span> (Tuple3&lt;String, Long, Long&gt; element : elements) &#123;</span><br><span class="line">count += element.f0.split(<span class="string">" "</span>).length;</span><br><span class="line">&#125;</span><br><span class="line">out.collect(<span class="string">"window: "</span> + context.window() + <span class="string">" word count: "</span> + count);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="5-增量与全量共同使用"><a href="#5-增量与全量共同使用" class="headerlink" title="5. 增量与全量共同使用"></a>5. 增量与全量共同使用</h4><p>增量聚合函数虽然性能好, 但是灵活性不如全量函数, 例如对窗口状态数据的操作以及对窗口中的元数据信息的获取. 但是如果用 ProcessWindowFunction 去完成一些基础的增量计算相对比较浪费资源, 因此可以两者结合的方式来实现.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">input.keyBy(x -&gt; x.f1)</span><br><span class="line">.timeWindow(Time.seconds(<span class="number">10</span>), Time.seconds(<span class="number">1</span>))</span><br><span class="line"><span class="comment">// 第一个Function为 ReduceFunction, 取窗口的最小值</span></span><br><span class="line">.reduce((r1, r2) -&gt; &#123;</span><br><span class="line"><span class="keyword">return</span> r1.f0 &lt; r2.f0 ? r1 : r2;</span><br><span class="line"><span class="comment">// 第二个Function为 ProcessWindowFunction, 获取窗口的时间信息</span></span><br><span class="line">&#125;, <span class="keyword">new</span> ProcessWindowFunction&lt;Tuple2&lt;Long, Long&gt;, String, Long, TimeWindow&gt;() &#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(Long aLong, Context context, Iterable&lt;Tuple2&lt;Long, Long&gt;&gt; elements, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">out.collect(<span class="string">"window: "</span> + context.window()); </span><br><span class="line">&#125;</span><br><span class="line">&#125;).print();</span><br></pre></td></tr></table></figure><h3 id="Flink-窗口中的其他组件"><a href="#Flink-窗口中的其他组件" class="headerlink" title="Flink 窗口中的其他组件"></a>Flink 窗口中的其他组件</h3><p>除了 Window Assigner 和 Window Function外,Flink的窗口中还有 Triger窗口触发器, 其负责判断何时将窗口中的数据取出做计算, flink已经默认为各种类型的窗口实现了 triger. 用户也可以自己手动指定. Evictors 是数据剔除器, 目的是把窗口中的数据按照需求做一定的剔除. Flink也有 API 针对延迟数据做处理, 延迟的数据可以丢弃也可以通过sideOutputLateDate( ) 方法处理.</p><h4 id="1-Triger-窗口触发器"><a href="#1-Triger-窗口触发器" class="headerlink" title="1. Triger 窗口触发器"></a>1. Triger 窗口触发器</h4><p><strong>EventTimeTrigger</strong>: 通过对比 watermark 和窗口 EndTime 确定是否触发窗口</p><p><strong>ProcessTimeTrigger</strong>: 通过对比 ProcessTime 和窗口 EndTime 确定是否触发窗口</p><p><strong>ContinuousEventTimeTrigger</strong>: 根据间隔时间周期性触发窗口</p><p><strong>ContinuousEventTimeTrigger</strong>: 同上, 区别是使用ProcessTime</p><p><strong>CountTrigger</strong>: 根据接入数量是否超过阈值</p><p><strong>DeltaTrigger</strong>: 根据计算出来的 Delta 指标是否超过指定的 Threshold</p><p><strong>PurgingTrigger</strong>: 可以将任意触发器作为参数转换为Purge类型触发器</p><h4 id="2-Evictors触发器"><a href="#2-Evictors触发器" class="headerlink" title="2. Evictors触发器"></a>2. Evictors触发器</h4><p>CountEvictor: 保持固定数量的数据, 超过的剔除</p><p>DeltaEvictor: 通过定义 delta 和 threshold , 计算两个数据之间的 delta 值, 超过则剔除</p><p>TimeEvictor: 指定时间间隔, 将当前窗口中的最新元素的时间减去Interval, 然后将小于该结果的数据全部剔除</p>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
            <tag> window </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink使用05-窗口简介和简单的使用</title>
      <link href="/2019/09/25/flink%E4%BD%BF%E7%94%A805-%E7%AA%97%E5%8F%A3%E7%AE%80%E4%BB%8B%E5%92%8C%E7%AE%80%E5%8D%95%E7%9A%84%E4%BD%BF%E7%94%A8/"/>
      <url>/2019/09/25/flink%E4%BD%BF%E7%94%A805-%E7%AA%97%E5%8F%A3%E7%AE%80%E4%BB%8B%E5%92%8C%E7%AE%80%E5%8D%95%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<h3 id="1-窗口组成简介"><a href="#1-窗口组成简介" class="headerlink" title="1. 窗口组成简介"></a>1. 窗口组成简介</h3><p>窗口是流式计算中非常重要的一个概念, 很多常见的功能都是通过各种窗口实现的, 比如每5分钟统计一下刚去1小时的热度. Flink DataStream API 将窗口独立成 Operator. 每个窗口算子包含了以下几个部分:</p><h4 id="Windows-Assigner"><a href="#Windows-Assigner" class="headerlink" title="Windows Assigner"></a>Windows Assigner</h4><p>指定窗口的类型, 定义如何将数据流分配到一个或者多个窗口</p><h4 id="Windows-Trigger"><a href="#Windows-Trigger" class="headerlink" title="Windows Trigger"></a>Windows Trigger</h4><p>指定窗口触发的时机, 定义窗口满足什么样的条件触发计算</p><h4 id="Evictor"><a href="#Evictor" class="headerlink" title="Evictor"></a>Evictor</h4><p>用户数据剔除</p><h4 id="Lateness"><a href="#Lateness" class="headerlink" title="Lateness"></a>Lateness</h4><p>标记是否处理迟到的数据, 当迟到数据到达窗口中是否触发计算</p><h4 id="Output-Tag"><a href="#Output-Tag" class="headerlink" title="Output Tag"></a>Output Tag</h4><p>标记输出标签, 然后再通过 getSideOutput 将窗口中的数据根据标签输出</p><h4 id="Windows-Function"><a href="#Windows-Function" class="headerlink" title="Windows Function"></a>Windows Function</h4><p>定义窗口上的数据处理的逻辑, 例如对数据进行sum</p><hr><h3 id="2-Window-Assigner"><a href="#2-Window-Assigner" class="headerlink" title="2. Window Assigner"></a>2. Window Assigner</h3><p>首先最需要了解的就是 windows Assigner了, 我们想要一个什么样的窗口划分, 主要就是通过他来实现的. </p><p>根据 flink 上游的数据集是否为 KeyedStream 类型 来做分别的处理. 如果使用了keyBy( ) 则对应使用window( ) 来处理, 否则可以使用 windowAll( )来使用</p><p>Flink 可以支持两种类型的窗口, 分别是基于时间的窗口和基于数量的窗口.基于时间的意思就是按照时间去划分窗口,同理,基于数量的也是根据窗口中的数量来做切分的. 对应的分别就是 timeWindow() 和 countWindow() 来使用, 下面的示例主要使用 timeWindow() 来演示.</p><p>对于不同的 Window Assigner, 还可以把窗口划分为4大类, 分别是 滚动窗口(Tumbling Windows) / 滑动窗口(Sliding Window) / 会话窗口(Session Window) 和 全局窗口(Global Window).</p><h4 id="滚动窗口"><a href="#滚动窗口" class="headerlink" title="滚动窗口"></a>滚动窗口</h4><p>DataStream API 提供基于 EventTime 和 ProcessingTime 的两种类型的 Tumbling window.对应的 Assigner 分别是 TumblingEventTimeWindow 和 ProcessingEventTimeWindow . 举例如下,完整代码见Github.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 使用ProcessTime的滚动时间窗口, 长度为10s</span></span><br><span class="line">stream.keyBy(x -&gt; x.f1)</span><br><span class="line">    .window(TumblingProcessingTimeWindows.of(Time.seconds(<span class="number">10</span>))).process(...)</span><br><span class="line"><span class="comment">// 使用ProcessTime的滚动时间窗口, 长度为10s</span></span><br><span class="line">stream.keyBy(x -&gt;x.f1).window(TumblingEventTimeWindows.of(Time.seconds(<span class="number">10</span>))).process(...)</span><br></pre></td></tr></table></figure><p>使用 window(TumblingProcessingTimeWindows.of(Time.seconds(10))) 的方法有点啰嗦, Flink 还提供了timeWindow( ) 的 API 来简化这一行代码. </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 直接使用 timeWindow API 便可实现滚动窗口的操作, 参数依旧是窗口的长度</span></span><br><span class="line"><span class="comment">// 窗口类型的时间由 time characteristic 确定, 如果指定为 event time,那么窗口也会自动用这个时间</span></span><br><span class="line">input.keyBy(x -&gt; x.f1).timeWindow(Time.seconds(<span class="number">10</span>));</span><br></pre></td></tr></table></figure><h4 id="滑动窗口"><a href="#滑动窗口" class="headerlink" title="滑动窗口"></a>滑动窗口</h4><p>滑动窗口顾名思义就是一个在不断往后滑动的窗口, 比如说 每5分钟 统计一个 最近一小时的时间, 那么就需要用滑动窗口来做处理. 滑动窗口主要是依靠 window size 和 slide time 来确定. 与滚动窗口类似的, flink 也提供了对应不同时间的 Assigner API(SlidingEventTimeWindow / SlidingEventTimeWindow), 语法基本类似, 只是由原本的一个参数(窗口长度) 变为了两个参数(窗口长度和滑动时间), 同样的, 为了简化代码, 依然可以使用timeWindow() 来简化.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 两个参数分别是 窗口长度 和 滑动时间, 窗口时间类型依旧通过time characteristic 确定</span></span><br><span class="line">input.keyBy(x -&gt; x.f1).timeWindow(Time.seconds(<span class="number">10</span>), Time.seconds(<span class="number">1</span>))</span><br></pre></td></tr></table></figure><h4 id="会话窗口"><a href="#会话窗口" class="headerlink" title="会话窗口"></a>会话窗口</h4><p>会话窗口主要是将某段时间内活跃度较高的数据聚合成一个窗口计算. 触发条件是 Session Gap. 在规定的时间内没有数据接入则认为这个窗口结束,然后触发窗口计算. Session Gap 除了固定间隔的方式, 也可以动态抽取.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建 Session Window, 间隔为 3s</span></span><br><span class="line">        DataStream&lt;Tuple3&lt;String, Long, Integer&gt;&gt; aggregated = source</span><br><span class="line">                .keyBy(<span class="number">0</span>)</span><br><span class="line">                .window(EventTimeSessionWindows.withGap(Time.seconds(<span class="number">3L</span>)))</span><br><span class="line">                .sum(<span class="number">2</span>);</span><br></pre></td></tr></table></figure><h4 id="全局窗口"><a href="#全局窗口" class="headerlink" title="全局窗口"></a>全局窗口</h4><p>全局窗口将所有key的数据分配到单个窗口中计算结果.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建 GlobalWindow</span></span><br><span class="line">        input.keyBy(<span class="number">1</span>)</span><br><span class="line">                .window(GlobalWindows.create())</span><br><span class="line">                .sum(<span class="number">1</span>);</span><br></pre></td></tr></table></figure><p><strong>上面就是构建不同的窗口的方法了, 下文会介绍在有了窗口之后怎样对窗口中的数据做处理</strong></p>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
            <tag> window </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink使用04-几种时间概念和watermark</title>
      <link href="/2019/09/24/flink%E4%BD%BF%E7%94%A804-%E5%87%A0%E7%A7%8D%E6%97%B6%E9%97%B4%E6%A6%82%E5%BF%B5%E5%92%8Cwatermark/"/>
      <url>/2019/09/24/flink%E4%BD%BF%E7%94%A804-%E5%87%A0%E7%A7%8D%E6%97%B6%E9%97%B4%E6%A6%82%E5%BF%B5%E5%92%8Cwatermark/</url>
      
        <content type="html"><![CDATA[<h3 id="时间概念"><a href="#时间概念" class="headerlink" title="时间概念"></a>时间概念</h3><p>在做实时计算的时候, 首先就需要搞清楚一个问题, 这个实时到底是怎么样的一个时间概念. 在 Flink 中, 总共有3种时间概念, 分别是 <strong>事件时间</strong> ( Event time ) / <strong>处理时间</strong> ( Processing time ) / <strong>接入时间</strong> ( Ingestion time).</p><p><img src="/2019/09/24/flink使用04-几种时间概念和watermark/%E6%97%B6%E9%97%B4%E7%9A%84%E6%A6%82%E5%BF%B5.png" alt></p><p><strong>事件时间</strong> ( Event time )就是真实的用户发生操作的时候所产生的时间, 对应到 flink 中, 需要用户 <strong>显示</strong> 的告诉 flink 到底每个输入中的哪一个字段代表这个事件时间。</p><p><strong>接入时间</strong> ( Ingestion time) 和<strong>处理时间</strong> ( Processing time )是不需要用户去指定的, flink自己会去处理这个时间. 接入时间的代表的是一个事件通过 source Operator 的时间, 相比于 event time, ingestion time 不能处理乱序事件, 因此也就不用生成对应的watermark. 处理时间是指事件在操作算子计算过程中获取到的所在主机的时间. processing time 适合用于时间计算精度要求不是特别高的计算场景, 例如统计某些延时非常高的日志数据.</p><hr><h3 id="水位线机制-watermark"><a href="#水位线机制-watermark" class="headerlink" title="水位线机制 watermark"></a>水位线机制 watermark</h3><h4 id="1-解释-watermark"><a href="#1-解释-watermark" class="headerlink" title="1, 解释 watermark"></a>1, 解释 watermark</h4><p>watermark 这个概念在 flink 中是与 event time 这个时间概念相互依存的, 其目的是为了解决数据乱序到达和系统延迟的问题. flink会把读取进系统的最新事件时间减去固定的时间间隔作为 watermark. 还是用一张图来解释watermark 的作用.</p><p>当事件进入 flink 中的时候, 根据提取的 event time 产生 watermark 时间戳, 记为 X, 进入 flink 中的 event time 记为 Y. 当窗口的 end time &lt; X 的时候, 则触发窗口计算结果并输出. 只要 X &lt; end time, 那么 事件就可以 一直进入到当前窗口中, 这样的话即便发生乱序, 也可以在窗口中调整. 调整的方法就是按照 Y. </p><p><img src="/2019/09/24/flink使用04-几种时间概念和watermark/watermark.gif" alt></p><h4 id="2-使用-watermark"><a href="#2-使用-watermark" class="headerlink" title="2, 使用 watermark"></a>2, 使用 watermark</h4><p> a. 在 Source Function 中 直接指定 Timestamps 和 Watermark</p><p>​    用户需要复写 SourceFunction 接口中 run( ) 方法实现数据逻辑, 同时调用 SourceContext 的 collectWithTimestamp( ) 方法生成 event time 时间戳, 调用 emitWatermark( ) 方法生成 watermark.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;String&gt; text = env.addSource(<span class="keyword">new</span> SourceFunction&lt;String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;String&gt; ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">for</span> (String s : elementInput) &#123;</span><br><span class="line">                    <span class="comment">// 切割每一条数据</span></span><br><span class="line">                    String[] inp = s.split(<span class="string">","</span>);</span><br><span class="line">                    Long timestamp = <span class="keyword">new</span> Long(inp[<span class="number">1</span>]);</span><br><span class="line">                    <span class="comment">// 生成 event time 时间戳</span></span><br><span class="line">                    ctx.collectWithTimestamp(s, timestamp);</span><br><span class="line">                    <span class="comment">// 调用 emitWatermark() 方法生成 watermark, 最大延迟设定为 2</span></span><br><span class="line">                    ctx.emitWatermark(<span class="keyword">new</span> Watermark(timestamp - <span class="number">2</span>));</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// 设定默认 watermark</span></span><br><span class="line">                ctx.emitWatermark(<span class="keyword">new</span> Watermark(Long.MAX_VALUE));</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br></pre></td></tr></table></figure><p>b. 通过 Flink 自带的 Timestamp Assigner 指定 Timestamp 和 生成 watermark </p><p>在使用了 flink 定义的外部数据源( 如 kafka) 之后, 就不能通过自定义 sourcefunction 的方式来生成 watermark 和 event time 了, 这个时候可以使用 Timestamp Assigner,  其需要在第一个时间相关的 Operator前使用. Flink 有自己定义好的 Timestamp Assigner 可以直接使用 (包括直接指定的方式和固定时间延迟的方式 ).Flink 将 watermark 分为 Periodic Watermarks (根据设定的时间间隔周期性的生成) 和 Punctuated Watermarks (根据接入数量生成), 用户也可以继承对应的类实现这两种 watermark.</p><p>b.1 使用 Ascending Timestamp Assigner 指定 Timestamps 和 Watermark</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 首先需要指定系统时间概念为 event time</span></span><br><span class="line">env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span><br><span class="line"><span class="comment">// 使用 Ascending 分配 时间信息和 watermark</span></span><br><span class="line">   DataStream&lt;Tuple2&lt;String, Long&gt;&gt; text = env.fromCollection(collectionInput);</span><br><span class="line">   text.assignTimestampsAndWatermarks(<span class="keyword">new</span> AscendingTimestampExtractor&lt;Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">   <span class="meta">@Override</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractAscendingTimestamp</span><span class="params">(Tuple2&lt;String, Long&gt; element)</span> </span>&#123;</span><br><span class="line">   <span class="keyword">return</span> element.f1;</span><br><span class="line">   &#125;</span><br><span class="line">   &#125;);</span><br></pre></td></tr></table></figure><p>b.2 使用固定时延间隔的 Timestamp Assigner 指定</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 使用 Ascending 分配 时间信息和 watermark 设定10s 代表最长的时延</span></span><br><span class="line">    DataStream&lt;Tuple2&lt;String, Long&gt;&gt; text = env.fromCollection(collectionInput);</span><br><span class="line">    text.assignTimestampsAndWatermarks(<span class="keyword">new</span> BoundedOutOfOrdernessTimestampExtractor&lt;Tuple2&lt;String, Long&gt;&gt;(Time.seconds(<span class="number">10</span>)) &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Tuple2&lt;String, Long&gt; element)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> element.f1;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure><p>c. 自定义 Timestamp Assigner 和 Watermark Generator</p><p>用户可以自定义实现 AssignerWithPeriodicWatermarks 和 AssignerWithPunctuatedWatermarks 两个接口来分别生成对应的两种 watermark. 这一块用的比较少, 以后有机会再细写.</p>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
            <tag> time </tag>
            
            <tag> watermark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink使用03-数据输入的几种不同方法</title>
      <link href="/2019/09/04/flink%E4%BD%BF%E7%94%A803-%E6%95%B0%E6%8D%AE%E8%BE%93%E5%85%A5%E7%9A%84%E5%87%A0%E7%A7%8D%E4%B8%8D%E5%90%8C%E6%96%B9%E6%B3%95/"/>
      <url>/2019/09/04/flink%E4%BD%BF%E7%94%A803-%E6%95%B0%E6%8D%AE%E8%BE%93%E5%85%A5%E7%9A%84%E5%87%A0%E7%A7%8D%E4%B8%8D%E5%90%8C%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h3 id="flink的数据输入源主要分为两大类"><a href="#flink的数据输入源主要分为两大类" class="headerlink" title="flink的数据输入源主要分为两大类:"></a>flink的数据输入源主要分为两大类:</h3><h4 id="1-内置数据源"><a href="#1-内置数据源" class="headerlink" title="1. 内置数据源"></a>1. 内置数据源</h4><ul><li><p>集合数据源</p><p>可以将数组或者集合作为 flink 的数据源,分别有不同的方法可以使用, 这种方式比较适合本地调试使用</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// 添加数组作为数据输入源</span><br><span class="line">String[] elementInput = new String[]&#123;&quot;hello Flink&quot;, &quot;Second Line&quot;&#125;;</span><br><span class="line">DataStream&lt;String&gt; text = env.fromElements(elementInput);</span><br><span class="line"></span><br><span class="line">// 添加List集合作为数据输入源</span><br><span class="line">List&lt;String&gt; collectionInput = new ArrayList&lt;&gt;();</span><br><span class="line">collectionInput.add(&quot;hello Flink&quot;);</span><br><span class="line">DataStream&lt;String&gt; text2 = env.fromCollection(collectionInput);</span><br></pre></td></tr></table></figure></li><li><p>Socket数据源</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">// 添加Socket作为数据输入源</span><br><span class="line">// 4个参数 -&gt; (hostname:Ip地址, port:端口, delimiter:分隔符, maxRetry:最大重试次数)</span><br><span class="line">DataStream&lt;String&gt; text3 = env.socketTextStream(&quot;localhost&quot;, 9999, &quot;\n&quot;, 4);</span><br></pre></td></tr></table></figure></li><li><p>文件数据源</p><p>可以使用 readTextFile 方法直接读取文本文件, 这种方式可以用来监控一下 log 日志文件, 也可以使用 readFile 方法通过指定 InputFormat 来读取特定数据类型的文件, InputFormat可以是内置类,如 CsvInputFormat 或者用户自定义 InputFormat 接口类.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">// 添加文件源</span><br><span class="line">// 直接读取文本文件</span><br><span class="line">DataStream&lt;String&gt; text4 = env.readTextFile(&quot;/opt/history.log&quot;);</span><br><span class="line"></span><br><span class="line">// 指定 CsvInputFormat, 监控csv文件(两种模式), 时间间隔是10ms</span><br><span class="line">        DataStream&lt;String&gt; text5 = env.readFile(new CsvInputFormat&lt;String&gt;(new Path(&quot;/opt/history.csv&quot;)) &#123;</span><br><span class="line">            @Override</span><br><span class="line">            protected String fillRecord(String s, Object[] objects) &#123;</span><br><span class="line">                return null;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;,&quot;/opt/history.csv&quot;, FileProcessingMode.PROCESS_CONTINUOUSLY,10);</span><br></pre></td></tr></table></figure><p>在 readFile() 方法中有一项参数为 WatchType, 共有两种模式 (PROCESS_CONTINUOUSLY / PROCESS_ONCE). 在 PROCESS_CONTINUOUSLY  模式下, 检测到文件变动就会将文件全部内容加载在 flink, 在 PROCESS_ONCE 模式下, 只会将文件变动的那部分加载到 flink.  </p></li></ul><h4 id="2-外部数据源"><a href="#2-外部数据源" class="headerlink" title="2. 外部数据源"></a>2. 外部数据源</h4><p>外部数据源是重头戏, 一般来说项目中均是使用外部数据源作为数据的源头, flink 通过实现 SourceFunction 定义了非常丰富的第三方数据连接器</p><ul><li><p>数据源连接器</p><p>对于第三方数据源, flink的支持分为三种,有只读型(Twitter Streaming API / Netty ), 只写型( Cassandra / Elasticsearch / hadoop FileSystem), 支持读写(Kafka / Amazon Kinesis / RabbitMQ)</p><p>Apache Kafka (Source / Sink)</p><p>Apache Cassandra (Sink)</p><p>Amazon Kinesis Streams (Source / Sink)</p><p>Elasticsearch (Sink)</p><p>Hadoop FileSystem (Sink)</p><p>RabbitMQ (Source / Sink)</p><p>Apache NiFI (Source / Sink)</p><p>Twitter Streaming API (Source)</p><p><strong>Apache Bahir 中的连接器</strong>:</p><p>Apache ActiveMQ (Source / Sink)</p><p>Apache Flume (Sink)</p><p>Redis (Sink)</p><p>Akka (Sink)</p><p>Netty (Source)</p></li></ul><p><strong>以Kafka 为例 做演示</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 配置 kafka 连接参数</span></span><br><span class="line">String topic = <span class="string">"topic_name"</span>;</span><br><span class="line">String bootStrapServers = <span class="string">"localhost:9092"</span>;</span><br><span class="line">String zkConnect = <span class="string">"localhost:2181"</span>;</span><br><span class="line">String groupID = <span class="string">"group_A"</span>;</span><br><span class="line">Properties prop = <span class="keyword">new</span> Properties();</span><br><span class="line">prop.setProperty(<span class="string">"bootstrap.servers"</span>, bootStrapServers);</span><br><span class="line">prop.setProperty(<span class="string">"zookeeper.connect"</span>, zkConnect);</span><br><span class="line">prop.setProperty(<span class="string">"group.id"</span>, groupID);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建 kafka connector source</span></span><br><span class="line">FlinkKafkaConsumer010&lt;String&gt; consumer010 = <span class="keyword">new</span> FlinkKafkaConsumer010&lt;&gt;(topic, <span class="keyword">new</span> SimpleStringSchema(), prop);</span><br><span class="line"></span><br><span class="line"><span class="comment">// add source</span></span><br><span class="line">DataStreamSource&lt;String&gt; dataStream = env.addSource(consumer010);</span><br></pre></td></tr></table></figure><ul><li><p>自定义数据源连接器</p><p>用户也可以自己定义连接器, 通过实现 SourceFunction 定义单个线程的接入的数据连接器, 也可以通过实现ParallelSourceFunction 接口或者继承 RichParallelSourceFunction 类定义并发数据源接入器.</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
            <tag> dataSource </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink使用02-从WordCount开始</title>
      <link href="/2019/09/03/flink%E4%BD%BF%E7%94%A802-%E4%BB%8EWordCount%E5%BC%80%E5%A7%8B/"/>
      <url>/2019/09/03/flink%E4%BD%BF%E7%94%A802-%E4%BB%8EWordCount%E5%BC%80%E5%A7%8B/</url>
      
        <content type="html"><![CDATA[<p>相信大家在学习spark的时候接触的第一个案例肯定也是 wordCount, 本文也想通过这样一个简单的例子来讲一下一个简单的 flink 程序是什么样子的, 让大家对 flink 的代码有一个简单的了解.</p><p>一个 flink程序主要分为5个部分:</p><ul><li><strong>1. 获取执行 Environment</strong></li></ul><p>environment 提供控制 job 执行的方法(例如设置并行度/容错/checkpoint 参数) 并且与外部系统做交互. flink可以做流计算也可以做批计算, 对应的也就有不同的environment , 在使用时根据不同的使用场景选择即可.</p><ul><li><p><strong>2. 获取输入流 Source</strong></p><p>一个流式计算框架自然是少不了数据的输入, 在 streamExecutionEnvironment 的可以看到有很多种创建输入流的方式, 不过在项目中使用最多的还是使用 addSource()方法来添加不同的数据源接入</p><p><img src="/2019/09/03/flink使用02-从WordCount开始/%E6%95%B0%E6%8D%AE%E6%BA%90%E6%8E%A5%E5%85%A5%E6%96%B9%E6%B3%95.png" alt></p></li><li><p><strong>3. 执行计算 Operator</strong></p><p>在spark中,对数据的转换计算是通过 action 算子和 transformation 算子来对 RDD 中的数据来进行操作, 而在flink中, 主要是通过 Operator来对一个流做处理, 通过一个 Operator 可以将一个流转换为另外一个流, flink中内置了很多算子来实现Operator操作.</p></li><li><p><strong>4. 输入结果 Sink</strong></p><p>在完成数据计算之后,就需要有一个输出的地方, 通常来讲也是通过 addSink() 方法来添加不同的数据输出目标,也可以通过 print() 来直接查看输出或者写入到csv等外部文件.</p></li><li><p><strong>5. 启动 flink,提交 job</strong></p><p>一个 flink 代码的启动执行, 必须通过 env.executor() 方法.这行代码主要做了以下事情:</p><ol><li>生成StreamGraph </li><li>生成JobGraph. </li><li>生成一系列配置</li><li>将 JobGraph和配置交给 flink 集群去运行</li><li>以本地模式运行的话,可以看到启动过程,如启动能量度,web模块,jobManager,ResourceManager,taskManager等等</li><li>启动任务</li></ol></li></ul><h4 id="以下为简单的-WordCount-代码"><a href="#以下为简单的-WordCount-代码" class="headerlink" title="以下为简单的 WordCount 代码"></a>以下为简单的 WordCount 代码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取 StreamEnv</span></span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取 输入流</span></span><br><span class="line">        DataStream&lt;String&gt; text = env.fromElements(WordCountData.WORDS);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 执行计算Operator</span></span><br><span class="line">        DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; counts</span><br><span class="line">                = text.flatMap(<span class="keyword">new</span> SplitFunction())</span><br><span class="line">                .keyBy(<span class="number">0</span>).sum(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 输出结果</span></span><br><span class="line">        counts.print();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 启动flink程序</span></span><br><span class="line">        env.execute(<span class="string">"WordCount Demo"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// *************************************************************************</span></span><br><span class="line">    <span class="comment">// 自定义切割Function切分一行输入</span></span><br><span class="line">    <span class="comment">// *************************************************************************</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">SplitFunction</span> <span class="keyword">implements</span> <span class="title">FlatMapFunction</span>&lt;<span class="title">String</span>, <span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Integer</span>&gt;&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String s, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            String[] words = s.toLowerCase().split(<span class="string">" "</span>);</span><br><span class="line">            <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">                <span class="keyword">if</span> (word.length() &gt; <span class="number">0</span>)&#123;</span><br><span class="line">                    collector.collect(<span class="keyword">new</span> Tuple2&lt;&gt;(word, <span class="number">1</span>));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink使用01-本系列简介</title>
      <link href="/2019/09/03/flink%E4%BD%BF%E7%94%A801-%E6%9C%AC%E7%B3%BB%E5%88%97%E7%AE%80%E4%BB%8B/"/>
      <url>/2019/09/03/flink%E4%BD%BF%E7%94%A801-%E6%9C%AC%E7%B3%BB%E5%88%97%E7%AE%80%E4%BB%8B/</url>
      
        <content type="html"><![CDATA[<p>​    Flink 是一款能够同时支持高吞吐/低延迟/高性能的分布式处理框架.</p><p>​    本系列叫做 &lt;Flink简易使用教程&gt;,  目的是记录自己学习 flink 的过程,并且把使用flink的方方面面介绍给大家.尽量用简单的话把使用方法说清楚,在使用某个具体功能的时候能够快速的查找到该使用方法.</p><p>​    本系列的主要例子会从 flink 官方仓库的 example 出发, 通过这些代码来使用 flink 的一些基本操作.</p><p><img src="/2019/09/03/flink使用01-本系列简介/flinkExample.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
