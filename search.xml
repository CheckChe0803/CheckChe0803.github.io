<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>English Daily</title>
      <link href="/2020/09/12/English-Daily/"/>
      <url>/2020/09/12/English-Daily/</url>
      
        <content type="html"><![CDATA[<p>记录每天一句英语听写</p><p>Day1:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Mother Teresa, who received a Nobel Peace Price for her work on behalf of the poor, dies in Calcutta, India -- she was 87 years old.</span><br></pre></td></tr></table></figure><p>Day2:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Jerry, what time do you have?</span><br><span class="line">I have 5 o&apos;clock.</span><br></pre></td></tr></table></figure><p>Day3:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">There are 3 things I have learned Never to discuss with poeple.</span><br><span class="line">Religion, Politics and Great Pumkin!</span><br></pre></td></tr></table></figure><p>Day4:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Don&apos;t ever let somebody tell you,</span><br><span class="line">you can&apos;t do something!</span><br></pre></td></tr></table></figure><p>Day5:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">I have come here to chew bubblegum and kick ass...</span><br><span class="line">And I&apos;m all out of bubblegum</span><br></pre></td></tr></table></figure><p>Day6:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">The secret of life is just to live every moment!</span><br></pre></td></tr></table></figure><p>Day 7:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> English </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive 常用操作备忘录</title>
      <link href="/2020/09/05/Hive-%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%A4%87%E5%BF%98%E5%BD%95/"/>
      <url>/2020/09/05/Hive-%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%A4%87%E5%BF%98%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<h2 id="🚗-表操作"><a href="#🚗-表操作" class="headerlink" title="🚗 表操作"></a>🚗 表操作</h2><ol><li>表重命名</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE table_name RENAME TO new_table_name</span><br></pre></td></tr></table></figure><ol><li>修改表注释</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE table_name SET TBLPROPERTIES(&apos;comment&apos; = new_comment);</span><br></pre></td></tr></table></figure><ol><li>查看表的创建语句</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SHOW CREATE TABLE table_name</span><br></pre></td></tr></table></figure><ol><li>创建表并导入txt文件</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">create table table_name (</span><br><span class="line">    uid INT,</span><br><span class="line">    name STRING</span><br><span class="line">) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\t&apos; STORED AS TEXTFILE; -- 使用 \t 分割字段</span><br><span class="line"></span><br><span class="line">load data local inpath &apos;/tmp/data.txt&apos; into table table_name; -- local 代表本地文件路径</span><br></pre></td></tr></table></figure><h2 id="🚗-列操作"><a href="#🚗-列操作" class="headerlink" title="🚗 列操作"></a>🚗 列操作</h2><ol><li>添加列</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE table_name ADD COLUMNS (col_name data_type )</span><br></pre></td></tr></table></figure><ol><li>修改列</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE table_name CHANGE a a1 INT</span><br></pre></td></tr></table></figure><ol><li>删除列 - 只会修改元数据，不会删除hdfs文件</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE table_name replace columns(name string);</span><br></pre></td></tr></table></figure><h2 id="🚗-分区操作"><a href="#🚗-分区操作" class="headerlink" title="🚗 分区操作"></a>🚗 分区操作</h2><ol><li>查看表的所有分区</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SHOW PARTITIONS table</span><br></pre></td></tr></table></figure><ol><li>删除分区</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE table_name DROP partition</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>为你的spark程序调优JAVA垃圾回收</title>
      <link href="/2020/09/05/%E4%B8%BA%E4%BD%A0%E7%9A%84spark%E7%A8%8B%E5%BA%8F%E8%B0%83%E4%BC%98JAVA%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/"/>
      <url>/2020/09/05/%E4%B8%BA%E4%BD%A0%E7%9A%84spark%E7%A8%8B%E5%BA%8F%E8%B0%83%E4%BC%98JAVA%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/</url>
      
        <content type="html"><![CDATA[<p>文章翻译自数砖 <a href="https://databricks.com/blog/2015/05/28/tuning-java-garbage-collection-for-spark-applications.html" target="_blank" rel="noopener">原文链接</a></p><p>Apache Spark 由于其优秀的性能，简单的 API 以及丰富的分析与计算库，被各大企业所广泛的使用。 就像很多其他的大数据系统一样，spark 也是运行在 Java 虚拟机之上（ JVM ）。由于 spark 需要存储大量的数据在内存中， 所以其十分依赖 Java 的内存管理和垃圾回收机制（GC）。新版本的 spark 会使用 Tungsten 来简化和优化内存的管理。在现阶段，了解 Java GC 选项和参数的用户可以来自主的调优他们的 spark 参数。本文主要讲解如果通过配置 JVM 的垃圾回收器来管理 spark， 并且用真实的使用案例来讲解如何通过 GC调优来提高 spark 程序的性能。我们主要关注于在调优 GC 时的关键因素， 例如回收的吞吐量和延迟。</p><h3 id="Spark-和-垃圾回收器的介绍"><a href="#Spark-和-垃圾回收器的介绍" class="headerlink" title="Spark 和 垃圾回收器的介绍"></a>Spark 和 垃圾回收器的介绍</h3><p>随着 spark 在企业中的广泛使用，spark 应用的稳定和性能调优问题日益受到更多的关注。由于 spark 是基于内存的运算策略，通常会使用 100 G 甚至是更多的内存， 这在一般的 java 程序还是很少见的。在大量使用 spark 的公司中，我们会碰到各种调整是关于 spark 在执行时候的 GC 问题的。比如垃圾回收花了很长的时间，造成我们的程序运行了非常长的时间，甚至是发生程序崩溃。在本文中，我们使用真实的案例，结合具体的问题，来讨论能够缓解这些问题，优化 spark 程序的 GC 调优方法。</p><p>Java 应用程序一般采用两种经典的垃圾收集策略之一：CMS 垃圾回收器和 ParallelOld 垃圾回收器。其性能表示在低延迟和高吞吐量上。这两种策略都有性能瓶颈：CMS GC 没有做压缩， 而 Parallel GC 仅仅能做全局的压缩，这也会导致相当长的暂停时间。一般来说实时程序推荐使用 CMS GC 而离线任务推荐使用 Parallel GC。</p><p>现如今，像 spark 这样的程序同时有实时计算和传统的离线计算，我们能否找到更好的收集器？Hotspot JVM 从1.6版本开始支持第三种垃圾回收器 ： Garbage-First GC ( G1GC )。 G1回收器被 Oracle 计划在未来代替 CMS GC。G1回收器的目标是为了实现 高吞吐量和低延迟。在我们讨论在spark中使用 G1 收集器时，我们先来了解一些 Java GC的背景。</p><h3 id="Java-如何如何进行垃圾回收？"><a href="#Java-如何如何进行垃圾回收？" class="headerlink" title="Java 如何如何进行垃圾回收？"></a>Java 如何如何进行垃圾回收？</h3><p>在传统的 JVM 管理中，堆空间被分为 <strong>年轻代</strong> 和 <strong>老年代</strong> 两种。而年轻代又分为 Eden 区和两个更小的servivor区。结构如下图所示。新创建的对象会被分配到 Eden 区。每一次进行 minor GC 的时候，JVM 都会复制还在 Eden 中存活的对象到一个空的 survivor 区中。 这个方法让一个 survivor 区来控制对象，而让另外一个 survivor 区域空置来为下一次垃圾回收做准备。在经过数次 minor GG后，仍然存活的对象将会被复制到老年代。当老年代填满之后，一次 major GC 会暂停所有的线程来进行 full GC，然后来移除老年代中的对象。这个暂停所有线程的操作叫做 Stop-The-World (STW)。大多数 GC 算法的性能损失都是发生在这个时候。</p><p><a href="https://xinze.fun/2020/01/11/为你的spark程序调优JAVA垃圾回收/Screen-Shot-2015-05-26-at-11.35.50-AM-1024x302.png"><img src="https://xinze.fun/2020/01/11/%E4%B8%BA%E4%BD%A0%E7%9A%84spark%E7%A8%8B%E5%BA%8F%E8%B0%83%E4%BC%98JAVA%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/Screen-Shot-2015-05-26-at-11.35.50-AM-1024x302.png" alt="img"></a></p><p>Java 新的 G1 回收器 完全改变了传统的 GC 方法。堆空间被划分为许多相同大小的堆区域。每一个区域都被分配相同的职责（Eden，survivor，old）在下一次的收集中，但是空间大小会发生改变。这带来了更灵活的内存使用。当一个对象被创建之后，他被分到到一个有空闲区间的区域中。当这个区域被填满后，JVM会创建一个新的区域来创建对象。当发生 minor GC 时， G1 复制活着的对象到一个或更多的区域。并且选择一些新的区域作为 Eden 区域。Full GC 仅仅在所有的区域都存放对象，并且没有其他空区域的的时候才会发生。G1在标记活动对象时使用“Remembered Sets（RSets）”概念。RSets使用外部的区域来监控每一个区域中对象的应用。在整个堆空间中只有一个 RSet 区域。RSet 提供完整的堆烧面，并且能够对每一个区域进行并行且独立的 GC。G1 GC 不仅能够提高在发生 full GC 时候的空间利用率，还能使 minor GC 的暂停时间得到控制，这一点对于大型内存情况比较友好。那么这些改变是如何影响 GC 性能的呢？下面我们通过简单的方法来观察性能的变化。例如通过对比老的GC 方法和 G1GC 的效果。</p><p><a href="https://xinze.fun/2020/01/11/为你的spark程序调优JAVA垃圾回收/Screen-Shot-2015-05-26-at-11.38.37-AM.png"><img src="https://xinze.fun/2020/01/11/%E4%B8%BA%E4%BD%A0%E7%9A%84spark%E7%A8%8B%E5%BA%8F%E8%B0%83%E4%BC%98JAVA%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/Screen-Shot-2015-05-26-at-11.38.37-AM.png" alt="img"></a></p><p>由于G1放弃了对年轻/老化对象使用固定堆分区的方法，因此我们必须相应地调整GC配置，以确保使用G1收集器的程序的平稳运行。 与旧的垃圾收集器不同，我们通常发现 G1 收集器的一个好的开始是不执行任何调整。 因此，我们建议仅从默认设置开始，然后仅通过-XX：+ UseG1GC选项启用G1。 我们发现有时有用的一项调整是，当应用程序使用多个线程时，最好使用-XX：-ResizePLAB关闭PLAB（）调整大小，并避免由于大量线程通信而导致性能下降。</p><p>如果想获得 JVM 的完整 GC 参数，可以使用参数 -XX: +PrintFlagsFinal 来控制打印。或者参照 Orcal 的官方文档的解释。</p><h3 id="理解-spark-的内存管理"><a href="#理解-spark-的内存管理" class="headerlink" title="理解 spark 的内存管理"></a>理解 spark 的内存管理</h3><p>RDD 是在 spark 中最核心的概念。RDD的创建和缓存于内存消耗有着直接的关系。spark 允许用户缓存 RDD 来重新使用。从而避免了重复计算带来的开销。 保留RDD的一种形式是将所有或部分数据缓存在JVM堆中。 Spark的执行程序将JVM堆空间分为两个部分：一个部分用于存储Spark应用程序持久性缓存到内存中的数据； 其余部分用作JVM堆空间，负责RDD转换期间的内存消耗。 我们可以使用spark.storage.memoryFraction参数调整这两个分数的比率，以使Spark通过确保所缓存的RDD的总大小不超过RDD堆空间量乘以该参数的值来控制其总大小。 RDD缓存部分的未使用部分也可以由JVM使用。 因此，针对Spark应用程序的GC分析应涵盖两个内存部分的内存使用情况。</p><p>当观察到由GC延迟导致的效率下降时，我们应该首先检查并确保Spark应用程序有效地使用了有限的内存空间。 RDD占用的内存空间越少，则留给程序执行的堆空间就越大，从而提高了GC效率； 相反，由于旧版本中大量的缓存对象，RDD占用的过多内存导致严重的性能损失。 在这里，我们拿一个例子对此进行解释：</p><p>例如，用户有一个基于Spark的Bagel组件的应用程序，该应用程序执行简单的迭代计算。一个步骤（迭代）的结果取决于上一个步骤的结果，因此每个步骤的结果将保留在内存空间中。在程序执行过程中，我们观察到，当迭代次数增加时，使用的内存空间会迅速增长，从而导致GC变得更糟。当我们仔细观察Bagel时，我们发现它将每个步骤的RDD缓存在内存中，而不会随着时间的流逝而释放它们，即使它们在一次迭代之后就没有使用。这导致内存消耗增加，从而触发更多的GC尝试。在 <a href="https://issues.apache.org/jira/i#browse/SPARK-2661?issueKey=SPARK-2661&serverRenderedViewIssue=true" target="_blank" rel="noopener">SPARK-2661</a>中删除了此不必要的缓存。修改缓存后，RDD大小将在经过三轮迭代后稳定下来，并且可以有效地控制缓存空间（如表所示）。结果，GC效率大大提高，程序的总运行时间缩短了10％〜20％。</p><table><thead><tr><th align="left">Iteration Number</th><th align="left">Cache Size of each Iteration</th><th align="left">Total Cache Size (Before Optimization)</th><th align="left">Total Cache Size (After Optimization)</th></tr></thead><tbody><tr><td align="left">Initialization</td><td align="left">4.3GB</td><td align="left">4.3GB</td><td align="left">4.3GB</td></tr><tr><td align="left">1</td><td align="left">8.2GB</td><td align="left">12.5GB</td><td align="left">8.2GB</td></tr><tr><td align="left">2</td><td align="left">98.8GB</td><td align="left">111.3 GB</td><td align="left">98.8GB</td></tr><tr><td align="left">3</td><td align="left">90.8GB</td><td align="left">202.1 GB</td><td align="left">90.8GB</td></tr></tbody></table><h3 id="选择垃圾收集器"><a href="#选择垃圾收集器" class="headerlink" title="选择垃圾收集器"></a>选择垃圾收集器</h3><p>如果我们的应用程序正在尽可能高效地使用内存，那么下一步就是调整我们选择的垃圾收集器。在实施SPARK-2661之后，我们建立了一个四节点集群，为每个 executor 分配了88GB的堆，并以独立模式启动Spark进行实验。我们从默认的Spark Parallel GC开始，发现由于Spark应用程序的内存开销相对较大，并且大多数对象无法在相当短的生命周期内回收，因此 Parallel GC 通常处于 full GC 中，这导致了每次 GC 时都会有性能下降。更糟糕的是，Parallel GC提供了非常有限的性能调整选项，因此我们只能使用一些基本参数来调整性能，例如每一代的大小比例以及将对象提升到旧一代之前的副本数。由于这些调整策略仅推迟了完整的GC，因此 Parallel GC调整对长期运行的应用程序几乎没有帮助。因此，在本文中，我们不会继续进行 Parallel GC调整。下表显示了 Parallel GC的操作，很明显，当执行完整GC时，CPU利用率最低。</p><table><thead><tr><th align="left"><strong>Configuration Options</strong></th><th align="left"><code>-XX:+UseParallelGC -XX:+UseParallelOldGC -XX:+PrintFlagsFinal -XX:+PrintReferenceGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintAdaptiveSizePolicy -Xms88g -Xmx88g</code></th></tr></thead><tbody><tr><td align="left"><strong>Stage*</strong></td><td align="left"><a href="https://xinze.fun/2020/01/11/为你的spark程序调优JAVA垃圾回收/Screen-Shot-2015-05-26-at-1.42.13-PM1.png"><img src="https://xinze.fun/2020/01/11/%E4%B8%BA%E4%BD%A0%E7%9A%84spark%E7%A8%8B%E5%BA%8F%E8%B0%83%E4%BC%98JAVA%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/Screen-Shot-2015-05-26-at-1.42.13-PM1.png" alt="img"></a></td></tr><tr><td align="left"><strong>Task*</strong></td><td align="left"><a href="https://xinze.fun/2020/01/11/为你的spark程序调优JAVA垃圾回收/Screen-Shot-2015-05-26-at-1.56.05-PM.png"><img src="https://xinze.fun/2020/01/11/%E4%B8%BA%E4%BD%A0%E7%9A%84spark%E7%A8%8B%E5%BA%8F%E8%B0%83%E4%BC%98JAVA%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/Screen-Shot-2015-05-26-at-1.56.05-PM.png" alt="img"></a></td></tr><tr><td align="left"><strong>CPU*</strong></td><td align="left"><a href="https://xinze.fun/2020/01/11/为你的spark程序调优JAVA垃圾回收/Screen-Shot-2015-05-26-at-1.57.56-PM.png"><img src="https://xinze.fun/2020/01/11/%E4%B8%BA%E4%BD%A0%E7%9A%84spark%E7%A8%8B%E5%BA%8F%E8%B0%83%E4%BC%98JAVA%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/Screen-Shot-2015-05-26-at-1.57.56-PM.png" alt="img"></a></td></tr><tr><td align="left"><strong>Mem*</strong></td><td align="left"><a href="https://xinze.fun/2020/01/11/为你的spark程序调优JAVA垃圾回收/Screen-Shot-2015-05-26-at-2.00.31-PM.png"><img src="https://xinze.fun/2020/01/11/%E4%B8%BA%E4%BD%A0%E7%9A%84spark%E7%A8%8B%E5%BA%8F%E8%B0%83%E4%BC%98JAVA%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/Screen-Shot-2015-05-26-at-2.00.31-PM.png" alt="img"></a></td></tr></tbody></table><p>CMS GC无法采取任何措施消除此Spark应用程序中的 full GC。 此外，CMS GC的 full GC 暂停时间比 Parallel GC长得多，这大大降低了应用程序的吞吐量。</p><p>接下来，我们使用<strong>默认</strong>的G1 GC配置运行我们的应用程序。 令我们惊讶的是，G1 GC还给出了不可接受的full GC（请参阅下表中的“ CPU利用率”，显然作业3暂停了将近100秒），并且长时间的暂停大大拖累了整个应用程序的运行。 如表所示，尽管总运行时间比 Parallel GC 略长，但G1 GC的性能略好于CMS GC。</p><table><thead><tr><th align="left"><strong>Configuration Options</strong></th><th align="left">-XX:+UseG1GC -XX:+PrintFlagsFinal -XX:+PrintReferenceGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintAdaptiveSizePolicy -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -Xms88g -Xmx88g</th></tr></thead><tbody><tr><td align="left"><strong>Stage*</strong></td><td align="left"><a href="https://xinze.fun/2020/01/11/为你的spark程序调优JAVA垃圾回收/stage-before.png"><img src="https://xinze.fun/2020/01/11/%E4%B8%BA%E4%BD%A0%E7%9A%84spark%E7%A8%8B%E5%BA%8F%E8%B0%83%E4%BC%98JAVA%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/stage-before.png" alt="img"></a></td></tr><tr><td align="left"><strong>Task*</strong></td><td align="left"><a href="https://xinze.fun/2020/01/11/为你的spark程序调优JAVA垃圾回收/task-before.png"><img src="https://xinze.fun/2020/01/11/%E4%B8%BA%E4%BD%A0%E7%9A%84spark%E7%A8%8B%E5%BA%8F%E8%B0%83%E4%BC%98JAVA%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/task-before.png" alt="img"></a></td></tr><tr><td align="left"><strong>CPU*</strong></td><td align="left"><a href="https://xinze.fun/2020/01/11/为你的spark程序调优JAVA垃圾回收/cpu-before.png"><img src="https://xinze.fun/2020/01/11/%E4%B8%BA%E4%BD%A0%E7%9A%84spark%E7%A8%8B%E5%BA%8F%E8%B0%83%E4%BC%98JAVA%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/cpu-before.png" alt="img"></a></td></tr><tr><td align="left"><strong>Mem*</strong></td><td align="left"><a href="https://xinze.fun/2020/01/11/为你的spark程序调优JAVA垃圾回收/mem-before.png"><img src="https://xinze.fun/2020/01/11/%E4%B8%BA%E4%BD%A0%E7%9A%84spark%E7%A8%8B%E5%BA%8F%E8%B0%83%E4%BC%98JAVA%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/mem-before.png" alt="img"></a></td></tr></tbody></table><table><thead><tr><th align="left">Garbage Collector</th><th align="left">Running Time for 88GB Heap</th></tr></thead><tbody><tr><td align="left">Parallel GC</td><td align="left">6.5min</td></tr><tr><td align="left">CMS GC</td><td align="left">9min</td></tr><tr><td align="left">G1 GC</td><td align="left">7.6min</td></tr></tbody></table><h3 id="基于-Log-来调优-G1-回收器"><a href="#基于-Log-来调优-G1-回收器" class="headerlink" title="基于 Log 来调优 G1 回收器"></a>基于 Log 来调优 G1 回收器</h3><p>设置G1 GC后，下一步是根据GC日志进一步调整收集器性能。</p><p>首先，我们希望JVM在GC日志中记录更多详细信息。 因此，对于Spark，我们将“ spark.executor.extraJavaOptions”设置为包括其他标志。 通常，我们需要设置以下选项：</p><p>-XX：+ PrintFlagsFinal -XX：+ PrintReferenceGC -verbose：gc -XX：+ PrintGCDetails -XX：+ PrintGCTimeStamps -XX：+ PrintAdaptiveSizePolicy -XX：+ UnlockDiagnosticVMOptions -XX：+ G1SummarizeConcMark</p><p>定义了这些选项后，我们会在Spark的执行程序日志中跟踪详细的GC日志和有效的GC选项。 接下来，我们可以根据GC日志分析问题的根本原因，并学习如何提高程序性能。</p><p>让我们看一下G1 GC日志的结构，例如，以G1 GC中的混合GC为例。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">251.354: [G1Ergonomics (Mixed GCs) continue mixed GCs, reason: candidate old regions available, candidate old regions: 363 regions, reclaimable: 9830652576 bytes (10.40 %), threshold: 10.00 %]</span><br><span class="line"></span><br><span class="line">[Parallel Time: 145.1 ms, GC Workers: 23]</span><br><span class="line"></span><br><span class="line">[GC Worker Start (ms): Min: 251176.0, Avg: 251176.4, Max: 251176.7, Diff: 0.7]</span><br><span class="line"></span><br><span class="line">[Ext Root Scanning (ms): Min: 0.8, Avg: 1.2, Max: 1.7, Diff: 0.9, Sum: 28.1]</span><br><span class="line"></span><br><span class="line">[Update RS (ms): Min: 0.0, Avg: 0.3, Max: 0.6, Diff: 0.6, Sum: 5.8]</span><br><span class="line"></span><br><span class="line">[Processed Buffers: Min: 0, Avg: 1.6, Max: 9, Diff: 9, Sum: 37]</span><br><span class="line"></span><br><span class="line">[Scan RS (ms): Min: 6.0, Avg: 6.2, Max: 6.3, Diff: 0.3, Sum: 143.0]</span><br><span class="line"></span><br><span class="line">[Object Copy (ms): Min: 136.2, Avg: 136.3, Max: 136.4, Diff: 0.3, Sum: 3133.9]</span><br><span class="line"></span><br><span class="line">[Termination (ms): Min: 0.0, Avg: 0.0, Max: 0.0, Diff: 0.0, Sum: 0.3]</span><br><span class="line"></span><br><span class="line">[GC Worker Other (ms): Min: 0.0, Avg: 0.1, Max: 0.2, Diff: 0.2, Sum: 1.9]</span><br><span class="line"></span><br><span class="line">[GC Worker Total (ms): Min: 143.7, Avg: 144.0, Max: 144.5, Diff: 0.8, Sum: 3313.0]</span><br><span class="line"></span><br><span class="line">[GC Worker End (ms): Min: 251320.4, Avg: 251320.5, Max: 251320.6, Diff: 0.2]</span><br><span class="line"></span><br><span class="line">[Code Root Fixup: 0.0 ms]</span><br><span class="line"></span><br><span class="line">[Clear CT: 6.6 ms]</span><br><span class="line"></span><br><span class="line">[Other: 26.8 ms]</span><br><span class="line"></span><br><span class="line">[Choose CSet: 0.2 ms]</span><br><span class="line"></span><br><span class="line">[Ref Proc: 16.6 ms]</span><br><span class="line"></span><br><span class="line">[Ref Enq: 0.9 ms]</span><br><span class="line"></span><br><span class="line">[Free CSet: 2.0 ms]</span><br><span class="line"></span><br><span class="line">[Eden: 3904.0M(3904.0M)-&gt;0.0B(4448.0M) Survivors: 576.0M-&gt;32.0M Heap: 63.7G(88.0G)-&gt;58.3G(88.0G)]</span><br><span class="line"></span><br><span class="line">[Times: user=3.43 sys=0.01, real=0.18 secs]</span><br></pre></td></tr></table></figure><p>从该日志中，我们可以看到G1 GC日志具有非常清晰的层次结构。 该日志列出了发生暂停的时间和原因，并对各个线程的时间消耗以及平均和最大CPU时间进行了分级。 最后，G1 GC会列出此暂停后的清理结果以及总的时间消耗。</p><p>在当前的G1 GC运行日志中，我们找到一个类似以下的特殊块：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">(to-space exhausted), 1.0552680 secs]</span><br><span class="line"></span><br><span class="line">[Parallel Time: 958.8 ms, GC Workers: 23]</span><br><span class="line"></span><br><span class="line">[GC Worker Start (ms): Min: 759925.0, Avg: 759925.1, Max: 759925.3, Diff: 0.3]</span><br><span class="line"></span><br><span class="line">[Ext Root Scanning (ms): Min: 1.1, Avg: 1.4, Max: 1.8, Diff: 0.6, Sum: 33.0]</span><br><span class="line"></span><br><span class="line">[SATB Filtering (ms): Min: 0.0, Avg: 0.0, Max: 0.3, Diff: 0.3, Sum: 0.3]</span><br><span class="line"></span><br><span class="line">[Update RS (ms): Min: 0.0, Avg: 1.2, Max: 2.1, Diff: 2.1, Sum: 26.9]</span><br><span class="line"></span><br><span class="line">[Processed Buffers: Min: 0, Avg: 2.8, Max: 11, Diff: 11, Sum: 65]</span><br><span class="line"></span><br><span class="line">[Scan RS (ms): Min: 1.6, Avg: 2.5, Max: 3.0, Diff: 1.4, Sum: 58.0]</span><br><span class="line"></span><br><span class="line">[Object Copy (ms): Min: 952.5, Avg: 953.0, Max: 954.3, Diff: 1.7, Sum: 21919.4]</span><br><span class="line"></span><br><span class="line">[Termination (ms): Min: 0.0, Avg: 0.1, Max: 0.2, Diff: 0.2, Sum: 2.2]</span><br><span class="line"></span><br><span class="line">[GC Worker Other (ms): Min: 0.0, Avg: 0.0, Max: 0.0, Diff: 0.0, Sum: 0.6]</span><br><span class="line"></span><br><span class="line">[GC Worker Total (ms): Min: 958.1, Avg: 958.3, Max: 958.4, Diff: 0.3, Sum: 22040.4]</span><br><span class="line"></span><br><span class="line">[GC Worker End (ms): Min: 760883.4, Avg: 760883.4, Max: 760883.4, Diff: 0.0]</span><br><span class="line"></span><br><span class="line">[Code Root Fixup: 0.0 ms]</span><br><span class="line"></span><br><span class="line">[Clear CT: 0.4 ms]</span><br><span class="line"></span><br><span class="line">[Other: 96.0 ms]</span><br><span class="line"></span><br><span class="line">[Choose CSet: 0.0 ms]</span><br><span class="line"></span><br><span class="line">[Ref Proc: 0.4 ms]</span><br><span class="line"></span><br><span class="line">[Ref Enq: 0.0 ms]</span><br><span class="line"></span><br><span class="line">[Free CSet: 0.1 ms]</span><br><span class="line"></span><br><span class="line">[Eden: 160.0M(3904.0M)-&gt;0.0B(4480.0M) Survivors: 576.0M-&gt;0.0B Heap: 87.7G(88.0G)-&gt;87.7G(88.0G)]</span><br><span class="line"></span><br><span class="line">[Times: user=1.69 sys=0.24, real=1.05 secs]</span><br><span class="line"></span><br><span class="line">760.981: [G1Ergonomics (Heap Sizing) attempt heap expansion, reason: allocation request failed, allocation request: 90128 bytes]</span><br><span class="line"></span><br><span class="line">760.981: [G1Ergonomics (Heap Sizing) expand the heap, requested expansion amount: 33554432 bytes, attempted expansion amount: 33554432 bytes]</span><br><span class="line"></span><br><span class="line">760.981: [G1Ergonomics (Heap Sizing) did not expand the heap, reason: heap expansion operation failed]</span><br><span class="line"></span><br><span class="line">760.981: [Full GC 87G-&gt;36G(88G), 67.4381220 secs]</span><br></pre></td></tr></table></figure><p>如我们所见，最大的性能下降是由这样一个 full GC 引起的，并且在日志中以 To-Space Exhausted，To-space Overflow或类似的形式输出（对于各种JVM版本，输出可能看起来略有不同）。 原因是，当G1 GC收集器尝试为某些区域收集垃圾时，它无法找到可以将活动对象复制到的空闲区域。 这种情况称为疏散失败，通常会导致GC满载。 显然，G1 GC中的 full GC比 Parallel GC 还要差，因此我们必须避免使用 Parallel GC才能获得更好的性能。 为了避免在G1 GC中使用完全GC，有两种常用方法：</p><ol><li>减小InitiatingHeapOccupancyPercent选项的值（默认值为45），以使G1 GC在较早的时间开始初始并发标记，以便我们更有可能避免使用完整的GC。</li><li>增加ConcGCThreads选项的值，以有更多线程用于并行标记，这样我们可以加快并行标记阶段。 请注意，根据您的工作负载CPU利用率，此选项还会占用一些有效的工作线程资源。</li></ol><p>调整这两个选项可以最大程度地减少发生 full GC的可能性。 消除了完全GC后，性能得到了极大提高。 但是，我们在GC期间仍然发现了长时间的停顿。 经过进一步调查，我们在日志中发现以下情况：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">280.008: [G1Ergonomics (Concurrent Cycles) request concurrent cycle initiation, reason: occupancy higher than threshold, occupancy: 62344134656 bytes, allocation request: 46137368 bytes, threshold: 42520176225 bytes (45.00 %), source: concurrent humongous allocation]</span><br></pre></td></tr></table></figure><p>在这里，我们看到了巨大的对象（对象的大小为标准区域的50％或更大）。 G1 GC会将这些对象中的每一个放置在连续的区域集中。而且由于复制这些对象会消耗大量资源，因此大型对象被直接分配到老年代区域中（绕过所有年轻的GC），然后被分类为大型区域。在1.8.0_u40之前，需要完整的堆活动性分析以回收大型区域。如果有很多这样的对象，堆将很快被填满，而回收它们太昂贵了。即使进行了修复（它们确实大大提高了回收大型对象的效率），但连续区域的分配仍然更加昂贵（尤其是在遇到严重的堆碎片时），因此我们要避免创建这种大小的对象。我们可以增加G1HeapRegionSize的值来减少创建巨型区域的可能性，但是如果我们使用相对较大的堆，则默认值已经是其最大大小32M。这意味着我们只能分析程序才能找到这些对象并最小化它们的创建。否则，可能会导致更多的并发标记阶段，此后，您需要仔细调整与混合GC相关的配置（例如，-XX：G1HeapWastePercent -XX：G1MixedGCLiveThresholdPercent），以避免长时间的混合GC暂停（由许多巨大的对象引起） 。</p><p>接下来，我们可以分析从混合周期开始到结束的单个GC周期的间隔。 如果时间太长，可以考虑增加ConcGCThreads的值，但是请注意，这将占用更多的CPU资源。</p><p>G1 GC还具有减少STW暂停长度的方法，以换取在垃圾回收的并发阶段中做更多的工作。 如上所述，G1 GC为每个区域维护一个Remembered Set（RSet），以通过外部区域将对象引用跟踪到给定区域，并且G1收集器在STW阶段和并发阶段都更新RSets。 如果要通过G1 GC减少STW暂停的长度，则可以在增加G1ConcRefinementThreads的值的同时减小G1RSetUpdatingPauseTimePercent的值。 选项G1RSetUpdatingPauseTimePercent用于指定RSets更新时间在整个STW时间中的理想比率，默认为10％，而G1ConcRefinementThreads用于定义在程序运行期间用于维护RSets的线程数。 通过调整这两个选项，我们可以将更多的RSets更新工作负载从STW阶段转移到并发阶段。</p><p>另外，对于长时间运行的应用程序，我们使用AlwaysPreTouch选项，因此JVM在启动时将所需的所有内存应用于操作系统，并避免使用动态应用程序。 这样以延长启动时间为代价提高了运行时性能。</p><p>最终，经过几轮GC参数调整，我们得出了下表中的结果。与之前的结果相比，我们最终获得了更令人满意的运行效率。</p><table><thead><tr><th align="left"><strong>Configuration Options</strong></th><th align="left"><code>-XX:+UseG1GC -XX:+PrintFlagsFinal -XX:+PrintReferenceGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintAdaptiveSizePolicy -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -Xms88g -Xmx88g -XX:InitiatingHeapOccupancyPercent=35 -XX:ConcGCThread=20</code></th></tr></thead><tbody><tr><td align="left"><strong>Stage*</strong></td><td align="left"><a href="https://xinze.fun/2020/01/11/为你的spark程序调优JAVA垃圾回收/stage-after.png"><img src="https://xinze.fun/2020/01/11/%E4%B8%BA%E4%BD%A0%E7%9A%84spark%E7%A8%8B%E5%BA%8F%E8%B0%83%E4%BC%98JAVA%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/stage-after.png" alt="img"></a></td></tr><tr><td align="left"><strong>Task*</strong></td><td align="left"><a href="https://xinze.fun/2020/01/11/为你的spark程序调优JAVA垃圾回收/task-after.png"><img src="https://xinze.fun/2020/01/11/%E4%B8%BA%E4%BD%A0%E7%9A%84spark%E7%A8%8B%E5%BA%8F%E8%B0%83%E4%BC%98JAVA%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/task-after.png" alt="img"></a></td></tr><tr><td align="left"><strong>CPU*</strong></td><td align="left"><a href="https://xinze.fun/2020/01/11/为你的spark程序调优JAVA垃圾回收/cpu-after.png"><img src="https://xinze.fun/2020/01/11/%E4%B8%BA%E4%BD%A0%E7%9A%84spark%E7%A8%8B%E5%BA%8F%E8%B0%83%E4%BC%98JAVA%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/cpu-after.png" alt="img"></a></td></tr><tr><td align="left"><strong>Mem*</strong></td><td align="left"><a href="https://xinze.fun/2020/01/11/为你的spark程序调优JAVA垃圾回收/mem-after.png"><img src="https://xinze.fun/2020/01/11/%E4%B8%BA%E4%BD%A0%E7%9A%84spark%E7%A8%8B%E5%BA%8F%E8%B0%83%E4%BC%98JAVA%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/mem-after.png" alt="img"></a></td></tr></tbody></table><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>对于严重依赖内存计算的Spark应用程序，GC调整尤为重要。当GC出现问题时，请勿着急调试GC本身。首先要考虑Spark程序的内存管理效率低下，例如持久性和释放RDD在缓存中。在调整垃圾收集器时，我们首先建议使用G1 GC运行Spark应用程序。 G1收集器已做好充分准备，可以处理Spark经常看到的不断增长的堆大小。使用G1，将需要更少的选项来提供更高的吞吐量和更低的延迟。当然，GC调整没有固定的模式。各种应用程序具有不同的特性，并且为了应对不可预测的情况，必须掌握根据日志和其他取证方法进行GC调整的技术。最后，我们不能忘记通过程序的逻辑和代码进行优化，例如减少中间对象的创建或复制，控制大型对象的创建，将长期存在的对象存储在堆外等等。</p><p>通过使用G1 GC，我们在Spark应用程序中实现了重大性能改进。 Spark的未来工作将把内存管理职责从Java的垃圾收集器转移到Spark本身。 这将减轻许多Spark应用程序的调整要求。 尽管如此，今天选择垃圾收集器可以提高关键任务Spark应用程序的性能。</p>]]></content>
      
      
      <categories>
          
          <category> spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark Shuffle 和 Spill 的区别</title>
      <link href="/2020/09/05/Spark-Shuffle-%E5%92%8C-Spill-%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
      <url>/2020/09/05/Spark-Shuffle-%E5%92%8C-Spill-%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
      
        <content type="html"><![CDATA[<p>本文翻译自 <a href="https://xuechendi.github.io/" target="_blank" rel="noopener"><strong>Chendi Xue’s Blog</strong></a>，<a href="https://xuechendi.github.io/2019/04/15/Spark-Shuffle-and-Spill-Explained" target="_blank" rel="noopener">原文链接</a>　</p><h5 id="spark-shuffle-做了什么事情"><a href="#spark-shuffle-做了什么事情" class="headerlink" title="spark shuffle 做了什么事情?"></a>spark shuffle 做了什么事情?</h5><p>shuffle是指 map 任务和 task 任务之间的流程。在后文我们提到 shuffling 都是指对数据做shuffle 的处理。</p><h5 id="为什么数据需要-shuffle"><a href="#为什么数据需要-shuffle" class="headerlink" title="为什么数据需要 shuffle?"></a>为什么数据需要 shuffle?</h5><p>我们首先来举个例子说明。假如我们需要统计美国各个州中每一个社区的GDP。那么我们最终的结果应该类似于 (Manhattan -&gt; xxx billion; Beverly Hills -&gt;xxx billion) 这样子的形式。</p><p>由于我们需要计算的社区非常的多，那么我们必然需要借助 terasort 这样的算法来帮助我们排名。整个任务的流程如下:</p><ol><li>map 任务从 hdfs 上拉取数据， 并且计算这些数据都是属于哪一个城市。比如某个社区属于 NewYork， 那么就把数据放到 NewYork 的桶里。</li><li>当所有的 map 任务完成后，也就意味着所有的社区都进入了对于的城市 bucket 中。所有的桶内的数据都像图中左边展示的那样。不同的颜色代表不同的城市。</li><li>这些 buckets 就是需要 shuffle 的数据!</li><li>之后，redue 任务就开始工作了，每一个 reduce 任务负责处理一个城市的数据。它会读取每一个 map任务写入的对应城市的数据。并且在同时还会对拉取的数据进行排序。</li><li>一个所有的 bucket 数据处理完成(如图中右边所示)，我们也就得到了对应每个城市的根据GDP排名的数据。</li></ol><p><img src="/2020/09/05/Spark-Shuffle-和-Spill-的区别/shuffle_explained.jpg" alt="img"></p><p>因此，在 spark UI 中，让一个任务需要 shuffling 的时候，它总是会被切分成两个 stage 。 包括一个 map stage 和 一个 reduce stage. 如下图所示。</p><p><img src="/2020/09/05/Spark-Shuffle-和-Spill-的区别/shuffle_DAG.jpg" alt></p><h5 id="怎样估计有多少数据需要被-shuffle-呢"><a href="#怎样估计有多少数据需要被-shuffle-呢" class="headerlink" title="怎样估计有多少数据需要被 shuffle 呢?"></a>怎样估计有多少数据需要被 shuffle 呢?</h5><p>shuffle 是 spark 的在相同或不同的 executor 节点交换内部 map 和 reduce 任务数据的过程。 map 任务负责写任务(shuffle write) ，而 reduce 任务则会拉取数据(shuffle read) . 因此 shuffle 数据的大小依赖于我们结果的需求。</p><p>假如我们现在的任务是 rank，就就是需要把没有排序的 (社区，GDP) 这样的数据，变成输出为 根据GDP排好序的结果。那么需要 shuffle 的数据就是这部分数据的压缩或者序列化的结果。</p><p>假如我们需要的结果是一个城市的总GDP，并且输入还是没有排序的社区GDP数据，那么 shuffle 的数据就是一个记录了每个社区GDP总和的list。</p><p>我们也可以通过 <strong>spark UI</strong> 来追踪 shuffle 数据的数量。</p><p>如果你想做一个预估，可以这样进行简单的计算: 比如我们的输入数据是 hdfs 上面 256m 的 block， 并且这样的数据总共有 100 G. 那么我们基本上可以预估需要 100GB/256MB=400 个map任务。 并且每一个 map 任务都包含了 256M 的数据。 这些数据之后会被通过序列化放入不同的城市的 bucket 中。因此我们也可以看到 shuffle write 的数据也接近 256M 的大小，这个值由于序列化的原因会稍微大一点。</p><p>之后当我们开始 reduce 时，reduce task 就会从所有 map task 中读取对应城市的记录。那么每一个 reduce task 的 shuffle read 数据应该是一个城市所有记录的总和。这个数据根据不同城市的社区数量的不同而不同。</p><p><img src="/2020/09/05/Spark-Shuffle-和-Spill-的区别/shuffle_screenshot.jpg" alt></p><h5 id="spark-spill-all-又做了什么呢"><a href="#spark-spill-all-又做了什么呢" class="headerlink" title="spark spill all 又做了什么呢?"></a>spark spill all 又做了什么呢?</h5><p>Spilling 是spark 从硬盘上读写数据的情况之一。还有一个情况是 spark 内存不够用的时候。</p><ol><li>当要进行 shuffle 的时候， 我们并不是把每一条数据都写入到磁盘中，一般来说我们会首先把数据写得这个城市的bucket对应在内存中的位置，如果内存设置了上限阈值，超出的部分数据就会被写入到磁盘中。</li><li>除了做 shuffle 的工作，spark还有一种操作叫做 External Sorter， 他会给每一个城市的桶做一次 TimeSort，因此这个过程需要较大的内存支持，当内存不充足的时候，它会把数据写入到磁盘中并重新进行新一轮的插入排序。最后对在磁盘和内存中的所有数据进行 merge sort。</li></ol><p><img src="/2020/09/05/Spark-Shuffle-和-Spill-的区别/external_sort_and_spill_explained.jpg" alt></p><h5 id="怎样估计有多少数据会被-spill-呢"><a href="#怎样估计有多少数据会被-spill-呢" class="headerlink" title="怎样估计有多少数据会被 spill 呢?"></a>怎样估计有多少数据会被 spill 呢?</h5><p>这取决于你的 JVM 有多少的可用内存。 spark的初始阈值是 5M 来尝试把在内存中进行 insert sort 的数据 spill 到磁盘中。</p><p><img src="/2020/09/05/Spark-Shuffle-和-Spill-的区别/spill_screenshot.jpg" alt></p><h5 id="Spark-Shuffle-DataFlow-细节"><a href="#Spark-Shuffle-DataFlow-细节" class="headerlink" title="Spark Shuffle DataFlow 细节"></a>Spark Shuffle DataFlow 细节</h5><p>在经过上文的解释之后，让我们来看一下具体的流程图。</p><p>无论是 shuffle write 还是 external spill，目前 spark 都是通过 <a href="https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/storage/DiskBlockObjectWriter.scala" target="_blank" rel="noopener">DiskBlockObkectWriter</a> 来控制数据的 kyro 序列化缓存和 达到阈值后的写入磁盘。</p><p>当从文件中读数据时，<a href="https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala" target="_blank" rel="noopener">shuffle read</a> 以不同的方式处理 相同节点的数据读取和 其他网络节点的数据读取。 相同节点的 数据读取会以 <a href="https://github.com/apache/spark/blob/master/common/network-common/src/main/java/org/apache/spark/network/buffer/FileSegmentManagedBuffer.java" target="_blank" rel="noopener">FileSegmentManagedBuffer</a> 的形式来拉取数据， 而远程节点则会以<a href="https://github.com/apache/spark/blob/master/common/network-common/src/main/java/org/apache/spark/network/buffer/NettyManagedBuffer.java" target="_blank" rel="noopener">NettyManagedBuffer</a> 的形式来拉取数据。</p><p>对于排序数据的读取，spark 会先 return 一个 排序过的 RDD 的迭代器， 然后读取操作会定义在<a href="https://github.com/apache/spark/blob/d4420b455ab81b86c29fc45a3107e45873c72dc2/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala#L577" target="_blank" rel="noopener">interator.hasNext()</a> 函数中，以惰性的方式来读取数据。</p><p><img src="/2020/09/05/Spark-Shuffle-和-Spill-的区别/spark_shuffle_dataflow.jpg" alt></p>]]></content>
      
      
      <categories>
          
          <category> spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> shuffle </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>git 使用进阶指南</title>
      <link href="/2020/09/05/git-%E4%BD%BF%E7%94%A8%E8%BF%9B%E9%98%B6%E6%8C%87%E5%8D%97/"/>
      <url>/2020/09/05/git-%E4%BD%BF%E7%94%A8%E8%BF%9B%E9%98%B6%E6%8C%87%E5%8D%97/</url>
      
        <content type="html"><![CDATA[<p>git 是作为一名程序员必不可少的工具。它能帮助你更好的管理你的代码。既保证了你的代码不丢失，又能保证能够第一时间获取到其他同事更新的代码。这两个作用我认为是 git 在我们的日常工作中最重要的能力。写这篇文章是想记录自己从一个只会 git pull &amp; git push 的小白变成能够熟练使用 git 各种命令的老鸟😋。</p><p>本文是一个进阶命令的收集，因此阅读前请先掌握 git 的基本知识和常见命令。</p><p>ok，废话不多说，👇就开始骚操作吧。</p><h2 id="🎁Case-：假设leader让你做一点点bugFix，-但是仓库中还有一些自己的其他改动怎么办？"><a href="#🎁Case-：假设leader让你做一点点bugFix，-但是仓库中还有一些自己的其他改动怎么办？" class="headerlink" title="🎁Case ：假设leader让你做一点点bugFix， 但是仓库中还有一些自己的其他改动怎么办？"></a>🎁Case ：假设leader让你做一点点bugFix， 但是仓库中还有一些自己的其他改动怎么办？</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git stash -&gt; Stash the changes in a dirty working directory away</span><br></pre></td></tr></table></figure><p>想想一下，自己工作干的好好的，repo管理的井井有条，这时候突然接到一个临时的更新需求。任务需要把当前目录的一些文件做修改并且提交，这时候应该怎么办呢？总不能撤回自己辛辛苦苦码的代码然后再提交吧？一个一个的手动提交修改的文件又太麻烦。这时候 git stash 命令就可以派上用场了。</p><p>这个命令的作用是把我们的一些修改隐藏在一个dirty的目录中。这个不难理解，就像是有一个盒子可以把我们的修改藏起来，等我们把这个目录需要做的改动提交之后再把我们的东西拿出来就可以了。与它配合的命令就是</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git stash pop  // 取出我们stash的change</span><br></pre></td></tr></table></figure><p>有了这两个命令我们就可以随便的让我们的<strong>长期工作流</strong>和<strong>临时工作流</strong>不打架了。提交临时的修改时只需要stash一下我们之前的工作就可以了。</p><h2 id="🎁Case-git-命令太长了怎么办？一个字都懒得输"><a href="#🎁Case-git-命令太长了怎么办？一个字都懒得输" class="headerlink" title="🎁Case: git 命令太长了怎么办？一个字都懒得输"></a>🎁Case: git 命令太长了怎么办？一个字都懒得输</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git Aliases -&gt; 就像配置你的mac快捷键一样配置git</span><br></pre></td></tr></table></figure><p>在我们使用mac或者linux的时候，配置Aliases是一个很方便的技巧，我们可以把自己很常用的命令用几个很简单的字母代替。同样的，git 也可以把常用的命令都设置几个简短的字母。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git config --global alias.co checkout</span><br><span class="line">git config --global alias.br branch</span><br><span class="line">git config --global alias.ci commit</span><br><span class="line">git config --global alias.st status</span><br></pre></td></tr></table></figure><p>比如👆设置的status命令， 我们现在只需要使用 git st 就可以实现原来相同的功能了。对于懒癌晚期患者来说，这个技能简直是必不可少的了。通过这个技巧，我们还可以把原来很长的操作变短，用一个方便记的短语来代替。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git config --global alias.unstage &apos;reset HEAD~1&apos;</span><br></pre></td></tr></table></figure><p>通过这个设置我们就可以简化reset命令， 输入 git unstage 就可以撤回我们刚刚 commit 的内容。</p><h2 id="🎁Case-文件夹中有太多的untrack文件怎么办？一键干掉！"><a href="#🎁Case-文件夹中有太多的untrack文件怎么办？一键干掉！" class="headerlink" title="🎁Case: 文件夹中有太多的untrack文件怎么办？一键干掉！"></a>🎁Case: 文件夹中有太多的untrack文件怎么办？一键干掉！</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clean -&gt; 将所有没有追踪的文件都给清除掉</span><br></pre></td></tr></table></figure><p>有时候文件夹中出现了莫名其妙的文件，而是自己确定不需要的文件，那么在将需要的新文件add之后就可以大胆的使用 git clean 命令啦，把烦人的文件统统干掉，不留痕迹。</p><blockquote><p>需要注意的是 clean 命令只能清除 untrack 类型的文件</p></blockquote><h2 id="🎁Case-项目中的文件不想要了，直接删掉可以嘛？"><a href="#🎁Case-项目中的文件不想要了，直接删掉可以嘛？" class="headerlink" title="🎁Case: 项目中的文件不想要了，直接删掉可以嘛？"></a>🎁Case: 项目中的文件不想要了，直接删掉可以嘛？</h2><p>答案当然是不可以的，如果直接通过系统删除掉文件，那么git 是不会接受到通知的，那git的树目录中自然也会保留着那个文件的一席之地。如果你不想让这个文件一直出现在你的视野中的话，那么请使用git命令来删除它。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git rm  -&gt; 通过git命令删除掉文件</span><br></pre></td></tr></table></figure><p>如果你只是单纯的想给文件改名或者移动它，那么最好也请使用git的命令。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git mv -&gt; 移动文件</span><br></pre></td></tr></table></figure><p>git rm 和 git mv 其实是跟linux命令没什么区别的，只是前面都加了一个git，只要在平时留意即可。</p><h2 id="🎁Case-平时项目开发，到底怎么样比较安全？"><a href="#🎁Case-平时项目开发，到底怎么样比较安全？" class="headerlink" title="🎁Case: 平时项目开发，到底怎么样比较安全？"></a>🎁Case: 平时项目开发，到底怎么样比较安全？</h2><p>既然要保证自己的开发与别人的流程不冲突，那么切分支是必然的。保证自己的开发在一个分支，而平时的push 和 pull 在 master 分支就可以最大程度的与其他人分离开。如果为了方便的话，那么只需要开一个feature分支就可以了，平时不断的进行 master 分支和 feature 分支的合并就可以把自己的工作合并进主分支。</p><p><strong>step 1: 新建 feature分支并切换</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout -b feature</span><br></pre></td></tr></table></figure><p><strong>step2: Work</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">do somethiing</span><br><span class="line">git add *</span><br><span class="line">git commit -m &apos;f1&apos;</span><br></pre></td></tr></table></figure><p><strong>step3: 回到 master 分支更新最新的代码</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git checkout master</span><br><span class="line">git pull --rebase</span><br></pre></td></tr></table></figure><p><strong>step4: 切换回 feature 分支并rebase master</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git checkout feature</span><br><span class="line">git rebase master</span><br></pre></td></tr></table></figure><p><strong>step5: 切换回 master 合并 feature 分支并 push</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git checkout master</span><br><span class="line">git merge feature</span><br><span class="line">git push</span><br></pre></td></tr></table></figure><p>以上⬆️的步骤虽然是有点啰嗦，但应该是比较标准的开发流程了。如果想保证自己的开发工作不出现意外的话，最好还是不要省掉这些步骤，以免出现不必要的麻烦。</p><h2 id="🎁Case-commit-提交的-message-写错了咋办？加个参数就可以搞定！"><a href="#🎁Case-commit-提交的-message-写错了咋办？加个参数就可以搞定！" class="headerlink" title="🎁Case: commit 提交的 message 写错了咋办？加个参数就可以搞定！"></a>🎁Case: commit 提交的 message 写错了咋办？加个参数就可以搞定！</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git commit --amend -m &apos;true message&apos;</span><br></pre></td></tr></table></figure><p><code>--amend</code> 顾名思义就是修改的意思，通过使用这个参数来进行一次 commit 实际上是对上一次 commit 的修改，也就是说如果你没有任何操作，仅仅是重新提交一次，那么你的新的message 就会覆盖掉上一次commit 的 message， 本质上这还是属于同一个message。</p><p>当然，这个参数可不仅仅是修改下 message 这么简单的操作，如果你还想在本次 commit 中追加一些其他的改动，那么你只需要按照正常的步骤使用 git add 将这部分文件提交的 stage 后，重新进行一次 commit， 并且只需要在这个时候hexo 使用<code>--amend</code>命令来进行提交，就可以把上次的 commit 和上次的 commit 进行合并。因此这个功能还是非常实用的。</p><h2 id="🎁Case-如果我提交错了，那么该咋撤回呢？"><a href="#🎁Case-如果我提交错了，那么该咋撤回呢？" class="headerlink" title="🎁Case: 如果我提交错了，那么该咋撤回呢？"></a>🎁Case: 如果我提交错了，那么该咋撤回呢？</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git reset --mixed / --soft / --hard</span><br></pre></td></tr></table></figure><p>git reset 命令可以让我们的仓库发生一次时光倒流，仓库中的文件回到什么状态取决于你想让他回到哪里，只要你确定了需要最后回到的位置，那么就只需要在上面👆的命令后面加上对应commit 的 hash值前几位即可，或者是用相对位置 HEAD～3这样类型的形式也是可以的。</p><p>那么你列出来的<code>--mixed</code> <code>--soft</code> <code>--hard</code> 这三个参数又是啥意思呢？其实这个只是对应让我们的仓库回到不同的文件状态，因为在一个commit之前我们的文件也是会经历很多变化的。具体经过的状态变化可以看👇</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git commit 1 -&gt; 状态1  -&gt; 修改文件 -&gt; 状态2 -&gt; git add * -&gt; 状态3 -&gt; git commit 2</span><br></pre></td></tr></table></figure><p>在两次 commit 之中，其实我们的文件状态会经历三次变化，那么上面对应的三个参数也就是对应了回到不同的状态位置。 在完成了 git commit 2 之后， 如果只是使用<code>git reset</code>命令，那么 默认的是 –mixed，该值会把我们的文件系统会到 <code>状态2</code>， 如果是配置了参数<code>--soft</code> ，那么文件系统会恢复到<code>状态3</code>， 而如果想彻彻底底的把上一次commit的东西给完全删除掉，不留任何痕迹，那么就可以使用<code>--hard</code>，这样就可以恢复到<code>状态1</code>。需要注意的是，如果在这一次的修改中存在新建的文件，那么即使是使用命令<code>-- hard</code>也不会将它删除，需要手动的再使用一次<code>git clean</code>来清除才可以。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git reset HEAD^  -&gt;  状态2</span><br><span class="line">git reset --soft HEAD^  -&gt;  状态3</span><br><span class="line">git reset --hard HEAD^  -&gt; 状态1</span><br></pre></td></tr></table></figure><h2 id="🎁Case-如果某个文件想从远程服务器删除，但是本地还要保留咋办？"><a href="#🎁Case-如果某个文件想从远程服务器删除，但是本地还要保留咋办？" class="headerlink" title="🎁Case: 如果某个文件想从远程服务器删除，但是本地还要保留咋办？"></a>🎁Case: 如果某个文件想从远程服务器删除，但是本地还要保留咋办？</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git rm --cached filename # 使文件变成untracked状态</span><br><span class="line">git commit &quot;xxxx&quot;</span><br><span class="line">git push</span><br></pre></td></tr></table></figure><p>前面我们已经讲过使用 <code>git rm</code>命令的作用是删除掉仓库中的某个文件，它不仅仅是从我们的本地仓库中删除了，还会直接在文件目录中删除这个文件。而如果我们单纯只是想要从仓库的 working directory 中删除这个文件，那么就可以添加<code>--cached</code>参数。</p><p>执行<code>git rm --cached filename</code> 这个命令后，这个文件就会变成<code>untracked</code>的状态，这时候重新<code>commit</code>并且<code>push</code>,那么我们的本地和远程仓库中也就不存在这个文件了。</p><p>（Still not Finish……）</p>]]></content>
      
      
      <categories>
          
          <category> git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink使用17-如何自定义各种UDF并在SQL中使用</title>
      <link href="/2019/11/28/flink%E4%BD%BF%E7%94%A817-%E5%A6%82%E4%BD%95%E8%87%AA%E5%AE%9A%E4%B9%89%E5%90%84%E7%A7%8DUDF%E5%B9%B6%E5%9C%A8SQL%E4%B8%AD%E4%BD%BF%E7%94%A8/"/>
      <url>/2019/11/28/flink%E4%BD%BF%E7%94%A817-%E5%A6%82%E4%BD%95%E8%87%AA%E5%AE%9A%E4%B9%89%E5%90%84%E7%A7%8DUDF%E5%B9%B6%E5%9C%A8SQL%E4%B8%AD%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<p>今天主要讲一下Flink SQL 中怎样使用 UDF ，目前1.9版本可用的UDF包括Scalar Function 、 Table Function 、 Aggregateion Function 、 Table Aggregation Function。完整代码见仓库 -&gt; <a href="https://github.com/CheckChe0803/flink-simple-tutorial/tree/master/table/src/main/java/udf" target="_blank" rel="noopener">Github</a></p><h3 id="如果注册一个-UDF？"><a href="#如果注册一个-UDF？" class="headerlink" title="如果注册一个 UDF？"></a>如果注册一个 UDF？</h3><p>注册的方法很简单，使用 <code>TableEnvironment</code>的registerFunction()方法就可以了。</p><h3 id="Scalar-Function"><a href="#Scalar-Function" class="headerlink" title="Scalar Function"></a>Scalar Function</h3><p>scalar Function 是一个一对一的转换。</p><p>这里我们以一个求字符串长度的方式来演示，首先需要继承ScalarFunction,需要重新实现eval()方法。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StringLength</span> <span class="keyword">extends</span> <span class="title">ScalarFunction</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">eval</span><span class="params">(String s)</span></span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (s == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> s.length();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>之后只需要注册该方法就可以使用了</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tEnv.registerFunction(<span class="string">"stringLength"</span>,<span class="keyword">new</span> StringLength());</span><br><span class="line"></span><br><span class="line">tEnv.sqlQuery(<span class="string">"select word,stringLength(word) from t"</span>);</span><br></pre></td></tr></table></figure><h3 id="Table-Function"><a href="#Table-Function" class="headerlink" title="Table Function"></a>Table Function</h3><p>Table Function是将一行数据转为多行数据的一个方法。</p><p>在这里用一个将一个字符串切割成多行的例子来演示</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StringSplit</span> <span class="keyword">extends</span> <span class="title">TableFunction</span>&lt;<span class="title">Row</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">eval</span><span class="params">(String str)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (String s : str.split(<span class="string">"#"</span>)) &#123;</span><br><span class="line">            Row row = <span class="keyword">new</span> Row(<span class="number">2</span>);</span><br><span class="line">            row.setField(<span class="number">0</span>, s);</span><br><span class="line">            row.setField(<span class="number">1</span>, s.length());</span><br><span class="line">            <span class="comment">// 使用collect（）来收集需要返回的数据</span></span><br><span class="line">            collect(row);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> TypeInformation&lt;Row&gt; <span class="title">getResultType</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> RowTypeInfo(Types.STRING,Types.INT);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>需要注意在实现eval（）方法的同时，也需要实现getResultType（）方法，用来告诉flink返回值的类型，方便其转换。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tEnv.registerFunction(<span class="string">"split"</span>, <span class="keyword">new</span> StringSplit());</span><br><span class="line">tEnv.sqlQuery(<span class="string">"SELECT a,word, length FROM t, LATERAL TABLE(split(a)) as T(word, length)"</span>);</span><br></pre></td></tr></table></figure><p><strong>LATERAL TABLE</strong>(split(a)) 的作用相当于 join。</p><h3 id="Aggregateion-Function"><a href="#Aggregateion-Function" class="headerlink" title="Aggregateion Function"></a>Aggregateion Function</h3><p> Aggregateion Function函数顾名思义是用来做聚合操作的。</p><p>这里我用一个求均值的函数来演示</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Mean</span> <span class="keyword">extends</span> <span class="title">AggregateFunction</span>&lt;<span class="title">Integer</span>, <span class="title">MeanValue</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Integer <span class="title">getValue</span><span class="params">(MeanValue accumulator)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> accumulator.sum/accumulator.count;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> MeanValue <span class="title">createAccumulator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> MeanValue();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">accumulate</span><span class="params">(MeanValue acc, <span class="keyword">int</span> iValue)</span> </span>&#123;</span><br><span class="line">        acc.sum += iValue;</span><br><span class="line">        acc.count ++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">merge</span><span class="params">(MeanValue acc, Iterable&lt;MeanValue&gt; it)</span> </span>&#123;</span><br><span class="line">        Iterator&lt;MeanValue&gt; iter = it.iterator();</span><br><span class="line">        <span class="keyword">while</span> (iter.hasNext()) &#123;</span><br><span class="line">            MeanValue a = iter.next();</span><br><span class="line">            acc.count += a.count;</span><br><span class="line">            acc.sum += a.sum;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">resetAccumulator</span><span class="params">(MeanValue acc)</span> </span>&#123;</span><br><span class="line">        acc.count = <span class="number">0</span>;</span><br><span class="line">        acc.sum = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Aggregateion Function 与流处理的Aggregateion Function也是很类似的，都需要自己创建累加器，之后就是控制对累加器的操作就可以了。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">tEnv.registerFunction(<span class="string">"get_mean"</span>, <span class="keyword">new</span> Mean());</span><br><span class="line">tEnv.sqlQuery(<span class="string">"SELECT name,get_mean(v) as mean_value FROM t3 GROUP BY name"</span>);</span><br></pre></td></tr></table></figure><p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/table/udfs.html" target="_blank" rel="noopener">参考文章1</a></p><p><a href="https://cloud.tencent.com/developer/article/1172926" target="_blank" rel="noopener">参考文章2</a></p>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
            <tag> udf </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用Docker搭建伪分布式Hbase(外置Zookeeper)</title>
      <link href="/2019/11/20/%E4%BD%BF%E7%94%A8Docker%E6%90%AD%E5%BB%BA%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8FHbase-%E5%A4%96%E7%BD%AEZookeeper/"/>
      <url>/2019/11/20/%E4%BD%BF%E7%94%A8Docker%E6%90%AD%E5%BB%BA%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8FHbase-%E5%A4%96%E7%BD%AEZookeeper/</url>
      
        <content type="html"><![CDATA[<p>本文是为了记录项目搭建一个可用的 Hbase镜像的过程，由于项目中还有独立的kafka和zookeeper， 所以不能使用常见的集成zk的Hbase镜像，故手动搭建了一个。本文记录在整个搭建过程的Bug。</p><p>最开始，在Docker hub 上面发现了一个<a href="https://hub.docker.com/r/pierrezemb/hbase-docker" target="_blank" rel="noopener">镜像</a></p><p>它已经做了standalone模式的hbase，包含集成zk的版本以及外置zk的版本，所以首先尝试用他的这个镜像来实现，Docker-compose文件如下：</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cat</span> <span class="string">docker-compose.yml</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line"><span class="attr">  zookeeper:</span></span><br><span class="line"><span class="attr">    image:</span> <span class="string">wurstmeister/zookeeper</span></span><br><span class="line"><span class="attr">    expose:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"2181"</span></span><br><span class="line"><span class="attr">    logging:</span></span><br><span class="line"><span class="attr">        driver:</span> <span class="string">"none"</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  hbase:</span></span><br><span class="line"><span class="attr">    image:</span> <span class="string">pierrezemb/hbase-docker:standalone-1.3.1</span></span><br><span class="line"><span class="attr">    links:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">zookeeper</span></span><br><span class="line"><span class="attr">    logging:</span></span><br><span class="line"><span class="attr">      driver:</span> <span class="string">"none"</span></span><br><span class="line"><span class="attr">    ports:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"16010:16010"</span></span><br><span class="line"><span class="attr">    expose:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"8080"</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"9090"</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"16000"</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"16010"</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"16020"</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"16030"</span></span><br><span class="line"><span class="attr">    command:</span> <span class="string">["/wait-for-it.sh",</span> <span class="string">"zookeeper:2181"</span><span class="string">,</span> <span class="string">"-t"</span><span class="string">,</span> <span class="string">"10"</span><span class="string">,</span> <span class="string">"--"</span><span class="string">,</span> <span class="string">"/usr/bin/supervisord"</span><span class="string">]</span></span><br><span class="line"><span class="comment"># ...</span></span><br></pre></td></tr></table></figure><p>为了能让远程客户端连接，我们把其他端口也做一个映射</p><p>启动后发现并不能连接，原因是在于 Hbase 的通信机制，其连接 Regionserver是通过Hostname:Port 的形式来连接的,zookeeper返回的地址就是（Hostname:Port：startID）的格式，可以在zk上面找到（list /hbase/rs）。因此如果我们使用的是Docker容器本身hostname，显然是无法通过客户端连接到Hbase的。</p><p><img src="/2019/11/20/使用Docker搭建伪分布式Hbase-外置Zookeeper/hbase%E5%AE%A2%E6%88%B7%E7%AB%AF%E8%BF%9E%E6%8E%A5.png" alt></p><p>因此，为了能够正确的连接到 Habse， 首先应该配置正确的hostname， 我们知道通过docker run 的命令直接启动容器的时候，通过 -h hostname 参数就可以设置容器的hostname。 在Docker-compose文件中，我们可以直接设置hostname参数来控制。</p><p><img src="/2019/11/20/使用Docker搭建伪分布式Hbase-外置Zookeeper/hbase-web.png" alt></p><p>这是如果通过 Java 客户端直接连接的话还是会发现无法连接，原因在于端口位置是错误的，可以通过上图发现 hbase为regionserver随机启动了一个端口，原因在于这个镜像版本启动的是standalone模式的hbase，在hbase的高版本的stabdalone模式端口是随机启动的，即便是在hbase-site.xml文件中配置<strong>hbase.regionserver.port</strong>或者<strong>hbase.master.port</strong>都是不行的，必须启动分布式模式才行。</p><p>由于这个版本的镜像是独立式的，所以必然会出现这个问题，只能重新搭建一个伪分布式版本的Hbase，其实现可以参考我的这个<a href>仓库</a>。</p><p>在hbase-site.xml文件中，添加如下配置</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.cluster.distributed<span class="tag">&lt;/<span class="name">name</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.regionserver.ipc.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>0.0.0.0<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.master.ipc.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>0.0.0.0<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>与此同时，还必须在hbase-env.sh中专门设置JAVA_HOME。</p><p>重新构建镜像并启动后，就可以在 Hbase的WebUI上看到启动正确的端口号了，这时候就可以正常的通过 Java 客户端来连接我们的这个Hbase容器了。</p>]]></content>
      
      
      <categories>
          
          <category> docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
            <tag> hbase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用Docker部署Flink大数据项目</title>
      <link href="/2019/11/19/%E4%BD%BF%E7%94%A8Docker%E9%83%A8%E7%BD%B2Flink%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%A1%B9%E7%9B%AE/"/>
      <url>/2019/11/19/%E4%BD%BF%E7%94%A8Docker%E9%83%A8%E7%BD%B2Flink%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%A1%B9%E7%9B%AE/</url>
      
        <content type="html"><![CDATA[<p>本文是为<a href="https://github.com/CheckChe0803/flink-recommandSystem-demo" target="_blank" rel="noopener">基于Flink的商品推荐系统</a>所搭建的Docker环境，目的为了方便体验项目，一键部署项目需要的所有环境，并预填充所需要的数据。完整的环境包括Zookeeper 、Kafka 、 Hbase 、 Mysql 、 Redis 、 Flink 。</p><h2 id="环境介绍："><a href="#环境介绍：" class="headerlink" title="环境介绍："></a>环境介绍：</h2><ul><li>zookeeper 3.4.5</li><li>kafka 2.12-2.2.1</li><li>hbase 1.5.0</li><li>mysql 8.0.18</li><li>redis</li><li>flink 1.9.0</li></ul><p><img src="/2019/11/19/使用Docker部署Flink大数据项目/%E5%AE%B9%E5%99%A8%E7%8E%AF%E5%A2%83.png" alt></p><p>整个项目的部署和工作环境如上图所示。由于 kafka 和 hbase 均需要 zookeeper 的支持，所以没有使用集成式的 hbase docker镜像，所以笔者自己基于 hbase 1.5.0 制作了一版镜像，由于 hbase 的 standlone模式端口会随机生成，故也搭建成了伪分布式，HMaster 和 HRegionserver均在同一个 Docker Container 中。 在Kafka容器中，配置了对内对外两个地址，这是为了方便远程的队列消费，以及在容器内自动生产消息。</p><h2 id="搭建步骤："><a href="#搭建步骤：" class="headerlink" title="搭建步骤："></a>搭建步骤：</h2><h3 id="1-拉取镜像并启动"><a href="#1-拉取镜像并启动" class="headerlink" title="1. 拉取镜像并启动"></a>1. 拉取镜像并启动</h3><p>首先请下载 <a href="https://github.com/CheckChe0803/flink-recommandSystem-demo/tree/master/resources/docker/docker-compose" target="_blank" rel="noopener">目录</a> 内的所有文件，因为mysql和hbase需要插入数据，kafka需要启动一个shell脚本自动的模拟用户点击行为。对应的请修改docker-compose.yml文件中对应的文件地址为自己所保存的地址。</p><p><img src="/2019/11/19/使用Docker部署Flink大数据项目/%E4%BF%AE%E6%94%B9volumes.png" alt></p><p>首先请保证已经成功安装了docker和docker-compose，其次请将docker-compose.yml文件中的 hbase 的 <code>hostname</code> 修改为自己的主机名。</p><p><strong>最后通过docker-compose启动所有容器</strong>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-linux recommend]# docker-compose up -d</span><br></pre></td></tr></table></figure><h3 id="2-给-Mysql-Redis-填充数据，给-Hbase-创建表结构"><a href="#2-给-Mysql-Redis-填充数据，给-Hbase-创建表结构" class="headerlink" title="2. 给 Mysql / Redis 填充数据，给 Hbase 创建表结构"></a>2. 给 Mysql / Redis 填充数据，给 Hbase 创建表结构</h3><p>进入mysql container， 连接mysql</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-linux recommend]# docker exec -ti mysql bash</span><br><span class="line">root@6fdc02332cd0:/# mysql -u root -p</span><br></pre></td></tr></table></figure><p>通过提前制作好的sql文件填充mysql</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span> source /opt/contact.sql</span><br></pre></td></tr></table></figure><p>之后是构建 hbase 表结构</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-linux recommend]# docker exec -ti hbase bash</span><br><span class="line">root@docker-linux:/opt/hbase# hbase shell /opt/hbase_ini.sql</span><br></pre></td></tr></table></figure><p>通过 list 命令可以看到我们创建的表都已成功</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):001:0&gt; list</span><br><span class="line">TABLE                                                             ··· </span><br><span class="line">8 row(s) in 0.1390 seconds</span><br><span class="line">=&gt; ["con", "p_history", "prod", "ps", "px", "u_history", "u_interest", "user"]</span><br></pre></td></tr></table></figure><p>最后进入redis，创建10个热度数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-linux recommend]# docker exec -ti redis bash</span><br><span class="line">root@7735aff1391f:/bin# redis-cli</span><br></pre></td></tr></table></figure><p>启动客户端后，之后按照如下格式创建 set 0  123之类的创建从0-9的10条数据即可，最后一位为商品id，在0-999内任意取值。</p><h3 id="3-启动-Kafka-消息生成器"><a href="#3-启动-Kafka-消息生成器" class="headerlink" title="3. 启动 Kafka 消息生成器"></a>3. 启动 Kafka 消息生成器</h3><p>进入kafka容器并启动shell脚本即可，脚本会按照每秒一次的频率发送message到log这个topic里。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-linux recommend]# docker exec -ti kafka bash</span><br><span class="line">bash-4.4# sh /opt/generator.sh -d</span><br></pre></td></tr></table></figure><p>验证成功发送消息的方法是重新开一个连接进入kafka容器并启动Consumer，注意kafka内部端口号配置的是9093</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash-4.4# $KAFKA_HOME/bin/kafka-console-consumer.sh --bootstrap-server kafka:9093 --topic log</span><br></pre></td></tr></table></figure><p>正确看到如下格式的消息即可</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">558,559,1574180802,1</span><br><span class="line">576,576,1574180819,3</span><br><span class="line">585,585,1574180828,3</span><br><span class="line">594,594,1574180837,3</span><br><span class="line">603,603,1574180846,3</span><br><span class="line">613,613,1574180856,1</span><br></pre></td></tr></table></figure><h2 id="执行任务："><a href="#执行任务：" class="headerlink" title="执行任务："></a>执行任务：</h2><p>我们这个项目一共有6个flink的任务，如果只是在本地测试的话，可以直接在IDEA中启动对应的任务即可，为了做实验，我们把项目打包并提交到 Docker 的 Flink 中，为了方便就不再把各种依赖单独放到集群中，而是直接打到任务的jar包中。正确的打包方法可以参考<a href="https://xinze.fun/2019/11/14/flink%E4%BD%BF%E7%94%A816-%E6%AD%A3%E7%A1%AE%E6%89%93%E5%8C%85Flink%E7%A8%8B%E5%BA%8F%E5%B9%B6%E4%BD%BF%E7%94%A8Cli%E6%8F%90%E4%BA%A4%E4%BB%BB%E5%8A%A1/">这篇博客</a></p><p>我们直接把打包好的jar包放到集群中，并拷贝到flink的docker中(jobmanager)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-linux pkg]# docker cp ./logtask.jar flink_jobmanager:/opt/</span><br></pre></td></tr></table></figure><p>然后进入flink jobmanager的容器中，提交jar包到集群</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-linux pkg]# docker exec -ti flink_jobmanager bash</span><br><span class="line">root@0f5edbeeecb9:/opt/flink# flink run /opt/logtask.jar</span><br></pre></td></tr></table></figure><p>提交成功后可以在Flink WebUI观察我们提交的任务</p><p><img src="/2019/11/19/使用Docker部署Flink大数据项目/flink%E7%95%8C%E9%9D%A2.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
            <tag> flink-recommend-demo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink使用16-正确打包Flink程序并使用Cli提交任务</title>
      <link href="/2019/11/14/flink%E4%BD%BF%E7%94%A816-%E6%AD%A3%E7%A1%AE%E6%89%93%E5%8C%85Flink%E7%A8%8B%E5%BA%8F%E5%B9%B6%E4%BD%BF%E7%94%A8Cli%E6%8F%90%E4%BA%A4%E4%BB%BB%E5%8A%A1/"/>
      <url>/2019/11/14/flink%E4%BD%BF%E7%94%A816-%E6%AD%A3%E7%A1%AE%E6%89%93%E5%8C%85Flink%E7%A8%8B%E5%BA%8F%E5%B9%B6%E4%BD%BF%E7%94%A8Cli%E6%8F%90%E4%BA%A4%E4%BB%BB%E5%8A%A1/</url>
      
        <content type="html"><![CDATA[<p>本文的计划是使用正确的maven插件打包当前教程代码库batch模块下的WordCount代码，并通过命令行的方式提交到Flink来启动任务。WordCount类即为Flink主方法类，该部分代码是Flink官方example的简单修改，只是对map方法填加了一点sleep来方便观察运行情况。</p><p><img src="/2019/11/14/flink使用16-正确打包Flink程序并使用Cli提交任务/package.png" alt></p><p>项目的运行环境使用Docker来部署Flink， Flink镜像可以从Docker hub上拉去，其Docker-Compose文件如下：</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">"2.1"</span></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line"><span class="attr">  jobmanager:</span></span><br><span class="line"><span class="attr">    image:</span> <span class="string">$&#123;FLINK_DOCKER_IMAGE_NAME:-flink&#125;</span></span><br><span class="line"><span class="attr">    expose:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"6123"</span></span><br><span class="line"><span class="attr">    ports:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"8081:8081"</span></span><br><span class="line"><span class="attr">    command:</span> <span class="string">jobmanager</span></span><br><span class="line"><span class="attr">    environment:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">JOB_MANAGER_RPC_ADDRESS=jobmanager</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  taskmanager:</span></span><br><span class="line"><span class="attr">    image:</span> <span class="string">$&#123;FLINK_DOCKER_IMAGE_NAME:-flink&#125;</span></span><br><span class="line"><span class="attr">    expose:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"6121"</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"6122"</span></span><br><span class="line"><span class="attr">    depends_on:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">jobmanager</span></span><br><span class="line"><span class="attr">    command:</span> <span class="string">taskmanager</span></span><br><span class="line"><span class="attr">    links:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"jobmanager:jobmanager"</span></span><br><span class="line"><span class="attr">    environment:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">JOB_MANAGER_RPC_ADDRESS=jobmanager</span></span><br></pre></td></tr></table></figure><p>正确启动Flink之后，就可以在WebUI上看到我们的环境了。</p><p><img src="/2019/11/14/flink使用16-正确打包Flink程序并使用Cli提交任务/webUI.png" alt></p><p>下面就开始打包我们的应用程序了。</p><p>官方推荐我们使用<code>maven-shade-plugin</code>插件，复制一下代码到POM中指定我们的主方法类即可。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-shade-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>shade<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">artifactSet</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">excludes</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>com.google.code.findbugs:jsr305<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>org.slf4j:*<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>log4j:*<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">excludes</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">artifactSet</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">filters</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">filter</span>&gt;</span></span><br><span class="line">                                <span class="comment">&lt;!-- Do not copy the signatures in the META-INF folder.</span></span><br><span class="line"><span class="comment">                                Otherwise, this might cause SecurityExceptions when using the JAR. --&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">artifact</span>&gt;</span>*:*<span class="tag">&lt;/<span class="name">artifact</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">excludes</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.SF<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.DSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                    <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.RSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;/<span class="name">excludes</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">filters</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">transformers</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">transformer</span> <span class="attr">implementation</span>=<span class="string">"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">mainClass</span>&gt;</span>my.programs<span class="tag">&lt;/<span class="name">mainClass</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">transformer</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">transformers</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure><p>需要注意的是一般来说我们是不会将flink的一些相关的包直接打到项目里，通常有两种方案：</p><ul><li><p>将相关的jar包统一都放到<strong>flink/lib</strong>目录下</p></li><li><p>构建一个单独的common模块，所以使用到的包都放在这个模块中打包并上传到集群，之后其他模块只需要引用该common模块即可</p><p>具体的操作可以见<a href="https://mp.weixin.qq.com/s?__biz=MzUzMDYwOTAzOA==&mid=2247483672&idx=1&sn=8721acbc6df78caa36d3c758b50d6115&chksm=fa4e67f9cd39eeef4d0b53f058caade35fb724e1f261b98c55c66c944c5d082c1bf11884e5d4&mpshare=1&scene=1&srcid=&sharer_sharetime=1573729663613&sharer_shareid=f92bcee4884a1d62558833c4d5a5d308&key=b6271393c1b0a8ed7bff3bcfc8c3573298bc16bcdc7abea801f7bf8209bbdeb460398de24395a8aafd6e4fac0c082c837ccd4c8ad0debd61292defec7d9710ecb833b5e09af4a7ef11f64e4f9f4fc184&ascene=1&uin=MTgyMTgxNDcwMQ%3D%3D&devicetype=Windows+10&version=62070152&lang=zh_CN&pass_ticket=XKnNFqmGAAZ3n6MGPrD0XK7e1jY0p43XBBMsX0bVXKGEz2t9Z%2FLTlxxc%2FhsS%2FejF" target="_blank" rel="noopener">这篇文章</a></p></li></ul><p>打包好后就可以直接是用 FLink Cli 提交到集群来开始job了 。</p><p>Flink Cli 一般来讲主要作用有：提交并执行任务、取消任务、获取任务状态信息、列出正在运行和等待的任务、触发savepoint等。</p><p>我们将已经打包好的jar包放到docker中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker cp /opt/flink/wordcount.jar flink_jobmanager_1:/opt/</span><br></pre></td></tr></table></figure><p>然后就可以通过命令行启动任务了，启动完成后我们可以在webUI上看到任务的执行情况。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker exec -ti flink_jobmanager_1 bash -c 'flink run /opt/wordcount.jar'</span><br></pre></td></tr></table></figure><p><img src="/2019/11/14/flink使用16-正确打包Flink程序并使用Cli提交任务/runningJob.png" alt></p><p>Flink Cli 的命令有很多，具体的内容可以参考官网示例：</p><p><a href="https://ci.apache.org/projects/flink/flink-docs-stable/ops/cli.html" target="_blank" rel="noopener">Flink Cli Examples</a></p>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
            <tag> package </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用Docker部署Kafka时的网络应该如何配置</title>
      <link href="/2019/11/11/%E4%BD%BF%E7%94%A8Docker%E9%83%A8%E7%BD%B2Kafka%E6%97%B6%E7%9A%84%E7%BD%91%E7%BB%9C%E5%BA%94%E8%AF%A5%E5%A6%82%E4%BD%95%E9%85%8D%E7%BD%AE/"/>
      <url>/2019/11/11/%E4%BD%BF%E7%94%A8Docker%E9%83%A8%E7%BD%B2Kafka%E6%97%B6%E7%9A%84%E7%BD%91%E7%BB%9C%E5%BA%94%E8%AF%A5%E5%A6%82%E4%BD%95%E9%85%8D%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<p>本文是我在使用Docker部署kafka遇到一些问题之后，在网上看到的一篇比较优秀的英文资料<a href="https://www.kaaproject.org/kafka-docker" target="_blank" rel="noopener">Link</a>。借此翻译一下这篇文章，也帮助自己搞懂在使用Docker时遇到的一些网络问题，尤其是Host怎样配置。</p><p>作者的Kafka使用环境是Kafka Producer 和 Broker 均在 Docker 网络中， Kafka Consumer 在宿主机环境中。结构如下图这样子：</p><p><img src="/2019/11/11/使用Docker部署Kafka时的网络应该如何配置/use-case.png" alt></p><p>首先，我从Docker hub 中找到了一个Kafka Docker image。 我使用的是Wurstmeister <a href="https://hub.docker.com/r/wurstmeister/kafka/" target="_blank" rel="noopener">Kafka</a> and <a href="https://hub.docker.com/r/wurstmeister/zookeeper/" target="_blank" rel="noopener">ZooKeeper</a> images， 然后Docker-compose的文件是按照下面的格式定义的：</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">'2'</span></span><br><span class="line"></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  zookeeper:</span></span><br><span class="line"><span class="attr">    image:</span> <span class="string">wurstmeister/zookeeper:3.4.6</span></span><br><span class="line"><span class="attr">    expose:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">"2181"</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  kafka:</span></span><br><span class="line"><span class="attr">    image:</span> <span class="string">wurstmeister/kafka:2.11-2.0.0</span></span><br><span class="line"><span class="attr">    depends_on:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">zookeeper</span></span><br><span class="line"><span class="attr">    ports:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">"9092:9092"</span></span><br><span class="line"><span class="attr">    environment:</span></span><br><span class="line"><span class="attr">      KAFKA_ADVERTISED_LISTENERS:</span> <span class="attr">PLAINTEXT://localhost:9092</span></span><br><span class="line"><span class="attr">      KAFKA_LISTENERS:</span> <span class="attr">PLAINTEXT://0.0.0.0:9092</span></span><br><span class="line"><span class="attr">      KAFKA_ZOOKEEPER_CONNECT:</span> <span class="attr">zookeeper:2181</span></span><br></pre></td></tr></table></figure><p>启动Docker，然后按照<a href="https://kafka.apache.org/quickstart" target="_blank" rel="noopener">kafka QuickStart</a> 的步骤来使用<code>kafka-console-producer.sh</code> 和 <code>kafka-console-consumer.sh</code></p><p>在宿主机中运行 Producer 的结果：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">andrew@host$</span> bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test</span><br><span class="line"><span class="meta">&gt;</span>Hi there!</span><br><span class="line"><span class="meta">&gt;</span>It is a test message.</span><br></pre></td></tr></table></figure><p>在宿主机中运行 Consumer的结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">andrew@host$ bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning</span><br><span class="line">Hi there!</span><br><span class="line">It&apos;s a test message.</span><br></pre></td></tr></table></figure><p>可以看到，不管是producer还是consumer都可以在宿主机网络中正常工作。需要注意的是<code>KAFKA_ADVERTISED_LISTENERS</code>这个环境变量在Compose文件中的设置.Kafka会在client第一次连接的时候把这个变量值发送给client。在接收到这个变量值之后，client就可以使用它来从kafka 的 broker 中消费或者生产数据了。</p><p>由于我们定义的变量值为 <code>PLAINTEXT://localhost:9092</code>， producer和consumer在初始化连接时都会使用它并且之后所有的通信都会通过 9092 这个端口。</p><p><img src="/2019/11/11/使用Docker部署Kafka时的网络应该如何配置/client-on-host-kafka-in-docker-wrong.png" alt></p><p>这里的关键要点是客户端使用指定的Kafka地址（<code>--bootstrap-server</code> and <code>--broker-list</code>的值）。Kafka之后冲顶下他们的值为<code>KAFKA_ADVERTISED_LISTENERS</code></p><p>下面让我们在运行Kafka容器的同一Docker网络内的任意Docker容器内运行Producer：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">oot@869f83f2f265:/kafka# bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test</span><br><span class="line"><span class="meta">&gt;</span>Hi there!</span><br><span class="line">[2018-10-10 14:37:40,397] WARN [Producer clientId=console-producer] Connection to node -1 could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)</span><br></pre></td></tr></table></figure><p>这时候就会报错了。这时候发生的事情是client收到了<code>KAFKA_ADVERTISED_LISTENERS</code>这个环境变量的值（<code>PLAINTEXT://localhost:9092</code>）,之后尝试去连接它然后发现失败了，因为在自己docker网络中并没有这个地址。显然，从client的角度来说可以通过<code>kafka:9092</code>这个地址来连接kafka。因此，为了使 client 能够和 broker 通信，<code>KAFKA_ADVERTISED_LISTENERS</code>这个变量值就必须设置为<code>PLAINTEXT://kafka:9092</code>,那么，下面就来重新构建一下我们的Compose文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">kafka:</span><br><span class="line">    image: wurstmeister/kafka:2.11-2.0.0</span><br><span class="line">    depends_on:</span><br><span class="line">    - zookeeper</span><br><span class="line">    ports:</span><br><span class="line">    - "9092:9092"</span><br><span class="line">    environment:</span><br><span class="line">      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092</span><br><span class="line">      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092</span><br><span class="line">      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181</span><br></pre></td></tr></table></figure><p>然后测试producer：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">oot@7dfb9eaa81dc:/kafka# bin/kafka-console-producer.sh --broker-list kafka:9092 --topic test</span><br><span class="line"><span class="meta">&gt;</span>Hi there!</span><br></pre></td></tr></table></figure><p>成功！现在我们可以在Docker容器内使用Producer发送消息了。</p><p>下面我们来尝试通过宿主机上的Consumer消费Kafka的数据：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">andrew@host$</span> bin/kafka-console-consumer.sh --bootstrap-server kafka:9092 --topic test --from-beginning</span><br><span class="line">[2018-10-10 23:57:06,827] WARN Removing server kafka:9092 from bootstrap.servers as DNS resolution failed for kafka (org.apache.kafka.clients.ClientUtils)</span><br></pre></td></tr></table></figure><p>正如预料的，此时consumer并不能连接到broker因为宿主机并不能识别<code>kafka:9092</code>这个地址。我们需要再重新设置一下上面的Compose文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">kafka:</span><br><span class="line">  image: wurstmeister/kafka:2.11-2.0.0</span><br><span class="line">  depends_on:</span><br><span class="line">  - zookeeper</span><br><span class="line">  ports:</span><br><span class="line">  - "9092:9092"</span><br><span class="line">  expose:</span><br><span class="line">  - "9093"</span><br><span class="line">  environment:</span><br><span class="line">    KAFKA_ADVERTISED_LISTENERS: INSIDE://kafka:9093,OUTSIDE://localhost:9092</span><br><span class="line">    KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT</span><br><span class="line">    KAFKA_LISTENERS: INSIDE://0.0.0.0:9093,OUTSIDE://0.0.0.0:9092</span><br><span class="line">    KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181</span><br><span class="line">    KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE</span><br></pre></td></tr></table></figure><p>下面让我们来解释一下上面的几个环境变量：</p><ul><li><p><code>KAFKA_ADVERTISED_LISTENERS</code></p><p>kafka的broker将监听地址表（<code>0.0.0.0:9093</code>, <code>0.0.0.0:9092</code>）和 listener （<code>INSIDE</code>, <code>OUTSIDE</code>）</p></li><li><p><code>KAFKA_ADVERTISED_LISTENERS</code></p><p>指向Broker的可用地址列表，kafka 将会在初始连接时将地址发送给client</p></li><li><p><code>KAFKA_LISTENER_SECURITY_PROTOCOL_MAP</code></p><p>将上面定义的 listener 名称（INSIDE，OUTSIDE）映射到PLAINTEXT Kafka协议。</p></li><li><p><code>KAFKA_INTER_BROKER_LISTENER_NAME</code></p><p>指向跨Broker间通信时的命名地址</p></li></ul><p>这里我们定义了两个listeners（<code>INSIDE://0.0.0.0:9093</code>, <code>OUTSIDE://0.0.0.0:9092</code>）来分别表示Docker网络内部的流量和Docker主机外部的流量。我们为跨Broker间的通信定义了 INSIDE listener。通过<code>KAFKA_ADVERTISED_LISTENERS</code> 和<code>KAFKA_LISTENER_SECURITY_PROTOCOL_MAP</code></p><p>我们将<code>PLAINTEXT://kafka:9093</code>发送给那些使用<code>kafka:9093</code>连接的客户端和</p><p><code>PLAINTEXT://localhost:9092</code>发送给那些使用<code>localhost:9092</code>连接的客户端。</p><p>总之，我们定义了两种类型的客户端-内部和外部-并且配置kafka返回不同的地址给对应的客户端。</p><p>现在我们再来尝试下在Docker网络中使用Producer：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@7dfb9eaa81dc:/kafka# bin/kafka-console-producer.sh --broker-list kafka:9093 --topic test</span><br><span class="line"><span class="meta">&gt;</span>Hi there!</span><br></pre></td></tr></table></figure><p>在宿主机中使用Consumer：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">andrew@host$</span> bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning</span><br><span class="line">Hi there!</span><br></pre></td></tr></table></figure><p>现在全部都是成功的了！</p><p>producer和kafka broker都是在docker网络的内部。</p><p><img src="/2019/11/11/使用Docker部署Kafka时的网络应该如何配置/client-and-kafka-in-docker.png" alt></p><p>Consumer在外部， borker在Docker网络内部</p><p><img src="/2019/11/11/使用Docker部署Kafka时的网络应该如何配置/client-on-host-kafka-in-docker.png" alt></p><p>现在，当服务和kafka 部署在不同的网络环境中的时候，我们也知道该如何去配置Docker了。</p>]]></content>
      
      
      <categories>
          
          <category> kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
            <tag> kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink使用15-在Flink SQL中连接kafka和mysql</title>
      <link href="/2019/11/10/flink%E4%BD%BF%E7%94%A815-%E5%9C%A8Flink-SQL%E4%B8%AD%E8%BF%9E%E6%8E%A5kafka%E5%92%8Cmysql/"/>
      <url>/2019/11/10/flink%E4%BD%BF%E7%94%A815-%E5%9C%A8Flink-SQL%E4%B8%AD%E8%BF%9E%E6%8E%A5kafka%E5%92%8Cmysql/</url>
      
        <content type="html"><![CDATA[<p>本文主要介绍如何使用 FLink SQL 自己的 DDL语言来构建基于 kafka 的表和 基于Mysql 的表，并直接把从 kafka 接过来的 Json 格式的数据转换为 表结构后直接写入到Mysql，有了这样的经验之后，大家可以自行修改 DML操作来实现不同的业务。文章内容参考了一些阿里云邪大佬的文章<a href="http://wuchong.me/blog/2019/09/02/flink-sql-1-9-read-from-kafka-write-into-mysql/" target="_blank" rel="noopener">Link</a>，写的很好。</p><p>环境配置如下：</p><ul><li>zookeeper : 3.4.6</li><li>kafka: 2.12-2.2.1</li><li>mysql: 8.0</li></ul><p>以上三个组件全部是通过Docker搭建，我的环境是使用VirtualBox搭建的Centos7虚拟机，在虚拟机上安装Docker， 之后在本地主机IDE内调试代码。其中遇到了不少坑，主要是FLink与Kafka的通信，可以参考我的Docker-Compose文件的配置，已经解决了网络问题。注意<code>KAFKA_ADVERTISED_LISTENERS</code>的地址修改成自己的虚拟机IP地址。</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">'2.1'</span></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line"><span class="attr">  zookeeper:</span></span><br><span class="line"><span class="attr">    image:</span> <span class="string">wurstmeister/zookeeper:3.4.6</span></span><br><span class="line"><span class="attr">    ports:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"2181:2181"</span></span><br><span class="line"><span class="attr">  kafka:</span></span><br><span class="line"><span class="attr">    image:</span> <span class="string">wurstmeister/kafka:2.12-2.2.1</span></span><br><span class="line"><span class="attr">    ports:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"9092:9092"</span></span><br><span class="line"><span class="attr">    depends_on:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">zookeeper</span></span><br><span class="line"><span class="attr">    expose:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">"9093"</span></span><br><span class="line"><span class="attr">    environment:</span></span><br><span class="line"><span class="attr">      KAFKA_ADVERTISED_LISTENERS:</span> <span class="attr">INSIDE://kafka:9093,OUTSIDE://192.168.56.103:9092</span></span><br><span class="line"><span class="attr">      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP:</span> <span class="attr">INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT</span></span><br><span class="line"><span class="attr">      KAFKA_LISTENERS:</span> <span class="attr">INSIDE://0.0.0.0:9093,OUTSIDE://0.0.0.0:9092</span></span><br><span class="line"><span class="attr">      KAFKA_ZOOKEEPER_CONNECT:</span> <span class="attr">zookeeper:2181</span></span><br><span class="line"><span class="attr">      KAFKA_INTER_BROKER_LISTENER_NAME:</span> <span class="string">INSIDE</span></span><br><span class="line"><span class="attr">      KAFKA_CREATE_TOPICS:</span> <span class="string">"flink:1:1"</span></span><br><span class="line"><span class="attr">    volumes:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">/var/run/docker.sock:/var/run/docker.sock</span></span><br><span class="line"><span class="attr">  mysql:</span></span><br><span class="line"><span class="attr">    image:</span> <span class="string">mysql</span></span><br><span class="line"><span class="attr">    command:</span> <span class="bullet">--default-authentication-plugin=mysql_native_password</span></span><br><span class="line"><span class="attr">    restart:</span> <span class="string">always</span></span><br><span class="line"><span class="attr">    ports:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"3306:3306"</span></span><br><span class="line"><span class="attr">    environment:</span></span><br><span class="line"><span class="attr">      MYSQL_ROOT_PASSWORD:</span> <span class="number">123456</span></span><br></pre></td></tr></table></figure><p>环境搭建好之后就是正式的代码部分了：</p><p>核心是两段SQL代码，分别是用来连接Kafka和MYSQL的。</p><p>其中kafka使用json格式来解析。</p><p>样例数据（{“t”:1570,”user_name”:”xiaoming”,”cnt”:100}）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- source</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> user_log (</span><br><span class="line">    user_id <span class="built_in">VARCHAR</span>,</span><br><span class="line">    item_id <span class="built_in">VARCHAR</span>,</span><br><span class="line">    category_id <span class="built_in">VARCHAR</span>,</span><br><span class="line">    behavior <span class="built_in">VARCHAR</span>,</span><br><span class="line">    ts <span class="built_in">TIMESTAMP</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">    <span class="string">'connector.type'</span> = <span class="string">'kafka'</span>,</span><br><span class="line">    <span class="string">'connector.version'</span> = <span class="string">'universal'</span>,</span><br><span class="line">    <span class="string">'connector.topic'</span> = <span class="string">'user_behavior'</span>,</span><br><span class="line">    <span class="string">'connector.startup-mode'</span> = <span class="string">'earliest-offset'</span>,</span><br><span class="line">    <span class="string">'connector.properties.0.key'</span> = <span class="string">'group.id'</span>,</span><br><span class="line">    <span class="string">'connector.properties.0.value'</span> = <span class="string">'test-group'</span>,</span><br><span class="line">    <span class="string">'connector.properties.1.key'</span> = <span class="string">'bootstrap.servers'</span>,</span><br><span class="line">    <span class="string">'connector.properties.1.value'</span> = <span class="string">'localhost:9092'</span>,</span><br><span class="line">    <span class="string">'connector.specific-offsets.0.partition'</span> = <span class="string">'0'</span>,</span><br><span class="line">    <span class="string">'connector.specific-offsets.0.offset'</span> = <span class="string">'0'</span>,</span><br><span class="line">    <span class="string">'update-mode'</span> = <span class="string">'append'</span>,</span><br><span class="line">    <span class="string">'format.type'</span> = <span class="string">'json'</span>,</span><br><span class="line">    <span class="string">'format.derive-schema'</span> = <span class="string">'true'</span></span><br><span class="line">);</span><br><span class="line"><span class="comment">-- sink</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> pvuv_sink (</span><br><span class="line">    dt <span class="built_in">VARCHAR</span>,</span><br><span class="line">    pv <span class="built_in">BIGINT</span>,</span><br><span class="line">    uv <span class="built_in">BIGINT</span></span><br><span class="line">) <span class="keyword">WITH</span> (</span><br><span class="line">    <span class="string">'connector.type'</span> = <span class="string">'jdbc'</span>,</span><br><span class="line">    <span class="string">'connector.url'</span> = <span class="string">'jdbc:mysql://localhost:3306/flink-test'</span>,</span><br><span class="line">    <span class="string">'connector.table'</span> = <span class="string">'pvuv_sink'</span>,</span><br><span class="line">    <span class="string">'connector.username'</span> = <span class="string">'root'</span>,</span><br><span class="line">    <span class="string">'connector.password'</span> = <span class="string">'123456'</span>,</span><br><span class="line">    <span class="string">'connector.write.flush.max-rows'</span> = <span class="string">'1'</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure><p>在 Java代码中，可以直接使用tEnv的sqlUpdate()方法来注册这两张表，之后就可以直接使用了。具体操作如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1. 连接kafka构建源表</span></span><br><span class="line">tEnv.sqlUpdate(kafkaSourceSql);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2. 定义要输出的表</span></span><br><span class="line">tEnv.sqlUpdate(mysqlSinkSql);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3. 自定义具体的 DML 操作,这里我直接将kafka写入到mysql</span></span><br><span class="line"><span class="comment">// 对于Insert Into 操作，同样还是要使用sqlUpdate（）方法</span></span><br><span class="line">tEnv.sqlUpdate(<span class="string">"INSERT INTO sink "</span> +</span><br><span class="line"><span class="string">"SELECT * from log where cnt=100"</span>);</span><br></pre></td></tr></table></figure><p>可以直接通过mysql的客户端看到我们的写入结果！</p><p><img src="/2019/11/10/flink使用15-在Flink-SQL中连接kafka和mysql/mysql.png" alt></p><p>以上就是使用 Flink SQL 的 DDL 语言通过不同的外部数据源建立表的过程。</p>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
            <tag> sql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink使用14-使用SQL操作几种window</title>
      <link href="/2019/11/01/flink%E4%BD%BF%E7%94%A814-%E4%BD%BF%E7%94%A8SQL%E6%93%8D%E4%BD%9C%E5%87%A0%E7%A7%8Dwindow/"/>
      <url>/2019/11/01/flink%E4%BD%BF%E7%94%A814-%E4%BD%BF%E7%94%A8SQL%E6%93%8D%E4%BD%9C%E5%87%A0%E7%A7%8Dwindow/</url>
      
        <content type="html"><![CDATA[<p>Flink SQL 支持三种窗口类型, 分别为 Tumble Windows / HOP Windows 和 Session Windows. 其中 HOP windows 对应 Table API 中的 Sliding Window, 同时每种窗口分别有相应的使用场景和方法.</p><table><thead><tr><th></th><th>Tumble Windows</th><th>HOP Window</th><th>Session Windows</th></tr></thead><tbody><tr><td></td><td>TUMBLE(time_attr, interval)</td><td>HOP(time_attr, interval1,interval2)</td><td>HOP(time_attr, interval)</td></tr></tbody></table><p>下面用几段代码演示如何使用上面 3组 API. 完整的代码见 <a href="https://github.com/CheckChe0803/flink-simple-tutorial/tree/master/table/src/main/java/sql/window" target="_blank" rel="noopener">Github</a></p><p>首先填充一点测试数据</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 初始数据  字段解释 -&gt; (timeStamp , name , value)</span></span><br><span class="line">DataStream&lt;Tuple3&lt;Long, String,Integer&gt;&gt; log = env.fromCollection(Arrays.asList(</span><br><span class="line">    <span class="comment">//时间 14:53:00</span></span><br><span class="line">    <span class="keyword">new</span> Tuple3&lt;&gt;(<span class="number">1572591180_000L</span>,<span class="string">"xiao_ming"</span>,<span class="number">300</span>),</span><br><span class="line">    <span class="comment">//时间 14:53:09</span></span><br><span class="line">    <span class="keyword">new</span> Tuple3&lt;&gt;(<span class="number">1572591189_000L</span>,<span class="string">"zhang_san"</span>,<span class="number">303</span>),</span><br><span class="line">    <span class="comment">//时间 14:53:12</span></span><br><span class="line">    <span class="keyword">new</span> Tuple3&lt;&gt;(<span class="number">1572591192_000L</span>, <span class="string">"xiao_li"</span>,<span class="number">204</span>),</span><br><span class="line">    <span class="comment">//时间 14:53:21</span></span><br><span class="line">    <span class="keyword">new</span> Tuple3&lt;&gt;(<span class="number">1572591201_000L</span>,<span class="string">"li_si"</span>, <span class="number">208</span>)));</span><br></pre></td></tr></table></figure><p>然后是转换为 Table</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//这里需要注意的是 如果采用了EventTime, 那么 对应字段后面加 .rowtime, 否则加 .proctime</span></span><br><span class="line">Table logT = tEnv.fromDataStream(logWithTime, <span class="string">"t.rowtime, name, v"</span>);</span><br></pre></td></tr></table></figure><h4 id="Tumble-Windows"><a href="#Tumble-Windows" class="headerlink" title="Tumble Windows"></a>Tumble Windows</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// GROUP BY TUMBLE(t, INTERVAL '10' SECOND) 相当于根据10s的时间来划分窗口</span></span><br><span class="line"><span class="comment">// TUMBLE_START(t, INTERVAL '10' SECOND) 获取窗口的开始时间</span></span><br><span class="line"><span class="comment">// TUMBLE_END(t, INTERVAL '10' SECOND) 获取窗口的结束时间</span></span><br><span class="line">tEnv.sqlQuery(<span class="string">"SELECT TUMBLE_START(t, INTERVAL '10' SECOND) AS window_start,"</span> +</span><br><span class="line">                <span class="string">"TUMBLE_END(t, INTERVAL '10' SECOND) AS window_end, SUM(v) FROM "</span></span><br><span class="line">                + logT + <span class="string">" GROUP BY TUMBLE(t, INTERVAL '10' SECOND)"</span>);</span><br></pre></td></tr></table></figure><h4 id="HOP-Windows"><a href="#HOP-Windows" class="headerlink" title="HOP Windows"></a>HOP Windows</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// HOP(time_attr, interval1, interval2)</span></span><br><span class="line"><span class="comment">// interval1 滑动长度</span></span><br><span class="line"><span class="comment">// interval2 窗口长度</span></span><br><span class="line"><span class="comment">// HOP_START(t, INTERVAL '5' SECOND, INTERVAL '10' SECOND) 表示窗口开始时间</span></span><br><span class="line"><span class="comment">// HOP_END(t, INTERVAL '5' SECOND, INTERVAL '10' SECOND) 表示窗口结束时间</span></span><br><span class="line">Table result = tEnv.sqlQuery(<span class="string">"SELECT HOP_START(t, INTERVAL '5' SECOND, INTERVAL '10' SECOND) AS window_start,"</span> </span><br><span class="line">                             + <span class="string">"HOP_END(t, INTERVAL '5' SECOND, INTERVAL '10' SECOND) AS window_end, SUM(v) FROM "</span></span><br><span class="line">                             + logT + <span class="string">" GROUP BY HOP(t, INTERVAL '5' SECOND, INTERVAL '10' SECOND)"</span>);</span><br></pre></td></tr></table></figure><h4 id="Session-Windows"><a href="#Session-Windows" class="headerlink" title="Session Windows"></a>Session Windows</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// SESSION(time_attr, interval)</span></span><br><span class="line"><span class="comment">// interval 表示两条数据触发session的最大间隔</span></span><br><span class="line">Table result = tEnv.sqlQuery(<span class="string">"SELECT SESSION_START(t, INTERVAL '5' SECOND) AS window_start,"</span> </span><br><span class="line">                             +<span class="string">"SESSION_END(t, INTERVAL '5' SECOND) AS window_end, SUM(v) FROM "</span></span><br><span class="line">                             + logT + <span class="string">" GROUP BY SESSION(t, INTERVAL '5' SECOND)"</span>);</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
            <tag> sql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink使用13-开始体验 Flink SQL</title>
      <link href="/2019/10/31/flink%E4%BD%BF%E7%94%A813-%E5%BC%80%E5%A7%8B%E4%BD%93%E9%AA%8C-Flink-SQL/"/>
      <url>/2019/10/31/flink%E4%BD%BF%E7%94%A813-%E5%BC%80%E5%A7%8B%E4%BD%93%E9%AA%8C-Flink-SQL/</url>
      
        <content type="html"><![CDATA[<p>SQL API 是 Flink 中最顶级的 API , 它构建了 Table API 之上, 也可以方便的和 Table 做转换, 构建 SQL 所使用的Environment 也是 Table Environment .  Flink SQL 底层使用 Apache Calcite 框架, 将标准的 Flink SQL 语句解析并转换成底层的算子处理逻辑. 下面就直接用 Flink 官方仓库中的 案例 <a href="https://github.com/CheckChe0803/flink-simple-tutorial/blob/master/table/src/main/java/sql/StreamSQLExample.java" target="_blank" rel="noopener">Code Link</a>来做一个演示.</p><ol><li>获取执行环境</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 首先同样有流处理和批处理的区别, </span></span><br><span class="line"><span class="comment">// 获取对应的environment之后直接转换为Table environment ,</span></span><br><span class="line"><span class="comment">// 就可以使用SQL API,</span></span><br><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);</span><br></pre></td></tr></table></figure><ol start="2"><li>拿到要操作的表</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 将 Stream 转换为 Table, 可以采用不同的办法</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 将 DataStream 转换为 Table</span></span><br><span class="line">Table tableA = tEnv.fromDataStream(orderA, <span class="string">"user, product, amount"</span>);</span><br><span class="line"><span class="comment">// 将 DataStream 注册成 Table</span></span><br><span class="line">tEnv.registerDataStream(<span class="string">"OrderB"</span>, orderB, <span class="string">"user, product, amount"</span>);</span><br></pre></td></tr></table></figure><ol start="3"><li>执行SQL语句</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// TableEnvironment 有 SqlQuery 和 SqlUpdate 两种操作符可以使用</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// union 两个 table</span></span><br><span class="line"> Table result = tEnv.sqlQuery(<span class="string">"SELECT * FROM "</span> + tableA + <span class="string">" WHERE amount &gt; 2 UNION ALL "</span> +<span class="string">"SELECT * FROM OrderB WHERE amount &gt; 2"</span>);</span><br></pre></td></tr></table></figure><p>SQL可以执行许多复杂的操作,本文先简单的了解下 SQL 的API</p>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
            <tag> sql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink使用12-用 table API 实现WordCount</title>
      <link href="/2019/10/08/flink%E4%BD%BF%E7%94%A812-%E7%94%A8-table-API-%E5%AE%9E%E7%8E%B0WordCount/"/>
      <url>/2019/10/08/flink%E4%BD%BF%E7%94%A812-%E7%94%A8-table-API-%E5%AE%9E%E7%8E%B0WordCount/</url>
      
        <content type="html"><![CDATA[<p>Table API 是 Flink 构建在 DataSet 和 DataStream API 之上的一套结构化编程接口. 本文希望通过一个简单的 wordCount 的例子首先来体验一下普通的 Flink Table 的代码是由哪些部分构成的.</p><ol><li><p><strong>获取 TableEnvironment</strong></p><p>ExecutionEnvironment 是必不可少的, 不管是Stream API 还是 batch API 都需要一个Environment来管理程序, TableEnvironment 也是在使用 Table API 时首先需要创建的, 它提供了注册内部表/ 执行 Flink SQL 语句/ 注册自定义函数等多种功能. 要获取 TableEnvironment, 首先需要根据情况先创建 DataSet 或者 DataStream 的 Environment, 之后再转换为 TableEnvironment.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 以获取 batch Table API 为例, stream类似</span></span><br><span class="line">ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">BatchTableEnvironment tEnv = BatchTableEnvironment.create(env);</span><br></pre></td></tr></table></figure></li><li><p><strong>拿到 Table</strong></p><p>在以 DataSet API 或者 DataStream API 操作时, 一般有通过集合 / 文件 / socket / 外部数据源 等多种方式来将数据输入. 在 Table API 中, 同样也可以以多种不同的形式去创建一个 Table, 只不过过程相对复杂一些. 拿到一张表的方式有多种, 有通过 Table Descriptor / 用户自定义 Table Source / 由DataStream或者DataSet转换的形式等来实现. 具体的实现操作以后的文章再来详细展示, 下面就以通过 DataSet转换的方式来简单完成.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// WC 为自定义的 POJO 来统计单词的信息. 首先构建DataSet</span></span><br><span class="line">DataSet&lt;WC&gt; input = env.fromElements(</span><br><span class="line">                <span class="keyword">new</span> WC(<span class="string">"Hello"</span>, <span class="number">1</span>),</span><br><span class="line">                <span class="keyword">new</span> WC(<span class="string">"flink"</span>, <span class="number">1</span>),</span><br><span class="line">                <span class="keyword">new</span> WC(<span class="string">"Hello"</span>, <span class="number">1</span>));</span><br><span class="line"><span class="comment">// 直接将 DataSet 转换为 Table</span></span><br><span class="line">Table table = tEnv.fromDataSet(input);</span><br></pre></td></tr></table></figure></li><li><p><strong>数据转换操作</strong></p><p>拿到了 Table 之后, 就可以正常的进行数据转换的操作了, 如底层的 API一样, 常见的操作 Table API 都已经实现好了, 包括 <strong>数据查询和过滤</strong> / <strong>窗口操作</strong> / <strong>聚合操作</strong> / <strong>多表关联</strong> / <strong>集合操作</strong> / <strong>排序操作</strong> / <strong>数据写入</strong>. 但是具体的 API语法还是与底层API有很大的不同的, 目的就是为了更方便更类似SQL的形式方便用户去写业务逻辑, 下面就展示一下如何实现 wordCount这个一个步骤.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Table filtered = table</span><br><span class="line">                .groupBy(<span class="string">"word"</span>)</span><br><span class="line">    <span class="comment">// 在 select 方法中可以方便的直接使用类sql语句进行操作</span></span><br><span class="line">                .select(<span class="string">"word, frequency.sum as frequency"</span>)</span><br><span class="line">                .filter(<span class="string">"frequency = 2"</span>);</span><br></pre></td></tr></table></figure></li><li><p><strong>数据输出</strong></p><p>数据处理完毕之后最后一步就是结果的输出, 这一步与底层的API也是大同小异的, 可以将结果直接insetInto()到其他在TableEnvironment注册的表中, 也可以将处理完成的结果转换为 DataSet 或者是 DataStream 亦或是通过自定义的 sink 输出. </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1. 直接输入到其他表中</span></span><br><span class="line">table.insertInto(<span class="string">"otherTbale"</span>);</span><br><span class="line"><span class="comment">// 2. 转换为 DataSet / DataStream</span></span><br><span class="line">DataSet&lt;WC&gt; result = tEnv.toDataSet(filtered, WC.class);</span><br><span class="line"><span class="comment">// 3. 自定义 sink</span></span><br><span class="line">CsvTableSink tableSink = <span class="keyword">new</span> CsvTableSink(path, <span class="string">","</span>);</span><br><span class="line">tEnv.registerTableSink(<span class="string">"csvSink"</span>, tableSink);</span><br></pre></td></tr></table></figure></li></ol><p>通过上面几步, 一个完整的 Flink Table API 的程序就构建完毕了, 有了前面学习 flink 的经验, 这部分其实是水到渠成的, 稍微了解下就可以上手了, 本文对应的 wordCount 案例在 <a href="https://github.com/CheckChe0803/flink-simple-tutorial/blob/master/table/src/main/java/wordCount/WordCountTable.java" target="_blank" rel="noopener">github</a> 中, 在后面的部分会继续讲解一下其他 Table API 的内容.</p>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
            <tag> table </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink使用11-了解broadcast的用法</title>
      <link href="/2019/10/02/flink%E4%BD%BF%E7%94%A811-%E4%BA%86%E8%A7%A3broadcast%E7%9A%84%E7%94%A8%E6%B3%95/"/>
      <url>/2019/10/02/flink%E4%BD%BF%E7%94%A811-%E4%BA%86%E8%A7%A3broadcast%E7%9A%84%E7%94%A8%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>在Flink中，同一个算子可能存在若干个不同的并行实例，计算过程可能不在同一个Slot中进行，不同算子之间更是如此，因此不同算子的计算数据之间不能像Java数组之间一样互相访问，而广播变量<code>Broadcast</code>便是解决这种情况的. 在 flink 中, 针对某一个算子需要使用公共变量的情况下, 就可以把对应的数据给广播出去, 这样在所有的节点中都可以使用了. 典型的代码结构如下所示:</p><p>在一个算子中使用广播变量主要有两个步骤:</p><ol><li><p><strong>广播变量</strong> (一般写在算子的后面即可) </p><p>使用 withBroadcastSet(data, “name”) 这个方法即可, name变量代表了获取该广播变量的名称</p></li><li><p><strong>使用广播变量</strong></p><p>使用方法主要是通过 RichFunction, 在 对应的 open( )方法中, 可以根据名称来获取对应的广播变量, 只需要一次获取, 就可以一直使用了, 具体方法如下:</p></li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">dataSet.map(<span class="keyword">new</span> RichMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">            List&lt;Integer&gt; bc;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">super</span>.open(parameters);</span><br><span class="line">                <span class="comment">// 2. 获取广播变量</span></span><br><span class="line">                <span class="keyword">this</span>.bc = getRuntimeContext().getBroadcastVariable(<span class="string">"broadcastData"</span>);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> s;</span><br><span class="line">            &#125;</span><br><span class="line">    <span class="comment">// 1. 将需要用的变量广播出去 (这一步可以写在后面)</span></span><br><span class="line">        &#125;).withBroadcastSet(broadcastData, <span class="string">"broadcastData"</span>).print();</span><br></pre></td></tr></table></figure><p>下面以一个获取用户年龄的例子来演示一个常见的使用案例:</p><p>broadcastData 是一个包含用户 (姓名, 年龄) 的数据表</p><p>需要在另外一个算子中通过姓名查找年龄, 那么就需要把上表广播</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BroadcastExample</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">4</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建需要广播的 数据集 (name, age)</span></span><br><span class="line">        Tuple2&lt;String, Integer&gt; john = <span class="keyword">new</span> Tuple2&lt;&gt;(<span class="string">"john"</span>, <span class="number">23</span>);</span><br><span class="line">        Tuple2&lt;String, Integer&gt; tom = <span class="keyword">new</span> Tuple2&lt;&gt;(<span class="string">"tom"</span>, <span class="number">24</span>);</span><br><span class="line">        Tuple2&lt;String, Integer&gt; shiny = <span class="keyword">new</span> Tuple2&lt;&gt;(<span class="string">"shiny"</span>, <span class="number">22</span>);</span><br><span class="line">        DataSource&lt;Tuple2&lt;String, Integer&gt;&gt; broadcastData = env.fromElements(john, tom, shiny);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 新建一个dataset -&gt; d1, 设置并行度为4</span></span><br><span class="line">        <span class="comment">// 此时 d1 是无法访问 broadcastData 的数据的, 因为两个dataset可能不在一个节点或者slot中, 所以 flink 是不允许去访问的</span></span><br><span class="line">        DataSet&lt;String&gt; d1 = env.fromElements(<span class="string">"john"</span>, <span class="string">"tom"</span>, <span class="string">"shiny"</span>).setParallelism(<span class="number">4</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 使用 RichMapFunction, 在open() 方法中拿到广播变量</span></span><br><span class="line">        d1.map(<span class="keyword">new</span> RichMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">            List&lt;Tuple2&lt;String, Integer&gt;&gt; bc;</span><br><span class="line">            HashMap&lt;String, Integer&gt; map = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">super</span>.open(parameters);</span><br><span class="line">                <span class="keyword">this</span>.bc = getRuntimeContext().getBroadcastVariable(<span class="string">"broadcastData"</span>);</span><br><span class="line">                <span class="keyword">for</span> (Tuple2&lt;String, Integer&gt; tp : bc) &#123;</span><br><span class="line">                    <span class="keyword">this</span>.map.put(tp.f0, tp.f1);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                Integer age = <span class="keyword">this</span>.map.get(s);</span><br><span class="line">                <span class="keyword">return</span> s + <span class="string">"-&gt;"</span> + age;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).withBroadcastSet(broadcastData, <span class="string">"broadcastData"</span>).print();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
            <tag> broadcast </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink使用10-通过Bulk iterator计算圆周率</title>
      <link href="/2019/10/02/flink%E4%BD%BF%E7%94%A810-%E9%80%9A%E8%BF%87Bulk-iterator%E8%AE%A1%E7%AE%97%E5%9C%86%E5%91%A8%E7%8E%87/"/>
      <url>/2019/10/02/flink%E4%BD%BF%E7%94%A810-%E9%80%9A%E8%BF%87Bulk-iterator%E8%AE%A1%E7%AE%97%E5%9C%86%E5%91%A8%E7%8E%87/</url>
      
        <content type="html"><![CDATA[<p> 迭代处理是批量处理处理中的常见操作, Flink 的 迭代计算支持两种模式, 分别是 Bulk Iteration (全量迭代计算) 和 Delt Iteration (增量迭代计算). 下面就一个计算圆周率的例子 来说一下使用 Bulk Iteration 都有哪几个步骤.</p><p>在 Bulk Iteration 中, 主要的步骤其实是分为3步, 第一步是指定最大循环次数, 第二步是指定在循环时的一个计算处理的过程, 最后一步就是调用计算过程, 指定结束条件. 具体代码如下所示</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BulkIteration</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">// 获取执行环境</span></span><br><span class="line">        ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(<span class="number">1</span>);</span><br><span class="line"><span class="comment">// 构建输出数据</span></span><br><span class="line">        DataSource&lt;Integer&gt; data = env.fromElements(<span class="number">0</span>);</span><br><span class="line">        <span class="comment">// 1. 指定循环次数</span></span><br><span class="line">        IterativeDataSet&lt;Integer&gt; loop = data.iterate(<span class="number">1000</span>);</span><br><span class="line">        <span class="comment">// 2. 指循环计算过程</span></span><br><span class="line">        MapOperator&lt;Integer, Integer&gt; process = loop.map(<span class="keyword">new</span> MapFunction&lt;Integer, Integer&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Integer <span class="title">map</span><span class="params">(Integer i)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">double</span> x = Math.random();</span><br><span class="line">                <span class="keyword">double</span> y = Math.random();</span><br><span class="line">                <span class="keyword">int</span> result = (x * x + y * y) &lt; <span class="number">1</span> ? <span class="number">1</span> : <span class="number">0</span>;</span><br><span class="line">                <span class="keyword">return</span> i + result;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="comment">// 3. 使用 closeWith 调用计算过程</span></span><br><span class="line">        List&lt;Integer&gt; collect = loop.closeWith(process).collect();</span><br><span class="line">        <span class="comment">// 输出最终结果</span></span><br><span class="line">        <span class="keyword">for</span> (Integer i : collect) &#123;</span><br><span class="line">            System.out.println( i / <span class="number">1000.0</span> * <span class="number">4</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
            <tag> iteration </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink使用09-DataSet初体验之通过Inputformat构建dataSet</title>
      <link href="/2019/09/29/flink%E4%BD%BF%E7%94%A809-DataSet%E5%88%9D%E4%BD%93%E9%AA%8C%E4%B9%8B%E9%80%9A%E8%BF%87Inputformat%E6%9E%84%E5%BB%BAdataSet/"/>
      <url>/2019/09/29/flink%E4%BD%BF%E7%94%A809-DataSet%E5%88%9D%E4%BD%93%E9%AA%8C%E4%B9%8B%E9%80%9A%E8%BF%87Inputformat%E6%9E%84%E5%BB%BAdataSet/</url>
      
        <content type="html"><![CDATA[<p>Flink 提供了一套 DataSet 的 API 来做批处理. 其实 DataSet 的使用方法还是和 DataStream 很相似的, 本章主要是先简单的说一下 DataSet 的基本使用. </p><p>DataSet API 其实和 DataStream ApI 相似, 都是需要创建 ExecutionEnvironment 环境, 然后通过 ExecutionEnvironment 环境提供的方法读取外部数据, 将外部数据转换为 DataSet 数据集, 之后利用 DataSet 提供的 API 进行转换操作, 并处理成最后的结果, 并对结果进行输出.</p><h3 id="DataSources-数据输入"><a href="#DataSources-数据输入" class="headerlink" title="DataSources 数据输入"></a>DataSources 数据输入</h3><p>数据输入共有3种类型的接口, 分别是文件系统类型 / Java Collection 类型 / 以及通用数据类型. 其中前两种其实与 DataStream类型, 在前面的系列文章中已经说过了, 这边主要再说一下 通用数据类型接口怎样使用.</p><p>DataSet ApI 提供了 Inputformat 通用的数据接口, 已接入不同的数据源和格式类型的数据. Inputformat 接口主要分为两种类型: 一种是基于文件类型, 在 DataSet API 对应的 readFile( ) 方法; 另外一种是基于通用数据类型的接口, 例如读取 RDBMS 或者 NoSQL 数据库等.</p><p>下面一个方法就以读取一个csv文件的方式举例, 其中首先定义好了每一行的转换类型, 之后将每一行数据输入都转换为对应的 pojo. 使用 env.createInput() 将 PojoCsvInputFormat 转换为 dataSet.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> &lt;T&gt; <span class="function">DataSet&lt;T&gt; <span class="title">getSource</span><span class="params">(ExecutionEnvironment env, String path, String[] fieldOrder, Class&lt;T&gt; type)</span> <span class="keyword">throws</span> URISyntaxException </span>&#123;</span><br><span class="line">        <span class="comment">// 本地文件路径</span></span><br><span class="line">        URL fileUrl = InputFormatExample.class.getClassLoader().getResource(path);</span><br><span class="line">        Path filePath = Path.fromLocalFile(<span class="keyword">new</span> File(fileUrl.getPath()));</span><br><span class="line">        <span class="comment">// 抽取  TypeInformation，是一个 PojoTypeInfo</span></span><br><span class="line">        PojoTypeInfo&lt;T&gt; pojoType = (PojoTypeInfo&lt;T&gt;) TypeExtractor.createTypeInfo(type);</span><br><span class="line">        <span class="comment">// 由于 Java 反射抽取出的字段顺序是不确定的，需要显式指定下文件中字段的顺序</span></span><br><span class="line">        <span class="comment">// 创建 PojoCsvInputFormat</span></span><br><span class="line">        PojoCsvInputFormat&lt;T&gt; csvInput = <span class="keyword">new</span> PojoCsvInputFormat&lt;&gt;(filePath, pojoType, fieldOrder);</span><br><span class="line">        <span class="keyword">return</span> env.createInput(csvInput, pojoType);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h3 id="DataSet-转换操作"><a href="#DataSet-转换操作" class="headerlink" title="DataSet 转换操作"></a>DataSet 转换操作</h3><p>Flink 提供了丰富的 API 对 dataSet 做转换处理, 例如数据处理(Map / FlatMap / MapPartiton / Filter), 聚合操作(Reduce / ReduceGroup / Aggregate), 多表关联(Join / OuterJoin / Cogroup / Cross), 集合操作 (Union / Rebalance / Hash-Partition / Range-Partition / Sort Partition), 排序操作(first / minBy / maxBy). 具体的API操作太多, 本文就不一一赘述了,这里就将一些 join 方法的使用.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 与流处理的合并类型, 也是依靠 where().equalTO()来实现两个dataSet的判断</span></span><br><span class="line">dataSet1.join(dataSet2).where(<span class="string">"key"</span>).equalTo(<span class="string">"key"</span>).with(&lt;JoinFunction&gt;)</span><br></pre></td></tr></table></figure><p> 下面一个例子展示了如何去使用 join 方法关联两个 dataSet.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 首先使用上面讲到的方法读取csv文件数据,转为DataSet&lt;POJO&gt;</span></span><br><span class="line"><span class="comment">// item dataSet 格式为(id, price)</span></span><br><span class="line">        String itemPath = <span class="string">"item.csv"</span>;</span><br><span class="line">        String[] itemField = <span class="keyword">new</span> String[]&#123;<span class="string">"id"</span>, <span class="string">"price"</span>&#125;; <span class="comment">// java反射会导致乱序,手动指定字段序</span></span><br><span class="line">        DataSet&lt;Item&gt; items = getSource(env, itemPath, itemField, Item.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// info dataSet 格式为(id, color, country)</span></span><br><span class="line">        String infoPath = <span class="string">"info.csv"</span>;</span><br><span class="line">        String[] infoField = <span class="keyword">new</span> String[]&#123;<span class="string">"id"</span>, <span class="string">"color"</span>, <span class="string">"country"</span>&#125;;</span><br><span class="line">        DataSet&lt;Info&gt; infos = getSource(env, infoPath, infoField, Info.class);</span><br><span class="line">        <span class="comment">// 关联两个dataset</span></span><br><span class="line">        JoinOperator.DefaultJoin&lt;Item, Info&gt; dataSet = items.join(infos).where(<span class="string">"id"</span>).equalTo(<span class="string">"id"</span>);</span><br><span class="line">        <span class="comment">// 使用 joinFunction 处理合并后的两个dataSet</span></span><br><span class="line">        dataSet.with(<span class="keyword">new</span> JoinFunction&lt;Item, Info, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> String <span class="title">join</span><span class="params">(Item item, Info info)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">"商品ID:"</span> + item.getId() + <span class="string">" 价格:"</span>+item.getPrice() + <span class="string">" 颜色:"</span>+ info.getColor() + <span class="string">" 国家:"</span> + info.getCountry();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).print();</span><br><span class="line"><span class="comment">// 样例数据结果:↓↓↓↓↓</span></span><br><span class="line"><span class="comment">//商品ID:1 价格:50 颜色:red 国家:china</span></span><br><span class="line"><span class="comment">//商品ID:2 价格:120 颜色:black 国家:usa</span></span><br><span class="line"><span class="comment">//商品ID:3 价格:89 颜色:green 国家:korea</span></span><br></pre></td></tr></table></figure><h3 id="DataSinks-数据输出"><a href="#DataSinks-数据输出" class="headerlink" title="DataSinks 数据输出"></a>DataSinks 数据输出</h3><p>为了能够让用户更灵活的使用外部数据, Flink抽象出通用的 OutputFormat 接口, 批量数据输出全部实现于此接口.</p><p>Flink 内置了常用的数据存储介质对应的接口, 如 TextOutputFormat /CsvOutputFormat / HadoopOutputFormat / JDBCOutputFormat 等. </p><p>Flink 在 DataSet ApI 中的数据输出总共以下3类:</p><h4 id="1-基于文件输出接口"><a href="#1-基于文件输出接口" class="headerlink" title="1. 基于文件输出接口"></a>1. 基于文件输出接口</h4><p>WriteAsText / WriteAsCsv. 可以直接使用这两个方法输出到文件, 用户也可以指定写入文件的模式, 分为 OVERWRITE 模式(覆盖) 和 NOT_OVERWRITE 模式(不覆盖 ). 均可以写到hdfs或者本地</p><h4 id="2-通用数据接口"><a href="#2-通用数据接口" class="headerlink" title="2. 通用数据接口"></a>2. 通用数据接口</h4><p>用户可以自己自定义OutputFormat 方法来定义存储, 例如 HadoopOutputFormat.</p><h4 id="3-客户端输出"><a href="#3-客户端输出" class="headerlink" title="3. 客户端输出"></a>3. 客户端输出</h4><p>如果想在本地调试的话, 那么最简单的方式就是通过 print()的方法直接将flink的数据拉回到client, 然后输出.</p>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flnik </tag>
            
            <tag> dataset </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink使用08-在dataStream中使用AsyncFunction</title>
      <link href="/2019/09/27/flink%E4%BD%BF%E7%94%A808-%E5%9C%A8dataStream%E4%B8%AD%E4%BD%BF%E7%94%A8AsyncFunction/"/>
      <url>/2019/09/27/flink%E4%BD%BF%E7%94%A808-%E5%9C%A8dataStream%E4%B8%AD%E4%BD%BF%E7%94%A8AsyncFunction/</url>
      
        <content type="html"><![CDATA[<p>在流式处理的过程中, 在中间步骤的处理中, 如果涉及到一些费事的操作或者是外部系统的数据交互, 那么就会给整个流造成一定的延迟. 在 flink 的 1.2 版本中引入了 Asynchronous I/O, 能够支持异步的操作, 以提高 flink 系统与外部数据系统交互的性能及吞吐量.</p><p>在使用 Flink 的异步 IO 时, 主要有两个 API可以使用, 一个是AsyncDataStream.unorderedWait( ), 另一个AsyncDataStream.orderedWait( ).在异步处理过程中,原本数据的顺序可能会发生变化, 使用unorderWait的方法, 不会考虑顺序的问题, 一旦处理完成就会直接返回结果, 这种方法具有较低的延迟和负载. 那么orderWait的方法就是想对应的, 严格按照原本流中的数据顺序做返回, 会对系统造成一定的延迟. 实际中应该根据具体的业务情况做选择.unorderedWait或orderedWait有两个关于async operation的参数，一个是timeout参数用于设置async的超时时间，一个是capacity参数用于指定同一时刻最大允许多少个(<code>并发</code>)async request在执行；</p><p>在使用异步IO时,需要自己去继承AsyncFunction,AsyncFunction接口继承了Function，它定义了asyncInvoke方法以及一个default的timeout方法；asyncInvoke方法执行异步逻辑，然后通过ResultFuture.complete将结果或异常设置到ResultFuture，如果异常则通过ResultFuture.completeExceptionally(Throwable)来传递 ResultFuture；RichAsyncFunction继承了AbstractRichFunction，同时声明实现AsyncFunction接口，它不没有实现asyncInvoke，交由子类实现；它覆盖了setRuntimeContext方法，这里使用RichAsyncFunctionRuntimeContext或者RichAsyncFunctionIterationRuntimeContext进行包装.</p><p>下面是一个验证 Async I/O 的demo, 具体代码见仓库 -&gt; <a href="https://github.com/CheckChe0803/flink-simple-tutorial/tree/master/streaming/src/main/java/async" target="_blank" rel="noopener">code link</a></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AsyncIOExample</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        DataStream&lt;String&gt; inp = env.fromElements(AsyncIOData.WORDS);</span><br><span class="line"><span class="comment">// 接收数据</span></span><br><span class="line">        SingleOutputStreamOperator&lt;String&gt; out = inp.map(<span class="keyword">new</span> MapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                System.out.println(<span class="string">"读取数据:"</span> + s + <span class="string">"  当前时间:"</span> + System.currentTimeMillis());</span><br><span class="line">                <span class="keyword">return</span> s;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="comment">// 使用 AsyncFunction 对函数做一个简单的处理, 中间随机睡眠 1-10s</span></span><br><span class="line">        DataStream&lt;String&gt; asyncStream = AsyncDataStream.unorderedWait(out, <span class="keyword">new</span> SimpleAsyncFunction(), <span class="number">20_000L</span>, TimeUnit.MILLISECONDS);</span><br><span class="line"><span class="comment">// 对已经被 AsyncFunction 处理过的数据再输出一次</span></span><br><span class="line">        asyncStream.map(<span class="keyword">new</span> MapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                System.out.println(<span class="string">"数据处理完毕:"</span> + s + <span class="string">"  当前时间:"</span> + System.currentTimeMillis());</span><br><span class="line">                <span class="keyword">return</span> s;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"AsyncFunction Demo"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">SimpleAsyncFunction</span> <span class="keyword">extends</span> <span class="title">RichAsyncFunction</span>&lt;<span class="title">String</span>, <span class="title">String</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">long</span> waitTime;</span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">final</span> Random rnd = <span class="keyword">new</span> Random(hashCode());</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">asyncInvoke</span><span class="params">(String input, ResultFuture&lt;String&gt; resultFuture)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            <span class="comment">// 随机睡眠 1 - 10s</span></span><br><span class="line">            System.out.println(<span class="string">"开始 AsyncFunction  target -&gt; "</span> + input);</span><br><span class="line">            waitTime = rnd.nextInt(<span class="number">10</span>);</span><br><span class="line">            Thread.sleep(waitTime * <span class="number">1000</span>);</span><br><span class="line">            String out = input + input;</span><br><span class="line">            resultFuture.complete(Collections.singletonList(out));</span><br><span class="line">            System.out.println(<span class="string">"结束 AsyncFunction  target -&gt; "</span> + input + <span class="string">"  Sleep time = "</span> + waitTime + <span class="string">"s"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>以上代码的输出结果为:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">读取数据:D  当前时间:<span class="number">1569574233046</span></span><br><span class="line">读取数据:C  当前时间:<span class="number">1569574233047</span></span><br><span class="line">读取数据:A  当前时间:<span class="number">1569574233048</span></span><br><span class="line">读取数据:B  当前时间:<span class="number">1569574233049</span></span><br><span class="line">开始 AsyncFunction  target -&gt; D</span><br><span class="line">开始 AsyncFunction  target -&gt; C</span><br><span class="line">开始 AsyncFunction  target -&gt; A</span><br><span class="line">开始 AsyncFunction  target -&gt; B</span><br><span class="line">结束 AsyncFunction  target -&gt; DSleep time = <span class="number">6</span>s</span><br><span class="line">数据处理完毕:DD  当前时间:<span class="number">1569574239065</span></span><br><span class="line">结束 AsyncFunction  target -&gt; CSleep time = <span class="number">6</span>s</span><br><span class="line">数据处理完毕:CC  当前时间:<span class="number">1569574239069</span></span><br><span class="line">结束 AsyncFunction  target -&gt; ASleep time = <span class="number">6</span>s</span><br><span class="line">数据处理完毕:AA  当前时间:<span class="number">1569574239072</span></span><br><span class="line">结束 AsyncFunction  target -&gt; BSleep time = <span class="number">6</span>s</span><br><span class="line">数据处理完毕:BB  当前时间:<span class="number">1569574239076</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
            <tag> async </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink使用07-通过join合并流的操作</title>
      <link href="/2019/09/27/flink%E4%BD%BF%E7%94%A807-%E9%80%9A%E8%BF%87join%E5%90%88%E5%B9%B6%E6%B5%81%E7%9A%84%E6%93%8D%E4%BD%9C/"/>
      <url>/2019/09/27/flink%E4%BD%BF%E7%94%A807-%E9%80%9A%E8%BF%87join%E5%90%88%E5%B9%B6%E6%B5%81%E7%9A%84%E6%93%8D%E4%BD%9C/</url>
      
        <content type="html"><![CDATA[<p>Flink 中支持窗口上的多流合并, 需要保证的是输入的 stream 要构建在相同的 Window 上, 并使用相同类型的 Key 作为关联条件.代码如下所示, 先通过 join 方法将 inputStream1 数据集和 inputStream2 关联, 调用 where( ) 方法指定  inputStream1 的 key, 调用 equalTo( ) 方法指定 inputStream2 对应关联的 key. 通过 window( ) 方法指定 window Assigner, 最后再通过 apply( ) 方法传入用户自定义的 JoinFunction 或者 FlatJoinFunction 对输入的数据元素做窗口计算.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">inputStream1.join(inputStream2)</span><br><span class="line"><span class="comment">// 指定inputStream1的关联key</span></span><br><span class="line">.where(<span class="number">0</span>)</span><br><span class="line"><span class="comment">// 指定inputStream2的关联key</span></span><br><span class="line">.equalTo(<span class="number">1</span>)</span><br><span class="line"><span class="comment">// 指定 window Assigner</span></span><br><span class="line">.window(TumblingEventTimeWindows.of(Time.seconds(<span class="number">10</span>)))</span><br><span class="line"><span class="comment">// 指定窗口计算函数</span></span><br><span class="line">.apply(&lt;JoinFunction&gt;)</span><br></pre></td></tr></table></figure><p>下面就用 flink 官方仓库中的join example来做演示, 完整代码见仓库 -&gt; <a href="https://github.com/CheckChe0803/flink-simple-tutorial/tree/master/streaming/src/main/java/join" target="_blank" rel="noopener">code link</a></p><p><strong>样例中有两个流, 分别记录的是员工的等级和员工的薪水, 流中数据的格式分别是 (name, grade) / (name, salary), 代码实现的功能是合并两个流, 转变为 (name, grade, salary) 格式的流.</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> <span class="keyword">long</span> windowSize = <span class="number">200L</span>;</span><br><span class="line"><span class="keyword">final</span> <span class="keyword">long</span> rate = <span class="number">3L</span>;</span><br><span class="line"></span><br><span class="line">System.out.println(<span class="string">"Using windowSize="</span> + windowSize + <span class="string">", data rate="</span> + rate);</span><br><span class="line">System.out.println(<span class="string">"To customize example, use: WindowJoin [--windowSize &lt;window-size-in-millis&gt;] [--rate &lt;elements-per-second&gt;]"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取env, 配置为"ingestion time"</span></span><br><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 生成 grade 和 salary 两个流 分别是 (name, grade) / (name, salary)</span></span><br><span class="line">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; grades = WindowJoinSampleData.GradeSource.getSource(env, rate);</span><br><span class="line">DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; salaries = WindowJoinSampleData.SalarySource.getSource(env, rate);</span><br><span class="line"></span><br><span class="line">DataStream&lt;Tuple3&lt;String, Integer, Integer&gt;&gt; joinedStream = runWindowJoin(grades, salaries, windowSize);</span><br><span class="line"></span><br><span class="line">joinedStream.print().setParallelism(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">env.execute(<span class="string">"Windowed Join Example"</span>);</span><br></pre></td></tr></table></figure><p>其中, 数据流的添加是通过一个Iterator 不停的添加进去的, 具体的 join 逻辑通过 runWindowJoin( )方法, 以为为该方法的具体内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">public static DataStream&lt;Tuple3&lt;String, Integer, Integer&gt;&gt; runWindowJoin(</span><br><span class="line">            DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; grades,</span><br><span class="line">            DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; salaries,</span><br><span class="line">            long windowSize) &#123;</span><br><span class="line"></span><br><span class="line">        return grades.join(salaries)</span><br><span class="line">                .where(new NameKeySelector())</span><br><span class="line">                .equalTo(new NameKeySelector())</span><br><span class="line"></span><br><span class="line">                .window(TumblingEventTimeWindows.of(Time.milliseconds(windowSize)))</span><br><span class="line"></span><br><span class="line">                .apply(new JoinFunction&lt;Tuple2&lt;String, Integer&gt;, Tuple2&lt;String, Integer&gt;, Tuple3&lt;String, Integer, Integer&gt;&gt;() &#123;</span><br><span class="line"></span><br><span class="line">                    @Override</span><br><span class="line">                    public Tuple3&lt;String, Integer, Integer&gt; join(</span><br><span class="line">                            Tuple2&lt;String, Integer&gt; first,</span><br><span class="line">                            Tuple2&lt;String, Integer&gt; second) &#123;</span><br><span class="line">                        return new Tuple3&lt;String, Integer, Integer&gt;(first.f0, first.f1, second.f1);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
            <tag> join </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink使用06-如何处理窗口内的数据</title>
      <link href="/2019/09/26/flink%E4%BD%BF%E7%94%A806-%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E7%AA%97%E5%8F%A3%E5%86%85%E7%9A%84%E6%95%B0%E6%8D%AE/"/>
      <url>/2019/09/26/flink%E4%BD%BF%E7%94%A806-%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E7%AA%97%E5%8F%A3%E5%86%85%E7%9A%84%E6%95%B0%E6%8D%AE/</url>
      
        <content type="html"><![CDATA[<p>上一节主要是大致介绍了下 flink 的窗口组成, 以及如何去划分窗口的. 那么这一篇文章主要是对剩下的内容做一下总结, 说一下如何对窗口内的数据做处理.  </p><h4 id="Window-Function"><a href="#Window-Function" class="headerlink" title="Window Function"></a>Window Function</h4><p>Window Assigner 的作用是划分窗口的, 而 Window Function 就是对窗口内的数据做处理的一个过程. Flink 提供了 4 种类型的 Window Function, 分别是 ReduceFunction / AggregateFunction / FoldFunction / ProcessWindowFunction. 另外, 这四类还根据计算原理的不同分为增量聚合函数和全量窗口函数. 增量的计算性能比较高, 主要是基于中间状态的计算结果, 窗口中只维护中间结果的状态值.</p><h4 id="1-ReduceFunction-增量"><a href="#1-ReduceFunction-增量" class="headerlink" title="1. ReduceFunction (增量)"></a><strong>1. ReduceFunction</strong> (增量)</h4><p>对输入的两个相同类型的元素按照指定的计算方式进行聚合, 通过实现 ReduceFunction 接口就可以在reduce( ) 函数内部进行聚合操作了.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 将Tuple2 按照 f1 进行 keyBy, 之后将 f0字符合并起来</span></span><br><span class="line">input.keyBy(x -&gt; x.f1)</span><br><span class="line">.timeWindow(Time.seconds(<span class="number">10</span>), Time.seconds(<span class="number">1</span>))</span><br><span class="line">    .reduce(<span class="keyword">new</span> ReduceFunction&lt;Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Long&gt; <span class="title">reduce</span><span class="params">(Tuple2&lt;String, Long&gt; t1, Tuple2&lt;String, Long&gt; t2)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(t1.f0 + t2.f0, t1.f1);</span><br><span class="line">        &#125;</span><br><span class="line">        &#125;);</span><br></pre></td></tr></table></figure><p>当然也可以使用匿名函数的方式,写起来会更加简洁.上述代码可以改为:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">input.keyBy(x -&gt; x.f1).timeWindow(Time.seconds(<span class="number">10</span>), Time.seconds(<span class="number">1</span>))</span><br><span class="line">.reduce((t1,t2) -&gt; <span class="keyword">new</span> Tuple2&lt;&gt;(t1.f0 + t2.f0, t1.f1));</span><br></pre></td></tr></table></figure><h4 id="2-AggregateFunction-增量"><a href="#2-AggregateFunction-增量" class="headerlink" title="2. AggregateFunction (增量)"></a><strong>2. AggregateFunction</strong> (增量)</h4><p>AggregateFunction 相对于ReduceFunction更加灵活,但是实现起来也更复杂, AggregateFunction有 4 个需要复写的方法, 其中createAccumulator( ) 定义累加器, add( ) 定义数据的添加逻辑, getResult( ) 定义了根据 accumulator 计算结果的逻辑, merge()方法定义合并 accumulator 的逻辑.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">input.keyBy(x -&gt; x.f1)</span><br><span class="line">    .timeWindow(Time.seconds(<span class="number">10</span>), Time.seconds(<span class="number">1</span>))</span><br><span class="line">    <span class="comment">// 自定义一个AggregateFunciton, 将相同标号 f1 的数据的 f0字符串字段合并在一起</span></span><br><span class="line">    <span class="comment">// ("hello", 1L) + ("world", 1L) = ("hello world", 1L)</span></span><br><span class="line">    .aggregate(<span class="keyword">new</span> MyAggregateFunction());</span><br></pre></td></tr></table></figure><p>通过自定义的 MyAggregateFunction() 来实现 AggregateFunction 接口</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyAggregateFunction</span> <span class="keyword">implements</span> <span class="title">AggregateFunction</span>&lt;<span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Long</span>&gt;, <span class="title">String</span>, <span class="title">String</span>&gt;</span>&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">createAccumulator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 初始化累加器</span></span><br><span class="line">            <span class="keyword">return</span> <span class="string">""</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">add</span><span class="params">(Tuple2&lt;String, Long&gt; t, String s)</span> </span>&#123;</span><br><span class="line">            <span class="comment">// 输入数据与累加器的合并</span></span><br><span class="line">            <span class="keyword">return</span> s + <span class="string">" "</span> +t.f0;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">getResult</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">            <span class="comment">// 得到累加器的结果</span></span><br><span class="line">            <span class="keyword">return</span> s.trim();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> String <span class="title">merge</span><span class="params">(String s, String acc1)</span> </span>&#123;</span><br><span class="line">            <span class="comment">// 合并累加器</span></span><br><span class="line">            <span class="keyword">return</span> s + <span class="string">" "</span> + acc1;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h4 id="3-FoldFunction-增量"><a href="#3-FoldFunction-增量" class="headerlink" title="3. FoldFunction (增量)"></a><strong>3. FoldFunction</strong> (增量)</h4><p>FoldFunction定义了如何将窗口中的输入元素与外部的元素合并的逻辑</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">input.keyBy(x -&gt; x.f1)</span><br><span class="line">.timeWindow(Time.seconds(<span class="number">10</span>), Time.seconds(<span class="number">1</span>)).fold(<span class="string">"flink"</span>, (acc, t) -&gt;t.f0 + acc);</span><br></pre></td></tr></table></figure><p>FoldFunction在新版本已经被标记@Deprecated了, 建议使用AggregateFunction代替</p><h4 id="4-ProcessWindowFunction-全量"><a href="#4-ProcessWindowFunction-全量" class="headerlink" title="4. ProcessWindowFunction (全量)"></a><strong>4. ProcessWindowFunction</strong> (全量)</h4><p>ProcessWindowFunction 相较于其他的 Window Function, 可以实现一些更复杂的计算, 比如基于整个窗口做某些指标计算 或者需要操作窗口中的状态数据和窗口元数据. Flink 提供了 ProcessWindowFunction 这个抽象类, 继承此类就可以实现ProcessWindowFunction, 其中, 必须要实现 process( ) 方法, 这是处理窗口数据的主要方法.还在一下跟窗口数据相关的方法可以有选择的实现.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyProcessWindowFunction</span> <span class="keyword">extends</span> <span class="title">ProcessWindowFunction</span>&lt;<span class="title">Tuple3</span>&lt;<span class="title">String</span>, <span class="title">Long</span>, <span class="title">Long</span>&gt;, <span class="title">String</span>, <span class="title">Long</span>, <span class="title">TimeWindow</span>&gt; </span>&#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(Long s, Context context, Iterable&lt;Tuple3&lt;String, Long, Long&gt;&gt; elements, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">// 统计每个窗口内的所有数据的 f0字段加起来共有多少个单词</span></span><br><span class="line">    <span class="comment">// 也就做单个窗口的 wordcount</span></span><br><span class="line">Long count = <span class="number">0L</span>;</span><br><span class="line"><span class="keyword">for</span> (Tuple3&lt;String, Long, Long&gt; element : elements) &#123;</span><br><span class="line">count += element.f0.split(<span class="string">" "</span>).length;</span><br><span class="line">&#125;</span><br><span class="line">out.collect(<span class="string">"window: "</span> + context.window() + <span class="string">" word count: "</span> + count);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="5-增量与全量共同使用"><a href="#5-增量与全量共同使用" class="headerlink" title="5. 增量与全量共同使用"></a>5. 增量与全量共同使用</h4><p>增量聚合函数虽然性能好, 但是灵活性不如全量函数, 例如对窗口状态数据的操作以及对窗口中的元数据信息的获取. 但是如果用 ProcessWindowFunction 去完成一些基础的增量计算相对比较浪费资源, 因此可以两者结合的方式来实现.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">input.keyBy(x -&gt; x.f1)</span><br><span class="line">.timeWindow(Time.seconds(<span class="number">10</span>), Time.seconds(<span class="number">1</span>))</span><br><span class="line"><span class="comment">// 第一个Function为 ReduceFunction, 取窗口的最小值</span></span><br><span class="line">.reduce((r1, r2) -&gt; &#123;</span><br><span class="line"><span class="keyword">return</span> r1.f0 &lt; r2.f0 ? r1 : r2;</span><br><span class="line"><span class="comment">// 第二个Function为 ProcessWindowFunction, 获取窗口的时间信息</span></span><br><span class="line">&#125;, <span class="keyword">new</span> ProcessWindowFunction&lt;Tuple2&lt;Long, Long&gt;, String, Long, TimeWindow&gt;() &#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(Long aLong, Context context, Iterable&lt;Tuple2&lt;Long, Long&gt;&gt; elements, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">out.collect(<span class="string">"window: "</span> + context.window()); </span><br><span class="line">&#125;</span><br><span class="line">&#125;).print();</span><br></pre></td></tr></table></figure><h3 id="Flink-窗口中的其他组件"><a href="#Flink-窗口中的其他组件" class="headerlink" title="Flink 窗口中的其他组件"></a>Flink 窗口中的其他组件</h3><p>除了 Window Assigner 和 Window Function外,Flink的窗口中还有 Triger窗口触发器, 其负责判断何时将窗口中的数据取出做计算, flink已经默认为各种类型的窗口实现了 triger. 用户也可以自己手动指定. Evictors 是数据剔除器, 目的是把窗口中的数据按照需求做一定的剔除. Flink也有 API 针对延迟数据做处理, 延迟的数据可以丢弃也可以通过sideOutputLateDate( ) 方法处理.</p><h4 id="1-Triger-窗口触发器"><a href="#1-Triger-窗口触发器" class="headerlink" title="1. Triger 窗口触发器"></a>1. Triger 窗口触发器</h4><p><strong>EventTimeTrigger</strong>: 通过对比 watermark 和窗口 EndTime 确定是否触发窗口</p><p><strong>ProcessTimeTrigger</strong>: 通过对比 ProcessTime 和窗口 EndTime 确定是否触发窗口</p><p><strong>ContinuousEventTimeTrigger</strong>: 根据间隔时间周期性触发窗口</p><p><strong>ContinuousEventTimeTrigger</strong>: 同上, 区别是使用ProcessTime</p><p><strong>CountTrigger</strong>: 根据接入数量是否超过阈值</p><p><strong>DeltaTrigger</strong>: 根据计算出来的 Delta 指标是否超过指定的 Threshold</p><p><strong>PurgingTrigger</strong>: 可以将任意触发器作为参数转换为Purge类型触发器</p><h4 id="2-Evictors触发器"><a href="#2-Evictors触发器" class="headerlink" title="2. Evictors触发器"></a>2. Evictors触发器</h4><p>CountEvictor: 保持固定数量的数据, 超过的剔除</p><p>DeltaEvictor: 通过定义 delta 和 threshold , 计算两个数据之间的 delta 值, 超过则剔除</p><p>TimeEvictor: 指定时间间隔, 将当前窗口中的最新元素的时间减去Interval, 然后将小于该结果的数据全部剔除</p>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
            <tag> window </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink使用05-窗口简介和简单的使用</title>
      <link href="/2019/09/25/flink%E4%BD%BF%E7%94%A805-%E7%AA%97%E5%8F%A3%E7%AE%80%E4%BB%8B%E5%92%8C%E7%AE%80%E5%8D%95%E7%9A%84%E4%BD%BF%E7%94%A8/"/>
      <url>/2019/09/25/flink%E4%BD%BF%E7%94%A805-%E7%AA%97%E5%8F%A3%E7%AE%80%E4%BB%8B%E5%92%8C%E7%AE%80%E5%8D%95%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<h3 id="1-窗口组成简介"><a href="#1-窗口组成简介" class="headerlink" title="1. 窗口组成简介"></a>1. 窗口组成简介</h3><p>窗口是流式计算中非常重要的一个概念, 很多常见的功能都是通过各种窗口实现的, 比如每5分钟统计一下刚去1小时的热度. Flink DataStream API 将窗口独立成 Operator. 每个窗口算子包含了以下几个部分:</p><h4 id="Windows-Assigner"><a href="#Windows-Assigner" class="headerlink" title="Windows Assigner"></a>Windows Assigner</h4><p>指定窗口的类型, 定义如何将数据流分配到一个或者多个窗口</p><h4 id="Windows-Trigger"><a href="#Windows-Trigger" class="headerlink" title="Windows Trigger"></a>Windows Trigger</h4><p>指定窗口触发的时机, 定义窗口满足什么样的条件触发计算</p><h4 id="Evictor"><a href="#Evictor" class="headerlink" title="Evictor"></a>Evictor</h4><p>用户数据剔除</p><h4 id="Lateness"><a href="#Lateness" class="headerlink" title="Lateness"></a>Lateness</h4><p>标记是否处理迟到的数据, 当迟到数据到达窗口中是否触发计算</p><h4 id="Output-Tag"><a href="#Output-Tag" class="headerlink" title="Output Tag"></a>Output Tag</h4><p>标记输出标签, 然后再通过 getSideOutput 将窗口中的数据根据标签输出</p><h4 id="Windows-Function"><a href="#Windows-Function" class="headerlink" title="Windows Function"></a>Windows Function</h4><p>定义窗口上的数据处理的逻辑, 例如对数据进行sum</p><hr><h3 id="2-Window-Assigner"><a href="#2-Window-Assigner" class="headerlink" title="2. Window Assigner"></a>2. Window Assigner</h3><p>首先最需要了解的就是 windows Assigner了, 我们想要一个什么样的窗口划分, 主要就是通过他来实现的. </p><p>根据 flink 上游的数据集是否为 KeyedStream 类型 来做分别的处理. 如果使用了keyBy( ) 则对应使用window( ) 来处理, 否则可以使用 windowAll( )来使用</p><p>Flink 可以支持两种类型的窗口, 分别是基于时间的窗口和基于数量的窗口.基于时间的意思就是按照时间去划分窗口,同理,基于数量的也是根据窗口中的数量来做切分的. 对应的分别就是 timeWindow() 和 countWindow() 来使用, 下面的示例主要使用 timeWindow() 来演示.</p><p>对于不同的 Window Assigner, 还可以把窗口划分为4大类, 分别是 滚动窗口(Tumbling Windows) / 滑动窗口(Sliding Window) / 会话窗口(Session Window) 和 全局窗口(Global Window).</p><h4 id="滚动窗口"><a href="#滚动窗口" class="headerlink" title="滚动窗口"></a>滚动窗口</h4><p>DataStream API 提供基于 EventTime 和 ProcessingTime 的两种类型的 Tumbling window.对应的 Assigner 分别是 TumblingEventTimeWindow 和 ProcessingEventTimeWindow . 举例如下,完整代码见Github.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 使用ProcessTime的滚动时间窗口, 长度为10s</span></span><br><span class="line">stream.keyBy(x -&gt; x.f1)</span><br><span class="line">    .window(TumblingProcessingTimeWindows.of(Time.seconds(<span class="number">10</span>))).process(...)</span><br><span class="line"><span class="comment">// 使用ProcessTime的滚动时间窗口, 长度为10s</span></span><br><span class="line">stream.keyBy(x -&gt;x.f1).window(TumblingEventTimeWindows.of(Time.seconds(<span class="number">10</span>))).process(...)</span><br></pre></td></tr></table></figure><p>使用 window(TumblingProcessingTimeWindows.of(Time.seconds(10))) 的方法有点啰嗦, Flink 还提供了timeWindow( ) 的 API 来简化这一行代码. </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 直接使用 timeWindow API 便可实现滚动窗口的操作, 参数依旧是窗口的长度</span></span><br><span class="line"><span class="comment">// 窗口类型的时间由 time characteristic 确定, 如果指定为 event time,那么窗口也会自动用这个时间</span></span><br><span class="line">input.keyBy(x -&gt; x.f1).timeWindow(Time.seconds(<span class="number">10</span>));</span><br></pre></td></tr></table></figure><h4 id="滑动窗口"><a href="#滑动窗口" class="headerlink" title="滑动窗口"></a>滑动窗口</h4><p>滑动窗口顾名思义就是一个在不断往后滑动的窗口, 比如说 每5分钟 统计一个 最近一小时的时间, 那么就需要用滑动窗口来做处理. 滑动窗口主要是依靠 window size 和 slide time 来确定. 与滚动窗口类似的, flink 也提供了对应不同时间的 Assigner API(SlidingEventTimeWindow / SlidingEventTimeWindow), 语法基本类似, 只是由原本的一个参数(窗口长度) 变为了两个参数(窗口长度和滑动时间), 同样的, 为了简化代码, 依然可以使用timeWindow() 来简化.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 两个参数分别是 窗口长度 和 滑动时间, 窗口时间类型依旧通过time characteristic 确定</span></span><br><span class="line">input.keyBy(x -&gt; x.f1).timeWindow(Time.seconds(<span class="number">10</span>), Time.seconds(<span class="number">1</span>))</span><br></pre></td></tr></table></figure><h4 id="会话窗口"><a href="#会话窗口" class="headerlink" title="会话窗口"></a>会话窗口</h4><p>会话窗口主要是将某段时间内活跃度较高的数据聚合成一个窗口计算. 触发条件是 Session Gap. 在规定的时间内没有数据接入则认为这个窗口结束,然后触发窗口计算. Session Gap 除了固定间隔的方式, 也可以动态抽取.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建 Session Window, 间隔为 3s</span></span><br><span class="line">        DataStream&lt;Tuple3&lt;String, Long, Integer&gt;&gt; aggregated = source</span><br><span class="line">                .keyBy(<span class="number">0</span>)</span><br><span class="line">                .window(EventTimeSessionWindows.withGap(Time.seconds(<span class="number">3L</span>)))</span><br><span class="line">                .sum(<span class="number">2</span>);</span><br></pre></td></tr></table></figure><h4 id="全局窗口"><a href="#全局窗口" class="headerlink" title="全局窗口"></a>全局窗口</h4><p>全局窗口将所有key的数据分配到单个窗口中计算结果.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建 GlobalWindow</span></span><br><span class="line">        input.keyBy(<span class="number">1</span>)</span><br><span class="line">                .window(GlobalWindows.create())</span><br><span class="line">                .sum(<span class="number">1</span>);</span><br></pre></td></tr></table></figure><p><strong>上面就是构建不同的窗口的方法了, 下文会介绍在有了窗口之后怎样对窗口中的数据做处理</strong></p>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
            <tag> window </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink使用04-几种时间概念和watermark</title>
      <link href="/2019/09/24/flink%E4%BD%BF%E7%94%A804-%E5%87%A0%E7%A7%8D%E6%97%B6%E9%97%B4%E6%A6%82%E5%BF%B5%E5%92%8Cwatermark/"/>
      <url>/2019/09/24/flink%E4%BD%BF%E7%94%A804-%E5%87%A0%E7%A7%8D%E6%97%B6%E9%97%B4%E6%A6%82%E5%BF%B5%E5%92%8Cwatermark/</url>
      
        <content type="html"><![CDATA[<h3 id="时间概念"><a href="#时间概念" class="headerlink" title="时间概念"></a>时间概念</h3><p>在做实时计算的时候, 首先就需要搞清楚一个问题, 这个实时到底是怎么样的一个时间概念. 在 Flink 中, 总共有3种时间概念, 分别是 <strong>事件时间</strong> ( Event time ) / <strong>处理时间</strong> ( Processing time ) / <strong>接入时间</strong> ( Ingestion time).</p><p><img src="/2019/09/24/flink使用04-几种时间概念和watermark/%E6%97%B6%E9%97%B4%E7%9A%84%E6%A6%82%E5%BF%B5.png" alt></p><p><strong>事件时间</strong> ( Event time )就是真实的用户发生操作的时候所产生的时间, 对应到 flink 中, 需要用户 <strong>显示</strong> 的告诉 flink 到底每个输入中的哪一个字段代表这个事件时间。</p><p><strong>接入时间</strong> ( Ingestion time) 和<strong>处理时间</strong> ( Processing time )是不需要用户去指定的, flink自己会去处理这个时间. 接入时间的代表的是一个事件通过 source Operator 的时间, 相比于 event time, ingestion time 不能处理乱序事件, 因此也就不用生成对应的watermark. 处理时间是指事件在操作算子计算过程中获取到的所在主机的时间. processing time 适合用于时间计算精度要求不是特别高的计算场景, 例如统计某些延时非常高的日志数据.</p><hr><h3 id="水位线机制-watermark"><a href="#水位线机制-watermark" class="headerlink" title="水位线机制 watermark"></a>水位线机制 watermark</h3><h4 id="1-解释-watermark"><a href="#1-解释-watermark" class="headerlink" title="1, 解释 watermark"></a>1, 解释 watermark</h4><p>watermark 这个概念在 flink 中是与 event time 这个时间概念相互依存的, 其目的是为了解决数据乱序到达和系统延迟的问题. flink会把读取进系统的最新事件时间减去固定的时间间隔作为 watermark. 还是用一张图来解释watermark 的作用.</p><p>当事件进入 flink 中的时候, 根据提取的 event time 产生 watermark 时间戳, 记为 X, 进入 flink 中的 event time 记为 Y. 当窗口的 end time &lt; X 的时候, 则触发窗口计算结果并输出. 只要 X &lt; end time, 那么 事件就可以 一直进入到当前窗口中, 这样的话即便发生乱序, 也可以在窗口中调整. 调整的方法就是按照 Y. </p><p><img src="/2019/09/24/flink使用04-几种时间概念和watermark/watermark.gif" alt></p><h4 id="2-使用-watermark"><a href="#2-使用-watermark" class="headerlink" title="2, 使用 watermark"></a>2, 使用 watermark</h4><p> a. 在 Source Function 中 直接指定 Timestamps 和 Watermark</p><p>​    用户需要复写 SourceFunction 接口中 run( ) 方法实现数据逻辑, 同时调用 SourceContext 的 collectWithTimestamp( ) 方法生成 event time 时间戳, 调用 emitWatermark( ) 方法生成 watermark.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;String&gt; text = env.addSource(<span class="keyword">new</span> SourceFunction&lt;String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(SourceContext&lt;String&gt; ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                <span class="keyword">for</span> (String s : elementInput) &#123;</span><br><span class="line">                    <span class="comment">// 切割每一条数据</span></span><br><span class="line">                    String[] inp = s.split(<span class="string">","</span>);</span><br><span class="line">                    Long timestamp = <span class="keyword">new</span> Long(inp[<span class="number">1</span>]);</span><br><span class="line">                    <span class="comment">// 生成 event time 时间戳</span></span><br><span class="line">                    ctx.collectWithTimestamp(s, timestamp);</span><br><span class="line">                    <span class="comment">// 调用 emitWatermark() 方法生成 watermark, 最大延迟设定为 2</span></span><br><span class="line">                    ctx.emitWatermark(<span class="keyword">new</span> Watermark(timestamp - <span class="number">2</span>));</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// 设定默认 watermark</span></span><br><span class="line">                ctx.emitWatermark(<span class="keyword">new</span> Watermark(Long.MAX_VALUE));</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cancel</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br></pre></td></tr></table></figure><p>b. 通过 Flink 自带的 Timestamp Assigner 指定 Timestamp 和 生成 watermark </p><p>在使用了 flink 定义的外部数据源( 如 kafka) 之后, 就不能通过自定义 sourcefunction 的方式来生成 watermark 和 event time 了, 这个时候可以使用 Timestamp Assigner,  其需要在第一个时间相关的 Operator前使用. Flink 有自己定义好的 Timestamp Assigner 可以直接使用 (包括直接指定的方式和固定时间延迟的方式 ).Flink 将 watermark 分为 Periodic Watermarks (根据设定的时间间隔周期性的生成) 和 Punctuated Watermarks (根据接入数量生成), 用户也可以继承对应的类实现这两种 watermark.</p><p>b.1 使用 Ascending Timestamp Assigner 指定 Timestamps 和 Watermark</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 首先需要指定系统时间概念为 event time</span></span><br><span class="line">env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span><br><span class="line"><span class="comment">// 使用 Ascending 分配 时间信息和 watermark</span></span><br><span class="line">   DataStream&lt;Tuple2&lt;String, Long&gt;&gt; text = env.fromCollection(collectionInput);</span><br><span class="line">   text.assignTimestampsAndWatermarks(<span class="keyword">new</span> AscendingTimestampExtractor&lt;Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">   <span class="meta">@Override</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractAscendingTimestamp</span><span class="params">(Tuple2&lt;String, Long&gt; element)</span> </span>&#123;</span><br><span class="line">   <span class="keyword">return</span> element.f1;</span><br><span class="line">   &#125;</span><br><span class="line">   &#125;);</span><br></pre></td></tr></table></figure><p>b.2 使用固定时延间隔的 Timestamp Assigner 指定</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 使用 Ascending 分配 时间信息和 watermark 设定10s 代表最长的时延</span></span><br><span class="line">    DataStream&lt;Tuple2&lt;String, Long&gt;&gt; text = env.fromCollection(collectionInput);</span><br><span class="line">    text.assignTimestampsAndWatermarks(<span class="keyword">new</span> BoundedOutOfOrdernessTimestampExtractor&lt;Tuple2&lt;String, Long&gt;&gt;(Time.seconds(<span class="number">10</span>)) &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(Tuple2&lt;String, Long&gt; element)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">return</span> element.f1;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure><p>c. 自定义 Timestamp Assigner 和 Watermark Generator</p><p>用户可以自定义实现 AssignerWithPeriodicWatermarks 和 AssignerWithPunctuatedWatermarks 两个接口来分别生成对应的两种 watermark. 这一块用的比较少, 以后有机会再细写.</p>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
            <tag> time </tag>
            
            <tag> watermark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink使用03-数据输入的几种不同方法</title>
      <link href="/2019/09/04/flink%E4%BD%BF%E7%94%A803-%E6%95%B0%E6%8D%AE%E8%BE%93%E5%85%A5%E7%9A%84%E5%87%A0%E7%A7%8D%E4%B8%8D%E5%90%8C%E6%96%B9%E6%B3%95/"/>
      <url>/2019/09/04/flink%E4%BD%BF%E7%94%A803-%E6%95%B0%E6%8D%AE%E8%BE%93%E5%85%A5%E7%9A%84%E5%87%A0%E7%A7%8D%E4%B8%8D%E5%90%8C%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h3 id="flink的数据输入源主要分为两大类"><a href="#flink的数据输入源主要分为两大类" class="headerlink" title="flink的数据输入源主要分为两大类:"></a>flink的数据输入源主要分为两大类:</h3><h4 id="1-内置数据源"><a href="#1-内置数据源" class="headerlink" title="1. 内置数据源"></a>1. 内置数据源</h4><ul><li><p>集合数据源</p><p>可以将数组或者集合作为 flink 的数据源,分别有不同的方法可以使用, 这种方式比较适合本地调试使用</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// 添加数组作为数据输入源</span><br><span class="line">String[] elementInput = new String[]&#123;&quot;hello Flink&quot;, &quot;Second Line&quot;&#125;;</span><br><span class="line">DataStream&lt;String&gt; text = env.fromElements(elementInput);</span><br><span class="line"></span><br><span class="line">// 添加List集合作为数据输入源</span><br><span class="line">List&lt;String&gt; collectionInput = new ArrayList&lt;&gt;();</span><br><span class="line">collectionInput.add(&quot;hello Flink&quot;);</span><br><span class="line">DataStream&lt;String&gt; text2 = env.fromCollection(collectionInput);</span><br></pre></td></tr></table></figure></li><li><p>Socket数据源</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">// 添加Socket作为数据输入源</span><br><span class="line">// 4个参数 -&gt; (hostname:Ip地址, port:端口, delimiter:分隔符, maxRetry:最大重试次数)</span><br><span class="line">DataStream&lt;String&gt; text3 = env.socketTextStream(&quot;localhost&quot;, 9999, &quot;\n&quot;, 4);</span><br></pre></td></tr></table></figure></li><li><p>文件数据源</p><p>可以使用 readTextFile 方法直接读取文本文件, 这种方式可以用来监控一下 log 日志文件, 也可以使用 readFile 方法通过指定 InputFormat 来读取特定数据类型的文件, InputFormat可以是内置类,如 CsvInputFormat 或者用户自定义 InputFormat 接口类.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">// 添加文件源</span><br><span class="line">// 直接读取文本文件</span><br><span class="line">DataStream&lt;String&gt; text4 = env.readTextFile(&quot;/opt/history.log&quot;);</span><br><span class="line"></span><br><span class="line">// 指定 CsvInputFormat, 监控csv文件(两种模式), 时间间隔是10ms</span><br><span class="line">        DataStream&lt;String&gt; text5 = env.readFile(new CsvInputFormat&lt;String&gt;(new Path(&quot;/opt/history.csv&quot;)) &#123;</span><br><span class="line">            @Override</span><br><span class="line">            protected String fillRecord(String s, Object[] objects) &#123;</span><br><span class="line">                return null;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;,&quot;/opt/history.csv&quot;, FileProcessingMode.PROCESS_CONTINUOUSLY,10);</span><br></pre></td></tr></table></figure><p>在 readFile() 方法中有一项参数为 WatchType, 共有两种模式 (PROCESS_CONTINUOUSLY / PROCESS_ONCE). 在 PROCESS_CONTINUOUSLY  模式下, 检测到文件变动就会将文件全部内容加载在 flink, 在 PROCESS_ONCE 模式下, 只会将文件变动的那部分加载到 flink.  </p></li></ul><h4 id="2-外部数据源"><a href="#2-外部数据源" class="headerlink" title="2. 外部数据源"></a>2. 外部数据源</h4><p>外部数据源是重头戏, 一般来说项目中均是使用外部数据源作为数据的源头, flink 通过实现 SourceFunction 定义了非常丰富的第三方数据连接器</p><ul><li><p>数据源连接器</p><p>对于第三方数据源, flink的支持分为三种,有只读型(Twitter Streaming API / Netty ), 只写型( Cassandra / Elasticsearch / hadoop FileSystem), 支持读写(Kafka / Amazon Kinesis / RabbitMQ)</p><p>Apache Kafka (Source / Sink)</p><p>Apache Cassandra (Sink)</p><p>Amazon Kinesis Streams (Source / Sink)</p><p>Elasticsearch (Sink)</p><p>Hadoop FileSystem (Sink)</p><p>RabbitMQ (Source / Sink)</p><p>Apache NiFI (Source / Sink)</p><p>Twitter Streaming API (Source)</p><p><strong>Apache Bahir 中的连接器</strong>:</p><p>Apache ActiveMQ (Source / Sink)</p><p>Apache Flume (Sink)</p><p>Redis (Sink)</p><p>Akka (Sink)</p><p>Netty (Source)</p></li></ul><p><strong>以Kafka 为例 做演示</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 配置 kafka 连接参数</span></span><br><span class="line">String topic = <span class="string">"topic_name"</span>;</span><br><span class="line">String bootStrapServers = <span class="string">"localhost:9092"</span>;</span><br><span class="line">String zkConnect = <span class="string">"localhost:2181"</span>;</span><br><span class="line">String groupID = <span class="string">"group_A"</span>;</span><br><span class="line">Properties prop = <span class="keyword">new</span> Properties();</span><br><span class="line">prop.setProperty(<span class="string">"bootstrap.servers"</span>, bootStrapServers);</span><br><span class="line">prop.setProperty(<span class="string">"zookeeper.connect"</span>, zkConnect);</span><br><span class="line">prop.setProperty(<span class="string">"group.id"</span>, groupID);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建 kafka connector source</span></span><br><span class="line">FlinkKafkaConsumer010&lt;String&gt; consumer010 = <span class="keyword">new</span> FlinkKafkaConsumer010&lt;&gt;(topic, <span class="keyword">new</span> SimpleStringSchema(), prop);</span><br><span class="line"></span><br><span class="line"><span class="comment">// add source</span></span><br><span class="line">DataStreamSource&lt;String&gt; dataStream = env.addSource(consumer010);</span><br></pre></td></tr></table></figure><ul><li><p>自定义数据源连接器</p><p>用户也可以自己定义连接器, 通过实现 SourceFunction 定义单个线程的接入的数据连接器, 也可以通过实现ParallelSourceFunction 接口或者继承 RichParallelSourceFunction 类定义并发数据源接入器.</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
            <tag> dataSource </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink使用02-从WordCount开始</title>
      <link href="/2019/09/03/flink%E4%BD%BF%E7%94%A802-%E4%BB%8EWordCount%E5%BC%80%E5%A7%8B/"/>
      <url>/2019/09/03/flink%E4%BD%BF%E7%94%A802-%E4%BB%8EWordCount%E5%BC%80%E5%A7%8B/</url>
      
        <content type="html"><![CDATA[<p>相信大家在学习spark的时候接触的第一个案例肯定也是 wordCount, 本文也想通过这样一个简单的例子来讲一下一个简单的 flink 程序是什么样子的, 让大家对 flink 的代码有一个简单的了解.</p><p>一个 flink程序主要分为5个部分:</p><ul><li><strong>1. 获取执行 Environment</strong></li></ul><p>environment 提供控制 job 执行的方法(例如设置并行度/容错/checkpoint 参数) 并且与外部系统做交互. flink可以做流计算也可以做批计算, 对应的也就有不同的environment , 在使用时根据不同的使用场景选择即可.</p><ul><li><p><strong>2. 获取输入流 Source</strong></p><p>一个流式计算框架自然是少不了数据的输入, 在 streamExecutionEnvironment 的可以看到有很多种创建输入流的方式, 不过在项目中使用最多的还是使用 addSource()方法来添加不同的数据源接入</p><p><img src="/2019/09/03/flink使用02-从WordCount开始/%E6%95%B0%E6%8D%AE%E6%BA%90%E6%8E%A5%E5%85%A5%E6%96%B9%E6%B3%95.png" alt></p></li><li><p><strong>3. 执行计算 Operator</strong></p><p>在spark中,对数据的转换计算是通过 action 算子和 transformation 算子来对 RDD 中的数据来进行操作, 而在flink中, 主要是通过 Operator来对一个流做处理, 通过一个 Operator 可以将一个流转换为另外一个流, flink中内置了很多算子来实现Operator操作.</p></li><li><p><strong>4. 输入结果 Sink</strong></p><p>在完成数据计算之后,就需要有一个输出的地方, 通常来讲也是通过 addSink() 方法来添加不同的数据输出目标,也可以通过 print() 来直接查看输出或者写入到csv等外部文件.</p></li><li><p><strong>5. 启动 flink,提交 job</strong></p><p>一个 flink 代码的启动执行, 必须通过 env.executor() 方法.这行代码主要做了以下事情:</p><ol><li>生成StreamGraph </li><li>生成JobGraph. </li><li>生成一系列配置</li><li>将 JobGraph和配置交给 flink 集群去运行</li><li>以本地模式运行的话,可以看到启动过程,如启动能量度,web模块,jobManager,ResourceManager,taskManager等等</li><li>启动任务</li></ol></li></ul><h4 id="以下为简单的-WordCount-代码"><a href="#以下为简单的-WordCount-代码" class="headerlink" title="以下为简单的 WordCount 代码"></a>以下为简单的 WordCount 代码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取 StreamEnv</span></span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取 输入流</span></span><br><span class="line">        DataStream&lt;String&gt; text = env.fromElements(WordCountData.WORDS);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 执行计算Operator</span></span><br><span class="line">        DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; counts</span><br><span class="line">                = text.flatMap(<span class="keyword">new</span> SplitFunction())</span><br><span class="line">                .keyBy(<span class="number">0</span>).sum(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 输出结果</span></span><br><span class="line">        counts.print();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 启动flink程序</span></span><br><span class="line">        env.execute(<span class="string">"WordCount Demo"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// *************************************************************************</span></span><br><span class="line">    <span class="comment">// 自定义切割Function切分一行输入</span></span><br><span class="line">    <span class="comment">// *************************************************************************</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">SplitFunction</span> <span class="keyword">implements</span> <span class="title">FlatMapFunction</span>&lt;<span class="title">String</span>, <span class="title">Tuple2</span>&lt;<span class="title">String</span>, <span class="title">Integer</span>&gt;&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String s, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; collector)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            String[] words = s.toLowerCase().split(<span class="string">" "</span>);</span><br><span class="line">            <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">                <span class="keyword">if</span> (word.length() &gt; <span class="number">0</span>)&#123;</span><br><span class="line">                    collector.collect(<span class="keyword">new</span> Tuple2&lt;&gt;(word, <span class="number">1</span>));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink使用01-本系列简介</title>
      <link href="/2019/09/03/flink%E4%BD%BF%E7%94%A801-%E6%9C%AC%E7%B3%BB%E5%88%97%E7%AE%80%E4%BB%8B/"/>
      <url>/2019/09/03/flink%E4%BD%BF%E7%94%A801-%E6%9C%AC%E7%B3%BB%E5%88%97%E7%AE%80%E4%BB%8B/</url>
      
        <content type="html"><![CDATA[<p>​    Flink 是一款能够同时支持高吞吐/低延迟/高性能的分布式处理框架.</p><p>​    本系列叫做 &lt;Flink简易使用教程&gt;,  目的是记录自己学习 flink 的过程,并且把使用flink的方方面面介绍给大家.尽量用简单的话把使用方法说清楚,在使用某个具体功能的时候能够快速的查找到该使用方法.</p><p>​    本系列的主要例子会从 flink 官方仓库的 example 出发, 通过这些代码来使用 flink 的一些基本操作.</p><p><img src="/2019/09/03/flink使用01-本系列简介/flinkExample.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> flink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
